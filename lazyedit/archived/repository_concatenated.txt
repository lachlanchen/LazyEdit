===== ./subtitle_translate_old.py =====
import json
import json5
import re
import traceback
import openai

from packaging import version
import json
import os

from concurrent.futures import ThreadPoolExecutor, as_completed



from lazyedit.openai_version_check import OpenAI


from lazyedit.utils import JSONParsingError, JSONValidationError
from lazyedit.utils import safe_pretty_print, sample_texts, find_font_size

from datetime import datetime
from pprint import pprint

import cjkwrap

import glob

import numpy as np


# def wrap_text(text, width, is_cjk):
#     if is_cjk:
#         # Use cjkwrap for CJK text
#         return cjkwrap.wrap(text, width)
#     else:
#         # Use cjkwrap for non-CJK text as well, as it should handle both appropriately
#         return cjkwrap.wrap(text, width)



class SubtitlesTranslator:
    def __init__(self, 
        openai_client, 
        input_json_path, 
        input_sub_path, 
        output_sub_path,
        video_length=None,
        video_width=1080,
        video_height=1920,
        max_retries=3,
        use_cache=False
    ):
        self.client = openai_client
        self.input_json_path = input_json_path
        self.input_sub_path = input_sub_path
        self.output_sub_path = output_sub_path
        self.max_retries = max_retries
        self.video_length = video_length

        self.video_width = video_width
        self.video_height = video_height
        self.base_width = 1920
        self.base_height = 1080

        self.wrapping_limit_half_width_default = 60  # Default wrapping_limit_half_width for landscape
        
        if not self.is_video_landscape:
            self.wrapping_limit_half_width_default = int(self.wrapping_limit_half_width_default * 0.5)  # Adjust wrapping_limit_half_width for portrait videos


        # self.is_video_landscape = video_width > video_height
        self.font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"

        self.translation_log_folder = 'translation_logs'
        self.use_cache = use_cache

        print("Using translation cache: ", use_cache)

        self.ensure_log_folder_exists()



    @property
    def is_video_landscape(self):
        """Determine if the video is landscape or portrait based on class variables."""
        return self.video_width > self.video_height

    def ensure_log_folder_exists(self):
        if not os.path.exists(self.translation_log_folder):
            os.makedirs(self.translation_log_folder)

    def get_log_filename(self, lang="ja", idx=0):
        base_filename = os.path.splitext(os.path.basename(self.input_json_path))[0]
        datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        return f"{self.translation_log_folder}/{base_filename}-part{idx}-{lang}-{datetime_str}.json"

    def save_translation_attempt(self, prompt, response, lang="ja", idx=0):
        log_filename = self.get_log_filename(lang=lang, idx=idx)
        translation_attempt = {
            "input_json_path": self.input_json_path,
            "prompt": prompt,
            "response": response
        }
        with open(log_filename, 'w', encoding='utf-8') as file:
            json.dump(translation_attempt, file, indent=4, ensure_ascii=False)


    def load_latest_translation_attempt(self, lang="ja", idx=0):
        """
        Load the latest translation attempt from the translation logs folder
        based on the input JSON path.
        """
        base_filename = os.path.splitext(os.path.basename(self.input_json_path))[0]
        pattern = f"{self.translation_log_folder}/{base_filename}-part{idx}-{lang}-*.json"
        files = glob.glob(pattern)
        if not files:
            return None  # No cache available

        # Find the latest file based on the naming convention
        latest_file = max(files, key=os.path.getctime)
        with open(latest_file, 'r', encoding='utf-8') as file:
            cached_data = json5.load(file)
        return cached_data["response"]


    def load_subtitles_from_json(self):
        """Load subtitles from a JSON file."""
        with open(self.input_json_path, 'r', encoding='utf-8') as file:
            return json5.load(file)

    @staticmethod
    def correct_json_string(s):
        # Remove trailing commas after object properties or array elements
        corrected_s = re.sub(r',\s*}', '}', s)
        corrected_s = re.sub(r',\s*\]', ']', corrected_s)
        return corrected_s

    def extract_and_parse_json(self, text):
        """Extract and parse JSON from text, handling potential parsing issues."""
        bracket_pattern = r'\[.*\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL)
        # json_string = ""

        if not matches:
            raise JSONParsingError("No JSON string found in text", text, text)
        json_string = matches[0].replace('\n', '')

        # pprint(json_string)
        safe_pretty_print(json_string)

        try:
            json_string = self.correct_json_string(json_string)
            return json5.loads(json_string)
        except ValueError as e:
            traceback.print_exc()
            raise JSONParsingError(f"JSON Decode Error: {e}", json_string, text)

    def validate_translated_subtitles(self, subtitles, required_fields=["start", "end", "en", "zh"]):
        """Validate the structure of translated subtitles."""
        # required_fields = ["start", "end", "en", "zh"]
        for subtitle in subtitles:
            if not all(field in subtitle for field in required_fields):
                raise JSONValidationError("Subtitle missing one of the required fields: " + ", ".join(required_fields))

    # def translate_and_merge_subtitles(self, subtitles):
    #     """Splits subtitles into 1-minute batches and processes each batch."""
    #     # subtitles = self.load_subtitles_from_json()

    #     # Splitting subtitles into 1-minute batches
    #     batches = self.split_subtitles_into_batches(subtitles)

    #     # Process each batch and accumulate results
    #     translated_subtitles = []
    #     for batch in batches:
    #         translated_batch = self.translate_and_merge_subtitles_in_batch(batch)
    #         translated_subtitles.extend(translated_batch)

    #     # Save the final translated subtitles
    #     # self.save_translated_subtitles_to_ass(translated_subtitles)
    #     print("All subtitles have been processed and saved successfully.")

    #     return translated_subtitles

    def translate_and_merge_subtitles(self, subtitles):
        """Splits subtitles into 1-minute batches and processes each batch in parallel."""
        # subtitles = self.load_subtitles_from_json()

        # Splitting subtitles into 1-minute batches
        batches = self.split_subtitles_into_batches(subtitles)

        # Process each batch and accumulate results in parallel
        translated_subtitles = self.process_batches_in_parallel(batches)

        # Save the final translated subtitles
        # self.save_translated_subtitles_to_ass(translated_subtitles)
        print("All subtitles have been processed and saved successfully.")

        return translated_subtitles

    def process_batches_in_parallel(self, batches):
        """Process subtitle batches in parallel using ThreadPoolExecutor."""
        translated_subtitles = []

        with ThreadPoolExecutor(max_workers=1) as executor:
            # Submit all batches to be processed in parallel
            future_to_batch = {executor.submit(self.translate_and_merge_subtitles_in_batch, batch, i): batch for i, batch in enumerate(batches)}

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    translated_batch = future.result()
                    translated_subtitles.extend(translated_batch)
                except Exception as exc:
                    print(f'Batch {batch} generated an exception: {exc}')
        
        # Optional: Sort the merged list by start timestamps if necessary
        translated_subtitles.sort(key=lambda x: datetime.strptime(x['start'], '%H:%M:%S,%f'))

        return translated_subtitles

    def split_subtitles_into_batches(self, subtitles):
        """Splits subtitles into 1-minute batches based on their timestamps."""
        batches = []
        current_batch = []
        current_batch_start_time = None

        for subtitle in subtitles:
            start_time = datetime.strptime(subtitle["start"], '%H:%M:%S,%f')
            if current_batch_start_time is None:
                current_batch_start_time = start_time

            if (start_time - current_batch_start_time).total_seconds() > 60:
                # Start a new batch if the current subtitle start time exceeds 1 minute from the batch's start time
                batches.append(current_batch)
                current_batch = [subtitle]
                current_batch_start_time = start_time
            else:
                current_batch.append(subtitle)

        # Add the last batch if it contains subtitles
        if current_batch:
            batches.append(current_batch)

        return batches

    # def translate_and_merge_subtitles_in_batch(self, subtitles):
    #     """Merge translations from Japanese-specific and other languages' functions."""
    #     translations_ja = self.translate_and_merge_subtitles_ja(subtitles)
    #     translations_other_lang = self.translate_and_merge_subtitles_other_languages(subtitles)

    #     # Creating a dictionary for other languages translations for quick lookup by timestamp
    #     timestamps_dict = {
    #         (translation['start'], translation['end']): translation for translation in translations_other_lang
    #     }

    #     # Iterate through Japanese translations to merge
    #     for ja_translation in translations_ja:
    #         key = (ja_translation['start'], ja_translation['end'])
    #         if key in timestamps_dict:
    #             # If timestamp exists, replace Japanese translation
    #             timestamps_dict[key]['ja'] = ja_translation['ja']
    #         else:
    #             # If timestamp does not exist, add new entry with Japanese translation
    #             timestamps_dict[key] = ja_translation

    #     # Convert the dictionary back to list to get the final merged translations
    #     merged_translations = list(timestamps_dict.values())

    #     # Optional: Sort the merged list by start timestamps if necessary
    #     merged_translations.sort(key=lambda x: x['start'])

    #     return merged_translations

    def translate_and_merge_subtitles_in_batch(self, subtitles, idx):
        """Merge translations from Japanese-specific and other languages' functions in parallel."""
        with ThreadPoolExecutor(max_workers=1) as executor:
            # Submit both translation tasks to the executor
            future_ja = executor.submit(self.translate_and_merge_subtitles_ja, subtitles, idx)
            future_other_lang = executor.submit(self.translate_and_merge_subtitles_other_languages, subtitles, idx)

            # Wait for both futures to complete and retrieve results
            translations_ja = future_ja.result()
            translations_other_lang = future_other_lang.result()

        # Merge translations as before
        timestamps_dict = {
            (translation['start'], translation['end']): translation for translation in translations_other_lang
        }

        for ja_translation in translations_ja:
            key = (ja_translation['start'], ja_translation['end'])
            if key in timestamps_dict:
                timestamps_dict[key]['ja'] = ja_translation['ja']
            else:
                timestamps_dict[key] = ja_translation

        merged_translations = list(timestamps_dict.values())
        merged_translations.sort(key=lambda x: x['start'])

        return merged_translations


    def translate_and_merge_subtitles_other_languages(self, subtitles, idx):
        """Translate and merge subtitles using the OpenAI API."""

        print("Translating subtitles into other languages...")
        
        client = self.client


        # Constructing the messages with the optimized prompt
        messages = [
            {
                "role": "system",
                "content": "Translate and merge mixed language subtitles into English and Chinese, providing coherent and accurate translations."
            },
            {
                "role": "user",
                "content": ""
            }
        ]

        retries = 0

        while retries < self.max_retries:
            try:
                # Placeholder for OpenAI API call setup
                # Construct the prompt for translation and merging
                # Constructing the detailed prompt with placeholders for subtitles
                # Prepare the detailed prompt for translation and merging
                
                prompt_content = (
                    "Below are mixed language subtitles extracted from a video, including timestamps, "
                    "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
                    "is presented with English (en), Chinese (zh)， Arabic (ar) translations, "
                    "maintaining the original timestamps. "
                    "If a subtitle is already in English, provide the corresponding Chinese, Arabic translation, and vice versa. "
                    "For subtitles in any other language, keep the original text but also provide translations in "
                    "English, Chinese, Arabic. \n\n"

                    # "Must provide Japanese subtitles with furigana in this format '< Kanji>[Furigana]'."
                    # "Use '<>' to confine the whole annotated kanji area. Use '[]' to confine the furigana. "
                    # "Please exact follow this format. Otherwise my parser will report error. \n\n"

                    # "Below are mixed language subtitles extracted from a video, including timestamps, "
                    # "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
                    # "is presented with English (en),  Chinese (zh), Arabic (ar) and Japanese (ja) translations, "
                    # "maintaining the original timestamps. "
                    # "If a subtitle is already in English, provide the corresponding Chinese translation, and vice versa. "
                    # "For subtitles in any other language, keep the original text but also provide translations in "
                    # "English, Chinese, Arabic and Japanese.\n\n"

                    # "I only care about most common languages like Chinese, English, Japanese and Arabic. "
                    # "If I said 阿南伯, it's regonition error of 阿拉伯. "
                    "Fullfill the instructions/requests in subtitles per se for other languages with iso_code_639_1 language key. "
                    "If I said in subtitles that I want to know or I don't know how to say something, "
                    "provide the whole subtitles in that language. "
                    # "For example, if I want to know something in Arabic 阿拉伯, provide the Arabic language subtitle. Or,"
                    # "if I said I don't know how to say something in Japanese 日语, Provide the Japanese language subtitle. \n\n"
                    # "Some weird language might be speech recognition error. "
                 
                    "Correct some apparent speech recognition error and inconsistencies, "
                    "especially homonym and mumble in both origin and its translation based on the context.\n\n"
            
                    "Process the following subtitles, ensuring translations are accurate and coherent, "
                    "and format the output as shown in the example. "
                    "Note that the original timestamps should be preserved for each entry.\n\n"

                    "Subtitles to process:\n"
                    f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"
                    
                    # "Please provide a complete and accurate translation and formatting for each subtitle entry."

                    "Output JSON format only:\n"
                    "```json"
                    "[\n"
                    "  {\n"
                    "    \"start\": \"timestamp\",\n"
                    "    \"end\": \"timestamp\",\n"
                    "    \"en\": \"English text\",\n"
                    "    \"zh\": \"Chinese text\",\n"
                    "    \"ar\": \"Arabic text\",\n"
                    # "    \"ja\": \"Japanese text\",\n"
                    "    \"...\": \"Text in the original language, if not English or Chinese\"\n"
                    "  }\n"
                    "]\n\n"
                    "```"
                )

                ai_response = None
                if self.use_cache:
                    ai_response = self.load_latest_translation_attempt(lang="zh_en_ar", idx=idx)
                
                if not self.use_cache or not ai_response:
                    

                    messages[1]["content"] = prompt_content

                    # Sending the request to the OpenAI API
                    client = openai.OpenAI()  # Initializing the OpenAI client
                    response = client.chat.completions.create(
                        model=os.environ.get("OPENAI_MODEL", "gpt-4-0125-preview"),
                        messages=messages
                    )

                    # Extracting and printing the AI's response
                    ai_response = response.choices[0].message.content.strip()

                translated_subtitles = self.extract_and_parse_json(ai_response)
                
                self.validate_translated_subtitles(translated_subtitles)

                 # Save the successful attempt with the new method
                self.save_translation_attempt(prompt_content, ai_response, lang="zh_en_ar", idx=idx)

                return translated_subtitles
            except (JSONParsingError, JSONValidationError) as e:
                self.use_cache = False

                print(f"Attempt {retries + 1} failed: {e}")

                # Append the response and error message for context
                messages.append({"role": "system", "content": ai_response})
                messages.append({"role": "user", "content": e.message})
                
                retries += 1
                
                if retries >= self.max_retries:
                    # # Check if the processed_sub_path exists and remove it if it does
                    # if os.path.exists(self.output_sub_path):
                    #     os.remove(self.output_sub_path)

                    # # Now, safely create a hard link
                    # try:
                    #     os.link(self.input_sub_path, self.output_sub_path)
                    # except OSError as e:
                    #     print(f"Error creating hard link: {e}")

                    raise Exception("Failed after maximum retries. ")

    # Function to annotate Kanji and Katakana independently
    @staticmethod
    def annotate_kanji_katakana(subtitles):
        # Define the regular expression patterns for Kanji and Katakana
        kanji_pattern = r'[\u4e00-\u9faf]+'
        katakana_pattern = r'[\u30a0-\u30ff]+'
        
        # Function to wrap text in <>[]
        def replacer(match):
            return f'<{match.group(0)}>[]'
        
        # Annotate each item in the list
        for item in subtitles:
            # Annotate Kanji
            item['ja'] = re.sub(kanji_pattern, replacer, item['ja'])
            # Annotate Katakana
            item['ja'] = re.sub(katakana_pattern, replacer, item['ja'])
        
        return subtitles

    def translate_and_merge_subtitles_ja(self, subtitles, idx):
        """Request Japanese subtitles separately with specific formatting for furigana."""
        print("Translating subtitles to Japanese...")

        messages_ja = [
            {
                "role": "system",
                # "content": "Translate subtitles into Japanese with furigana format, correcting any errors based on context."
                "content": "Translate subtitles into Japanese, correcting any errors based on context."
            },
            {
                "role": "user",
                "content": ""
            }
        ]

        retries = 0
        while retries < self.max_retries:
            try:
                prompt_content_ja = (
                    "Translate the following subtitles into Japanese. "
                    # "with furigana in angle and square brackets like <漢字>[かんじ]. "
                    # "ensuring to include furigana in the specified format '<Kanji>[Furigana]'. \n\n"
                    "\n\n"
                    
                    # "Please exact follow this format. Otherwise my parser will report error. \n\n"

                    # "Maintain the original timestamps and provide translations only in Japanese, "
                    # "applying the correct formatting for furigana. "

                    "Correct speech recognition errors and inconsistencies based on context.\n\n"
                    
                    "Note that the original timestamps should be preserved for each entry.\n\n"

                    
                    "Subtitles to process:\n"
                    f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

                    # "KEEP the 漢字 in angle brackets like '<漢字>'. "
                    # "KEEP the ふりがな in square brackets like '[ふりがな]'. "
                    # "Prefer English transliterated word over furigana if exists."
                    # "Provide furigana/transliterated word for ALL kanji/漢字. \n\n"
                    # "Use '<>' to confine the whole annotated kanji area. Use '[]' to confine the furigana. "

                    "Output JSON format only:\n"
                    "```json\n"
                    "[\n"
                    "  {\n"
                    "    \"start\": \"timestamp\",\n"
                    "    \"end\": \"timestamp\",\n"
                    "    \"ja\": \"Japanese text\",\n"
                    "  }\n"
                    "]\n"
                    "```"
                )

                ai_response_ja = None
                if self.use_cache:
                    ai_response_ja = self.load_latest_translation_attempt(lang="ja", idx=idx)
                
                if not self.use_cache or not ai_response_ja:

                    
                    messages_ja[1]["content"] = prompt_content_ja
                    

                    response_ja = self.client.chat.completions.create(
                        model=os.environ.get("OPENAI_MODEL", "gpt-4-0125-preview"),
                        messages=messages_ja
                    )

                    ai_response_ja = response_ja.choices[0].message.content.strip()

                translated_subtitles_ja = self.extract_and_parse_json(ai_response_ja)
                self.validate_translated_subtitles(translated_subtitles_ja, required_fields=["start", "end", "ja"])
                self.save_translation_attempt(prompt_content_ja, ai_response_ja, lang="ja", idx=idx)

                annotated_subtitles = self.annotate_kanji_katakana(translated_subtitles_ja)

                print("annotated subtitles: \n")
                pprint(annotated_subtitles)

                translated_subtitles_ja_with_furigana = self.add_furigana_for_japanese_subtitles(annotated_subtitles, idx)

                print("furigana subtitles: \n")
                pprint(translated_subtitles_ja_with_furigana)

                # return translated_subtitles_ja
                return translated_subtitles_ja_with_furigana

            except (JSONParsingError, JSONValidationError) as e:
                self.use_cache = False

                print(f"Attempt {retries + 1} failed: {e}")

                # Append the response and error message for context
                messages_ja.append({"role": "system", "content": ai_response_ja})
                messages_ja.append({"role": "user", "content": e.message})
                
                retries += 1
                
                if retries >= self.max_retries:
                    # # Check if the processed_sub_path exists and remove it if it does
                    # if os.path.exists(self.output_sub_path):
                    #     os.remove(self.output_sub_path)

                    # # Now, safely create a hard link
                    # try:
                    #     os.link(self.input_sub_path, self.output_sub_path)
                    # except OSError as e:
                    #     print(f"Error creating hard link: {e}")

                    raise Exception("Failed after maximum retries. ")


    def add_furigana_for_japanese_subtitles(self, subtitles, idx):
        """Request Japanese subtitles separately with specific formatting for furigana."""
        print("Adding furigana to translated subtitles...")

        messages_ja = [
            {
                "role": "system",
                "content": "Add furigana (inside []) to Japanese kanji (inside <>) with provided format. "
            },
            {
                "role": "user",
                "content": ""
            }
        ]

        retries = 0
        while retries < self.max_retries:
            try:
                prompt_content_ja = (
                    "Add the correct furigana inside the square brackets like <漢字/katakana>[かんじ]. "
                    # "ensuring to include furigana in the specified format '<Kanji>[Furigana]'. \n\n"
                    "\n\n"
                    
                    # "Please exact follow this format. Otherwise my parser will report error. \n\n"

                    # "Maintain the original timestamps and provide translations only in Japanese, "
                    # "applying the correct formatting for furigana. "

                    # "Correct speech recognition errors and inconsistencies based on context.\n\n"


                    
                    # "Prefer English transliterated word over furigana if exists."
                    # "Provide furigana/transliterated word for ALL kanji/漢字. \n\n"
                    # "Use '<>' to confine the whole annotated kanji area. Use '[]' to confine the furigana. "

                    
                    "Note that the original timestamps should be preserved for each entry.\n\n"

                    
                    "Subtitles to process:\n"
                    f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

                    "KEEP the 漢字/katakana in angle brackets like '<漢字/katakana>' as original. "
                    "Add the furigana ふりがな inside the square brackets like '[ふりがな]'. "

                    
                    "Output JSON format only:\n"
                    "```json\n"
                    "[\n"
                    "  {\n"
                    "    \"start\": \"timestamp\",\n"
                    "    \"end\": \"timestamp\",\n"
                    "    \"ja\": \"Japanese text with furigana format\",\n"
                    "  }\n"
                    "]\n"
                    "```"
                )

                ai_response_ja = None
                if self.use_cache:
                    ai_response_ja = self.load_latest_translation_attempt(lang="furigana", idx=idx)
                
                if not self.use_cache or not ai_response_ja:

                    
                    messages_ja[1]["content"] = prompt_content_ja
                    

                    response_ja = self.client.chat.completions.create(
                        model=os.environ.get("OPENAI_MODEL", "gpt-4-0125-preview"),
                        messages=messages_ja
                    )

                    ai_response_ja = response_ja.choices[0].message.content.strip()

                translated_subtitles_ja = self.extract_and_parse_json(ai_response_ja)
                self.validate_translated_subtitles(translated_subtitles_ja, required_fields=["start", "end", "ja"])
                self.save_translation_attempt(prompt_content_ja, ai_response_ja, lang="furigana", idx=idx)

                return translated_subtitles_ja
            except (JSONParsingError, JSONValidationError) as e:
                self.use_cache = False

                print(f"Attempt {retries + 1} failed: {e}")

                # Append the response and error message for context
                messages_ja.append({"role": "system", "content": ai_response_ja})
                messages_ja.append({"role": "user", "content": e.message})
                
                retries += 1
                
                if retries >= self.max_retries:
                    # # Check if the processed_sub_path exists and remove it if it does
                    # if os.path.exists(self.output_sub_path):
                    #     os.remove(self.output_sub_path)

                    # # Now, safely create a hard link
                    # try:
                    #     os.link(self.input_sub_path, self.output_sub_path)
                    # except OSError as e:
                    #     print(f"Error creating hard link: {e}")

                    raise Exception("Failed after maximum retries. ")

    # def save_translated_subtitles_to_srt(self, translated_subtitles):
    #     """Save the translated subtitles to an SRT file."""
    #     srt_content = ""
    #     for index, subtitle in enumerate(translated_subtitles, start=1):
    #         srt_content += f"{index}\n{subtitle['start']} --> {subtitle['end']}\n{subtitle['zh']}\n{subtitle['en']}\n\n"
    #     with open(self.output_sub_path, 'w', encoding='utf-8') as file:
    #         file.write(srt_content)

    def save_translated_subtitles_to_srt(self, translated_subtitles):
        """Save the translated subtitles to an SRT file, ensuring language order."""
        srt_content = ""
        for index, subtitle in enumerate(translated_subtitles, start=1):
            # Start the subtitle entry with its sequence number and time range
            srt_content += f"{index}\n{subtitle['start']} --> {subtitle['end']}\n"
            
            # Always add Chinese (zh) translation first if it exists
            if 'zh' in subtitle:
                srt_content += f"{subtitle['zh']}\n"
            
            # Add English (en) translation
            if 'en' in subtitle:
                srt_content += f"{subtitle['en']}\n"
            
            # Add any additional languages present, excluding 'start', 'end', 'zh', and 'en'
            additional_languages = {k: v for k, v in subtitle.items() if k not in ['start', 'end', 'zh', 'en']}
            for lang_code, text in additional_languages.items():
                srt_content += f"{text}\n"
            
            # Separate subtitles with an empty line
            srt_content += "\n"
        
        # Write the constructed SRT content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(srt_content)


    def process_subtitles(self):
        """Main process to load, translate, merge, and save subtitles."""
        subtitles = self.load_subtitles_from_json()
        translated_subtitles = self.translate_and_merge_subtitles(subtitles)
        # self.save_translated_subtitles_to_srt(translated_subtitles)
        self.save_translated_subtitles_to_ass(translated_subtitles)
        print("Subtitles have been processed and saved successfully.")

    # @staticmethod
    # def convert_furigana_to_ass(ja_text):
    #     """Convert furigana format from <kanji>[furigana] to ASS ruby format with kanji in almost white with a hint of blue and furigana in salmon and larger size."""
    #     def replace_with_ruby(match):
    #         kanji, furigana = match.groups()
    #         # Apply the Kanji style for kanji and the Furigana style for furigana
    #         return f"{{\\rKanji}}{kanji}{{\\rFurigana}}{furigana}{{\\rDefault}}"
    #         # return f"{{\\rFurigana}}{furigana}{{\\rKanji}}{kanji}{{\\rDefault}}"
    #     # Updated regex to match the new format <kanji>[furigana]
    #     return re.sub(r"<([^>]+)>\[([^]]+)\]", replace_with_ruby, ja_text)

    # @staticmethod
    # def convert_furigana_to_ass(ja_text):
    #     """Convert furigana format from <kanji>[furigana] or standalone [furigana] to ASS ruby format,
    #     applying styles for kanji and furigana where present."""
    #     def replace_with_ruby(match):
    #         kanji, furigana = match.groups()
    #         # Check if kanji is None (which means only [furigana] was present)
    #         if kanji is None:
    #             # Format just the furigana with the Furigana style
    #             return f"{{\\rFurigana}}{furigana}{{\\rDefault}}"
    #         else:
    #             # Format with both Kanji and Furigana styles
    #             return f"{{\\rKanji}}{kanji}{{\\rFurigana}}{furigana}{{\\rDefault}}"

    #     # Updated regex to match both <kanji>[furigana] and standalone [furigana]
    #     # The kanji part is made optional by including '?' after the non-capturing group for kanji
    #     pattern = r"(?:<([^>]+)>)?\[([^]]+)\]"
    #     return re.sub(pattern, replace_with_ruby, ja_text)

    # def clean_redundant_hiragana_sequence(self, ja_text):
    #     """
    #     Removes angle brackets when the content inside <> is identical to the content inside the immediately following [].
    #     """
        
    #     # Define a regex pattern to find <content>[content] and capture the content
    #     pattern = re.compile(r'<([^>]+)>\[\1\]')
        
    #     # Replace found patterns with just the content in square brackets
    #     simplified_text = pattern.sub(r'[\1]', ja_text)
        
    #     return simplified_text

    def rearrange_brackets(self, ja_text):
        """
        Detects and rearranges text where [] are found inside <> to follow the <>[] pattern.
        """
        
        # Define a regex pattern to match <...[...]...>
        pattern = re.compile(r'<([^>\[\]]+)\[([^\[\]]+)\]([^>\[\]]*)>')
        
        # Function to rearrange the matched pattern to <...> [...]
        def rearrange(match):
            # Extract the parts of the match
            before_bracket = match.group(1)
            inside_bracket = match.group(2)
            after_bracket = match.group(3)
            
            # Return the rearranged format
            return f'<{before_bracket}{after_bracket}>[{inside_bracket}]'

        # Apply the rearrange function to all matching patterns in the text
        rearranged_text = pattern.sub(rearrange, ja_text)

        return rearranged_text

    def remove_preceding_repetition(self, ja_text):
        """
        Removes a word or phrase that is repeated immediately before <...> when
        the content inside <> is the same as the preceding word.
        """
        
        # Define a regex pattern to match repetition before <>
        pattern = re.compile(r'(\S+)<\1>')
        
        # Function to replace the matched pattern with just the content in angle brackets
        def deduplicate(match):
            # Return only the content within angle brackets
            return f'<{match.group(1)}>'

        # Apply the deduplication function to all matching patterns in the text
        deduplicated_text = pattern.sub(deduplicate, ja_text)

        return deduplicated_text


    def clean_triplicated_sequences(self, ja_text):
        """
        Simplifies text by converting sequences of the form same<same>[same] to just 'same'.
        """
        # Define a regex pattern to match the 'same<same>[same]' structure
        pattern = re.compile(r'(?:(\S+)<\1>\[\1\])')
        
        # Function to perform the substitution
        def replacement(match):
            # Return just the first 'same' part of the match
            return match.group(1)
        
        # Remove same hiragana<same hiragana>[ same hiragana] anywhere
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text



    


    def clean_duplicated_kanji_hiragana_sequence(self, ja_text):
        """
        Converts sequences of the form 'same kanji<same kanji>[hiragana]' to '<kanji>[hiragana]'.
        """
        # Define a regex pattern to match the specified structure
        pattern = re.compile(r'([一-龠ァ-ヶ々ー]+)(<\1>)\[([ぁ-ん]+)\]')
        
        # Function to perform the substitution
        def replacement(match):
            # for i in range(4):
            #     print(match.group(i))
            
            # Return the simplified structure '<kanji>[hiragana]'
            kanji = match.group(2)
            hiragana = match.group(3)
            return f'{kanji}[{hiragana}]'
        
        # Convert kanji<kanji>[hiragana] to <kanji>[hiragana], preserving the sequence (two kanji seq is the same)
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text

    def clean_redundant_hiragana_sequence(self, ja_text):
        """
        Modifies text by removing redundant angle and square brackets for matching hiragana or kanji sequences.
        - For hiragana immediately before or at the start, followed by <hiragana>[hiragana], it leaves just the hiragana.
        - For kanji followed by <hiragana>[hiragana], it replaces them with [hiragana].
        """

        

        # Remove <same hiragana>[same hiragana] when at the start or directly after punctuation or preceded by different hiragana
        # hiragana_pattern = re.compile(r'(^|(?<=[ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        hiragana_pattern = re.compile(r'(^|(?<=[、。！？>\]ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        simplified_text = hiragana_pattern.sub(r'\1\2', ja_text)
        
        # Convert kanji<hiragana>[hiragana] to kanji[hiragana], preserving the sequence
        kanji_hiragana_pattern = re.compile(r'(?<=[一-龠ァ-ヶ々ー])<([ぁ-ん]+)>\[\1\]')
        simplified_text = kanji_hiragana_pattern.sub(r'[\1]', simplified_text)

        
        
        return simplified_text

    def clean_duplicated_hiragana_inside_angle_brackets(self, ja_text):
        """
        Simplifies sequences of the form 'same hiragana<same hiragana>' to just 'hiragana'.
        """
        # Define a regex pattern to match hiragana sequences repeated before and inside angle brackets
        pattern = re.compile(r'<?([ぁ-ん]+)>?<\1>')

        
        # Perform the substitution to replace matched patterns with just the hiragana
        simplified_text = pattern.sub(r'\1', ja_text)

        return simplified_text


    def convert_standalone_angle_to_square_brackets(self, ja_text):
        """
        Converts standalone angle brackets to square brackets in the given text,
        while leaving <kanji>[furigana] pairs unchanged.
        """
        
        # Define a regex pattern to find standalone <> not followed directly by []
        # This pattern assumes 'standalone' means there's no furigana in square brackets immediately following
        pattern = re.compile(r'<([^>]+)>(?!\[)')
        
        # Replace found patterns with square brackets
        converted_text = pattern.sub(r'[\1]', ja_text)
        
        return converted_text






    # # Example usage
    # original_text = "<字幕>[じまく]上[じょう]でこの<日本語>[にほんご]の発音[はつおん]を学[まな]べるようにしています。"
    # preprocessed_text = preprocess_text_for_furigana(original_text)

    # print("Preprocessed Text:", preprocessed_text)

    def katakana_to_hiragana(self, katakana):
        """
        Converts a katakana string to hiragana.
        """
        katakana_hiragana_map = str.maketrans(
            "アイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワヲンガギグゲゴザジズゼゾダヂヅデドバビブベボパピプペポ",
            "あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもやゆよらりるれろわをんがぎぐげござじずぜぞだぢづでどばびぶべぼぱぴぷぺぽ"
        )
        return katakana.translate(katakana_hiragana_map)

    def fill_blank_of_katakana_without_furigana(self, ja_text):
        """
        Converts sequences of the form <katakana>[] to <katakana>[hiragana],
        where hiragana is the equivalent of the katakana text inside the angle brackets.
        """
        # Define a regex pattern to match <katakana>[] structures
        pattern = re.compile(r'<([ァ-ヶー]+)>\[\]')
        
        # Function to perform the substitution
        def replacement(match):
            # Convert the matched katakana to hiragana
            katakana_text = match.group(1)
            hiragana_text = self.katakana_to_hiragana(katakana_text)
            return f'<{katakana_text}>[{hiragana_text}]'
        
        # Apply the replacement function to all matching patterns in the text
        converted_text = pattern.sub(replacement, ja_text)

        return converted_text

    def convert_katakana_in_brackets_to_hiragana(self, ja_text):
        """
        Detects <Katakana>[Katakana] then converts the [Katakana] into [Hiragana].
        """
        # # Define a regex pattern to match <Katakana>[Katakana]
        # pattern = re.compile(r'<([ァ-ヶー]+)>\[\1\]')

        # Define a regex pattern to match both <Katakana>[Katakana] and Katakana[Katakana]
        pattern = re.compile(r'(?:(<([ァ-ヶー]+)>)|([ァ-ヶー]+))\[(\2|\3)\]')

        # # Use the katakana_to_hiragana function to convert matched katakana to hiragana in []
        # corrected_text = re.sub(pattern, lambda m: f'<{m.group(1)}>[{katakana_to_hiragana(m)}]', ja_text)

        def replacement(match):
            # Determine if the match includes angle brackets or not
            katakana = match.group(2) if match.group(2) else match.group(3)
            hiragana = self.katakana_to_hiragana(katakana)
            # Reconstruct the string with the katakana in brackets converted to hiragana
            if match.group(1):  # If katakana was enclosed in angle brackets
                return f'<{katakana}>[{hiragana}]'
            else:  # If katakana was not enclosed in angle brackets
                return f'{katakana}[{hiragana}]'

        # Use the replacement function to convert matched katakana to hiragana in []
        corrected_text = pattern.sub(replacement, ja_text)

        return corrected_text

    # @staticmethod
    # def preprocess_text_for_furigana(self, ja_text):
    #     """
    #     Adjusts Japanese text to ensure that kanji/katakana sequences directly preceding standalone furigana
    #     are enclosed in <>, without altering or removing any unassociated hiragana or other characters.
    #     """
        
    #     # Define a regex pattern that captures sequences to be enclosed based on the presence of furigana.
    #     # This pattern aims to avoid capturing leading hiragana that's not part of the kanji/katakana sequence for furigana.
        

    #     pattern = re.compile(r'(?<!<)([一-龠ァ-ヶ々ー]+)(?=\[([^\]]+)\])')

    #     # Function to enclose matched kanji/katakana sequence in <>
    #     def enclose_kanji_katakana(match):
    #         kanji_katakana = match.group(1)  # The kanji/katakana sequence
    #         return f'<{kanji_katakana}>'

    #     # Apply the enclosure function to all appropriate sequences in the text
    #     preprocessed_text = pattern.sub(enclose_kanji_katakana, ja_text)

    #     # Return the modified text with appropriate enclosures
    #     return preprocessed_text
    def preprocess_text_for_furigana(self, ja_text):
        """
        Adjusts Japanese text to ensure that kanji/katakana sequences directly preceding standalone furigana
        are enclosed in <>, without altering or removing any unassociated hiragana or other characters.
        This version stops including characters in the sequence based on the starting character type (kanji or katakana).
        """
        
        # This pattern is updated to differentiate between sequences starting with kanji or katakana.
        # It ensures that if a sequence starts with kanji, it only includes kanji,
        # and if it starts with katakana, it only includes katakana, directly preceding the furigana annotation.
        # pattern = re.compile(
        #     r'(?<!<)'
        #     r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
        #     r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        # )
        pattern = re.compile(
            r'(?<!<)'
            r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
            r'([ぁ-ん]{0,2})'  # Optionally matches zero to two hiragana characters
            r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        )

        # Function to enclose matched kanji/katakana sequence in <>
        def enclose_kanji_katakana(match):
            kanji_katakana = match.group(1)  # The kanji/katakana sequence
            return f'<{kanji_katakana}>'

        # Apply the enclosure function to all appropriate sequences in the text
        preprocessed_text = pattern.sub(enclose_kanji_katakana, ja_text)

        # Return the modified text with appropriate enclosures
        return preprocessed_text

    # @staticmethod
    def convert_furigana_to_ass(self, ja_text):
        """Convert furigana format from <kanji>[furigana], standalone [furigana], or <kanji> to ASS ruby format,
        applying styles for kanji and furigana where present."""


        

        def replace_with_ruby(match):
            # Adjusted to handle both standalone and combined cases properly.
            kanji = match.group(1)   # Kanji could be in group 1 when present
            furigana = match.group(2 )or match.group(3)  # Furigana is in group 2  or standalone furigana in group 3
            
            # Initialize an empty string for the result
            result = ""
            
            # Format kanji and furigana if present
            if kanji and furigana:
                result = f"{{\\rKanji}}{kanji}{{\\rFurigana}}{furigana}{{\\rDefault}}"
            elif kanji:
                result = f"{{\\rKanji}}{kanji}{{\\rDefault}}"
            elif furigana:
                result = f"{{\\rFurigana}}{furigana}{{\\rDefault}}"
            
            return result

        # Adjusted regex to correctly match <kanji>[furigana], standalone [furigana], or <kanji>
        pattern = r"<([^>]+)>(?:\[(.*?)\])?|\[([^\]]+)\]"

        # Use lambda to pass match object directly to replace_with_ruby
        return re.sub(pattern, replace_with_ruby, ja_text)

    def estimate_character_width(self, text):
        half_width_count = 0
        full_width_count = 0

        for char in text:
            # Ordinal value of the character
            ord_char = ord(char)
            # Simple checks for half-width vs. full-width character ranges
            if 0x0020 <= ord_char <= 0x007E or 0xFF61 <= ord_char <= 0xFFDC or 0xFFA0 <= ord_char <= 0xFFBE:
                half_width_count += 1
            elif 0x1100 <= ord_char <= 0x11FF or 0x2E80 <= ord_char <= 0x9FFF or 0xAC00 <= ord_char <= 0xD7AF or 0xFF01 <= ord_char <= 0xFF60 or 0xFFE0 <= ord_char <= 0xFFE6:
                full_width_count += 1

        # Determine the predominant character width in the text
        if half_width_count > full_width_count:
            return "half-width"
        else:
            return "full-width"


    # @staticmethod
    def generate_ass_header(self):
        """Generates the header for an ASS file, adjusting `PlayResX` and `PlayResY` based on video orientation."""
        # Determine if the video is landscape or portrait
        # is_video_landscape = self.video_width > self.video_height
        is_video_landscape = self.is_video_landscape

        # Adjust PlayResX and PlayResY based on orientation
        # play_res_x, play_res_y = (self.base_width, self.base_height) if is_video_landscape else (self.base_height, self.base_width)
        play_res_x, play_res_y = (self.video_width, self.video_height) if is_video_landscape else (self.video_height, self.video_width)

        # wrapping_limit_half_width_default = self.wrapping_limit_half_width_default
        # wrapping_limit_full_width_default = self.wrapping_limit_half_width_default // 2

        max_width = self.video_width * 0.8

        if self.is_video_landscape:
            max_height = self.video_height * 0.5 / 8
        else:
            max_height = self.video_height * 0.5 / 16

        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
        # english_portrait_font_size = find_font_size(sample_texts["english"][:wrapping_limit_half_width_default], font_path, max_width, max_height)


        font_sizes = {}
        for language, text in sample_texts.items():
            char_width_type = self.estimate_character_width(text)
            wrapping_limit = self.wrapping_limit_half_width_default # if char_width_type == 'half-width' else self.wrapping_limit_half_width_default // 2
            text_section = text[:wrapping_limit]
            font_sizes[language] = find_font_size(text_section, self.font_path, max_width, max_height)


        print("calculated font size: ")
        pprint(font_sizes)


        # rescale = 4  # Scaling factor

        # # Adjust base font sizes
        # base_font_size = 24 * rescale if is_video_landscape else 20 * rescale  # Larger for landscape
        # furigana_font_size = 20 * rescale if is_video_landscape else 18 * rescale
        # arabic_font_size = 26 * rescale if is_video_landscape else 22 * rescale  # Specific for Arabic

        

        # # Calculate scale factors (assuming base resolution is for landscape; adjust if your baseline is portrait)
        # scale_factor = min(self.video_width / play_res_x, self.video_height / play_res_y)

        # # scale_factor = np.power(scale_factor, 4/5)

        # # Apply scale factor to font sizes
        # base_font_size = int(base_font_size * scale_factor)
        # furigana_font_size = int(furigana_font_size * scale_factor)


        base_font_size = font_sizes["English"]
        chinese_font_size = font_sizes["Chinese"]
        english_font_size = font_sizes["English"]
        japanese_font_size = font_sizes["Japanese"]
        kanji_font_size = font_sizes["Japanese"]
        furigana_font_size = font_sizes["Japanese"]
        arabic_font_size = font_sizes["Arabic"]
        

        return f"""[Script Info]
ScriptType: v4.00+
Collisions: Normal
PlayResX: {play_res_x}
PlayResY: {play_res_y}

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Vernada,{base_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: English,Vernada,{english_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chinese,Vernada,{chinese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Japanese,Vernada,{japanese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Kanji,Vernada,{kanji_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Furigana,Vernada,{furigana_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Arabic,Arial,{arabic_font_size},&H00FACE87,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1


[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""             


    def save_translated_subtitles_to_ass(self, translated_subtitles):
        
        """Save the translated subtitles to an ASS file with text wrapping based on video orientation."""
        

        wrapping_limit_half_width_default = self.wrapping_limit_half_width_default

        wrapping_limit_half_width_ja = wrapping_limit_half_width_default

        # if self.is_video_landscape:
        #     wrapping_limit_half_width_ja = wrapping_limit_half_width_default
        # else:
        #     wrapping_limit_half_width_ja = int(wrapping_limit_half_width_default * 1.)

        # wrapping_limit_half_width_zh = wrapping_limit_half_width_default * 1.5
        # wrapping_limit_half_width_en = wrapping_limit_half_width_default * 1.5
        # wrapping_limit_half_width_ar = wrapping_limit_half_width_default * 1.5

        ass_content = self.generate_ass_header()
        for index, item in enumerate(translated_subtitles, start=1):
            # Convert HH:MM:SS,mmm format to seconds
            def convert_to_seconds(t):
                parts = re.split('[:|,]', t)
                if len(parts) == 4:
                    h, m, s, ms = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
                elif len(parts) == 3:
                    # Fallback in case the format is not as expected
                    h, m, s = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + float(s.replace(',', '.'))
                else:
                    raise Exception(f"The format of timestamps is not recognized: {t}. ")
                
                return total_seconds
            
            # Format start and end times to ensure two decimal places with comma
            start_seconds = convert_to_seconds(item['start'])
            end_seconds = convert_to_seconds(item['end'])
            start = "{:02d}:{:02d}:{:05.2f}".format(int(start_seconds // 3600), int((start_seconds % 3600) // 60), start_seconds % 60)#.replace('.', ',')
            end = "{:02d}:{:02d}:{:05.2f}".format(int(end_seconds // 3600), int((end_seconds % 3600) // 60), end_seconds % 60)#.replace('.', ',')

            dialogue_lines = []

            # Process languages in a specific order if needed, then all additional languages
            preferred_order = ['zh', 'en', 'ja', "ar"]  # Example: Start with Chinese, then English, then Japanese
            handled_keys = set(preferred_order)

            # Add preferred languages first
            for lang in preferred_order:
                if lang in item:
                    text = item[lang]
                    
                    is_cjk = lang in ['zh', 'ja']  # Assuming CJK for Chinese and Japanese
                    
                    if lang == "ja":
                        wrapping_limit_half_width = wrapping_limit_half_width_ja

                        # Switch to control printing
                        enable_print = False

                        def custom_print(message, text):
                            if enable_print:
                                print(message, text)

                        ja_text = text
                        custom_print("Initial text:", ja_text)

                        ja_text = self.rearrange_brackets(ja_text)
                        custom_print("After rearranging brackets:", ja_text)

                        ja_text = self.fill_blank_of_katakana_without_furigana(ja_text)
                        custom_print("After filling in the blank of <katakana>[]:", ja_text)

                        ja_text = self.convert_katakana_in_brackets_to_hiragana(ja_text)
                        custom_print("After converting katakana in brackets to hiragana:", ja_text)

                        ja_text = self.clean_triplicated_sequences(ja_text)
                        custom_print("After simplifying triplicated sequences:", ja_text)

                        ja_text = self.clean_duplicated_kanji_hiragana_sequence(ja_text)
                        custom_print("After converting duplicated kanji-hiragana sequence:", ja_text)

                        ja_text = self.clean_redundant_hiragana_sequence(ja_text)
                        custom_print("After removing redundant hiragana sequence:", ja_text)

                        ja_text = self.clean_duplicated_hiragana_inside_angle_brackets(ja_text)
                        custom_print("After simplifying duplicated hiragana inside angle brackets:", ja_text)

                        ja_text = self.convert_standalone_angle_to_square_brackets(ja_text)
                        custom_print("After converting standalone angle to square brackets:", ja_text)

                        ja_text = self.preprocess_text_for_furigana(ja_text)
                        custom_print("After preprocessing text for furigana:", ja_text)

                        text = ja_text
                        custom_print("Final text:", text)


                    else:
                        wrapping_limit_half_width = wrapping_limit_half_width_default


                    wrapped_text_lines = self.wrap_text(text, wrapping_limit_half_width, is_cjk=is_cjk)
                    dialogue_line = '\\N'.join(wrapped_text_lines)

                    # if "歴史" in dialogue_line:
                    #         print("ja_text: ", ja_text)
                    #         pprint(wrapped_text_lines)

                    # if lang == 'ja':
                    #     dialogue_lines.append(self.convert_furigana_to_ass(item[lang]))
                    # else:
                    #     dialogue_lines.append(item[lang])

                    if lang == 'ja':
                        dialogue_line = self.convert_furigana_to_ass(dialogue_line)

                    # Specify the style for Arabic subtitles and apply it directly in the dialogue line
                    # style = "Arabic" if lang == 'ar' else "Default"

                    if lang == "ar":
                        style = "Arabic"
                    elif lang == "zh":
                        style = "Chinese"
                    elif lang == "en":
                        style = "English"
                    elif lang == "ja":
                        style = "Japanese"
                    else:
                        style = "Default"

                    subtitle_line = f"Dialogue: 0,{start},{end},{style},,0,0,0,,{dialogue_line}"
                    ass_content += subtitle_line + "\n"

            # Add any additional languages present
            additional_languages = {k: v for k, v in item.items() if k not in handled_keys.union(['start', 'end'])}
            for lang_code, text in additional_languages.items():
                # Additional languages will use the default style as this block does not handle style differentiation
                dialogue_line = self.wrap_text(text, wrapping_limit_half_width_default, is_cjk=False)
                formatted_text = '\\N'.join(dialogue_line)
                subtitle_line = f"Dialogue: 0,{start},{end},Default,,0,0,0,,{formatted_text}"
                ass_content += subtitle_line + "\n"

            #         # # Specify the style for Arabic subtitles
            #         # style = "Arabic" if lang == 'ar' else "Default"
            #         # if lang == 'ar':
            #         #     # dialogue_line = f"Dialogue: 0,{start},{end},{style},,0,0,0,,{dialogue_line}\n"
            #         #     dialogue_line = f"{style}:{dialogue_line}"


            #         dialogue_lines.append(dialogue_line)

            # # Add any additional languages present
            # additional_languages = {k: v for k, v in item.items() if k not in handled_keys.union(['start', 'end'])}
            # for lang_code, text in additional_languages.items():
            #     dialogue_lines.append(text)

            # # Format the dialogue line
            # formatted_text = '\\N'.join(dialogue_lines)  # New line in ASS format
            # subtitle = f"Dialogue: 0,{start},{end},Default,,0,0,0,,{formatted_text}\n"
            # ass_content += subtitle

        # Write the constructed ASS content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(ass_content)
        print(f"Subtitles have been processed and saved successfully to {self.output_sub_path}.")

    # def wrap_text(self, text, wrapping_limit_half_width, is_cjk=False):

    #     # is_video_landscape = self.video_width > self.video_height
    #     is_video_landscape = self.is_video_landscape

    #     """Wrap text with special handling for Japanese furigana annotations, aiming to minimize the maximum length of any line."""
    #     if not is_cjk:
    #         # For non-CJK text
    #         return self.cjkwrap_punctuation(text, wrapping_limit_half_width)

        
    #     # Step 1: Wrap the text into lines
    #     wrapped_lines = self.cjkwrap_punctuation(text, wrapping_limit_half_width)  # Replace textwrap.wrap with cjkwrap.wrap in your environment

    #     # Step 2: Join lines with a special marker ('###') to easily identify original line breaks
    #     joined_text = '###'.join(wrapped_lines)

    #     # Step 3: Correct breakpoints within structured texts
    #     pattern = r'(<[^>]*>\[[^\]]*\]|<[^>]*>|\[[^\]]*\])'

    #     corrected_text = re.sub(pattern, lambda m: m.group(0).replace('###', ''), joined_text)

    #     # Split the text back into lines temporarily to manipulate structured text placement
    #     temp_lines = corrected_text.split('###')

    #     # Step 4: Refine placement of structured texts by evaluating line length differences
    #     final_lines = []
    #     for i, line in enumerate(temp_lines):
    #         if i + 1 < len(temp_lines):  # Ensure there's a next line to compare with
    #             next_line = temp_lines[i + 1]
    #             # Find structured text at the end of the current line or beginning of the next line
    #             structured_text_end = re.search(r'(<[^>]*>\[[^\]]*\]|<[^>]*>|\[[^\]]*\])$', line)
    #             structured_text_start = re.search(r'^(<[^>]*>\[[^\]]*\]|<[^>]*>|\[[^\]]*\])', next_line)

    #             if structured_text_end and structured_text_start:
    #                 # Extract structured text
    #                 structured_text = structured_text_end.group(0)
    #                 # Compare line lengths to decide placement
    #                 option_end = len(line) - len(structured_text)
    #                 option_start = len(next_line) + len(structured_text)
    #                 # Choose the option that minimizes the length difference
    #                 if abs(option_end - len(next_line)) <= abs(len(line) - option_start):
    #                     # Keep the structured text at the end of the current line
    #                     final_lines.append(line)
    #                 else:
    #                     # Move the structured text to the beginning of the next line
    #                     final_lines.append(line[:-len(structured_text)])
    #                     temp_lines[i + 1] = structured_text + next_line
    #                     continue  # Skip appending next_line in this iteration
    #             else:
    #                 final_lines.append(line)
    #         else:
    #             final_lines.append(line)  # Append last line without comparison

    #     # Step 5: Optionally re-wrap lines if needed to ensure they adhere to the width constraint
    #     # This step is skipped in this solution for simplicity and based on the approach description

    #     return final_lines

    def wrap_text(self, text, wrapping_limit_half_width, is_cjk=False):
        if not is_cjk:
            # For non-CJK text, directly use the wrapping function.
            # return self.cjkwrap_punctuation(text, wrapping_limit_half_width)
            return [text]

        # Step 1: Wrap the text into lines.
        wrapped_lines = self.cjkwrap_punctuation(text, wrapping_limit_half_width)

        # Step 2: Join lines with '###' to mark original line breaks.
        joined_text = '###'.join(wrapped_lines)

        # if "歴史" in joined_text:
        #     print("joined_text: ", joined_text)

        # Step 3: Correct breakpoints only for structured texts that were split.
        def correct_breakpoints(match):
            structured_text = match.group(0)

            # if "歴史" in joined_text:
            #     for i in range(4):
            #         try:
            #             print(match.group(i))
            #         except:
            #             pass

            # If '###' is inside the structured text, it indicates an incorrect break.
            if '###' in structured_text:
                # Remove '###' and keep the structure intact.
                return "###" + structured_text.replace('###', '') + "###"
            else:
                # If no '###' inside, return the structured text as is.
                return structured_text

        # pattern = r'(<[^>]*>\[[^\]]*\]|<[^>]*>|\[[^\]]*\])'
        # pattern = r'(<[^>]*>\[[^\]]*\])'
        pattern = r'(<[^>]*>)(###)?\[[^\]]*\]|<[^>]*>|(\[[^\]]*\])'
        corrected_text = re.sub(pattern, correct_breakpoints, joined_text)
        # pattern = r'(<[^>]*>|\[[^\]]*\])'
        # corrected_text = re.sub(pattern, correct_breakpoints, joined_text)

        # if "歴史" in joined_text:
        #     print("corrected_text: ", corrected_text)

        # Step 4: Split the text back into lines at '###'.
        corrected_lines = corrected_text.split('###')

        joined_lines = self.join_lines_with_length_check(corrected_lines, wrapping_limit_half_width)

        # return corrected_lines
        return joined_lines

    @staticmethod
    def strip_brackets(input_string):
        # Regular expression pattern to match <, >, [, and ]
        pattern = r'[\<\>\[\]]'
        # Replace matched characters with an empty string
        stripped_string = re.sub(pattern, '', input_string)
        return stripped_string

    def join_lines_with_length_check(self, lines, wrapping_limit_half_width):
        final_lines = []
        current_line = ""
        for line in lines:
            if current_line:
                # Attempt to join with the next line and check if it exceeds the wrapping limit.
                test_line = current_line + line
                wrapped_test = self.cjkwrap_punctuation(self.strip_brackets(test_line), wrapping_limit_half_width)
                if len(wrapped_test) > 1:
                    # If joining exceeds the limit, finalize the current line and start a new one.
                    final_lines.append(current_line)
                    current_line = line
                else:
                    # Otherwise, update the current line to include the next line.
                    current_line = test_line
            else:
                current_line = line

        # Ensure the last accumulated line is added to the final output.
        if current_line:
            final_lines.append(current_line)

        return final_lines


    def cjkwrap_punctuation(self, text, width):
        """
        Custom wrapper for CJK text that prioritizes wrapping at punctuation,
        and adjusts lines to avoid starting with punctuation.
        """


        wrapped_lines = cjkwrap.wrap(text, width)

        # return wrapped_lines


        # Define full-width and half-width punctuation marks for CJK text
        punctuations = ".。、，,！!？?；;：:「」『』（）()【】[]《》<>「」『』“”\"\""
        # punctuations = ".。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""
        # punctuations = "。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""

        # is_video_landscape = self.video_width > self.video_height

        # if self.is_video_landscape:
        #     # punctuations = "。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""  
        #     punctuations = "。！!？?；;：:"  
        # else:
        #     # punctuations = "。、，,！!？?；;：:「」『』（）()【】[]《》<>「」『』“”\"\""
        #     # punctuations = "。！!？?；;：:[]<>"
        #     # punctuations = "。！!？?；;：:[]<>"
        #     punctuations = "。！!？?；;：:"



        # segments = []
        # start = 0

        # # Split text at punctuation marks
        # for i, char in enumerate(text):
        #     if char in punctuations:
        #         # Include punctuation in the segment
        #         segment = text[start:i + 1]
        #         if segment:
        #             segments.append(segment)
        #         start = i + 1
        # # Add the last segment if there's any
        # if start < len(text):
        #     segments.append(text[start:])

        # wrapped_lines = []
        # for segment in segments:
        #     # Wrap each segment with cjkwrap
        #     wrapped_segment = cjkwrap.wrap(segment, width)
        #     wrapped_lines.extend(wrapped_segment)

        # Adjust lines to move punctuation from the beginning of a line to the end of the previous line
        for i in range(1, len(wrapped_lines)):
            if wrapped_lines[i][0] in punctuations:
                # Move punctuation to the end of the previous line if it doesn't exceed width
                if len(wrapped_lines[i-1]) + 1 <= width:
                    wrapped_lines[i-1] += wrapped_lines[i][0]  # Move punctuation to the end of the previous line
                    wrapped_lines[i] = wrapped_lines[i][1:]  # Remove punctuation from the current line

        # Handle case where the first line starts with punctuation and there's no previous line to adjust
        # This might involve a specific strategy, such as re-wrapping with cjkwrap if needed
        # For simplicity, this case is not explicitly handled here but should be considered based on your requirements

        return wrapped_lines



if __name__ == '__main__':
    
    # Example usage:
    input_json_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.json'
    output_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/translated_subtitles.srt'
    input_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.srt'


    openai_client = OpenAI()
    subtitles_processor = SubtitlesTranslator(openai_client, input_json_path, input_sub_path, output_sub_path)
    subtitles_processor.process_subtitles()

===== ./words_card.py =====
import subprocess
import cv2
import tempfile
import shutil
import os
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip

from PIL import Image

def overlay_word_card_on_cover(words_card_path, cover_path, output_path, transparency=0.5):
    # Open the word card and the cover images
    words_card_img = Image.open(words_card_path).convert("RGBA")
    cover_img = Image.open(cover_path).convert("RGBA")

    # Calculate the scaling factor to maintain aspect ratio
    aspect_ratio_word_card = words_card_img.width / words_card_img.height
    aspect_ratio_cover = cover_img.width / cover_img.height

    if aspect_ratio_word_card > aspect_ratio_cover:
        # Fit to width
        new_width = cover_img.width
        new_height = int(new_width / aspect_ratio_word_card)
    else:
        # Fit to height
        new_height = int(0.7 * cover_img.height)
        new_width = int(new_height * aspect_ratio_word_card)

    # Resize word card image
    words_card_img_resized = words_card_img.resize((new_width, new_height), Image.LANCZOS)

    # Set transparency
    words_card_img_resized.putalpha(int(255 * transparency))

    # Calculate position to center the word card on the cover
    x_position = (cover_img.width - new_width) // 2
    y_position = (cover_img.height - new_height) // 2

    # Create a transparent image for compositing
    transparent_img = Image.new("RGBA", cover_img.size, (0, 0, 0, 0))
    
    # Paste the word card image onto the transparent image
    transparent_img.paste(words_card_img_resized, (x_position, y_position), words_card_img_resized)

    # Create a composite image
    combined_img = Image.alpha_composite(cover_img, transparent_img)

    # Save the result
    combined_img = combined_img.convert("RGB")  # Convert back to RGB to save as JPG or other formats
    combined_img.save(output_path, quality=100)

class VideoAddWordsCard:
    def __init__(self, video_path, image_path, duration=3):
        self.video_path = video_path
        self.image_path = image_path
        self.output_dir = os.path.dirname(video_path)
        self.duration = duration

        self.width, self.height = self.get_video_info()

    def get_video_info(self):
        video = cv2.VideoCapture(self.video_path)
        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        video.release()
        return width, height

    @property
    def is_video_landscape(self):
        return self.width > self.height


    def correct_video_metadata(self, correct_width, correct_height, tmp_folder):
        filename, file_extension = os.path.splitext(os.path.basename(self.video_path))
        corrected_video_path = os.path.join(tmp_folder, f"corrected_{filename}{file_extension}")
        cmd = f"ffmpeg -y -i \"{self.video_path}\" -vf scale={correct_width}:{correct_height} -c:a copy \"{corrected_video_path}\""
        subprocess.call(cmd, shell=True)
        return corrected_video_path

    def add_image_to_video(self):
        with tempfile.TemporaryDirectory() as tmp_folder:
            video_width, video_height = self.get_video_info()
            corrected_video_path = self.correct_video_metadata(video_width, video_height, tmp_folder)
            video_clip = VideoFileClip(corrected_video_path)

            image_clip = ImageClip(self.image_path).set_duration(min(self.duration, video_clip.duration))
            aspect_ratio_image = image_clip.h / image_clip.w

            if self.is_video_landscape:
                new_width = int(video_width * 0.7)
            else:
                new_width = video_width
            
            new_height = int(new_width * aspect_ratio_image)
            # image_clip = image_clip.resize(newsize=(new_width, new_height)).set_position(("center", "center")).set_opacity(0.68).fadeout(1)
            image_clip = image_clip.resize(newsize=(new_width, new_height)).set_position(("center", "center")).set_opacity(0.38).fadeout(1)

            final_video = CompositeVideoClip([video_clip, image_clip.set_start(0)], size=(video_width, video_height))
            filename, file_extension = os.path.splitext(os.path.basename(self.video_path))
            output_video_path = os.path.join(tmp_folder, f"{filename}_with_image{file_extension}")
            final_video.write_videofile(output_video_path, audio=False, verbose=False, logger=None, codec='libx264', ffmpeg_params=["-movflags", "+faststart"])

            temp_audio_path = os.path.join(tmp_folder, 'temp_audio.mp3')
            final_output_path = os.path.join(self.output_dir, f"{filename}_with_words_card{file_extension}")
            self.add_audio_to_video(output_video_path, temp_audio_path, final_output_path)

            first_frame_path = self.extract_first_frame(self.video_path)
            return final_output_path, first_frame_path

    # def add_audio_to_video(self, processed_video_path, temp_audio_path, final_output_path):
    #     extract_audio_cmd = f"ffmpeg -y -i \"{self.video_path}\" -q:a 0 -map a \"{temp_audio_path}\""
    #     subprocess.call(extract_audio_cmd, shell=True)
    #     add_audio_cmd = f"ffmpeg -y -i \"{processed_video_path}\" -i \"{temp_audio_path}\" -c:v copy -c:a aac -strict experimental -map 0:v:0 -map 1:a:0 \"{final_output_path}\""
    #     subprocess.call(add_audio_cmd, shell=True)

    def add_audio_to_video(self, processed_video_path, temp_audio_path, final_output_path):
        extract_audio_cmd = f"ffmpeg -y -i \"{self.video_path}\" -q:a 0 -map a \"{temp_audio_path}\""
        extract_status = subprocess.call(extract_audio_cmd, shell=True)
        if extract_status == 0 and os.path.exists(temp_audio_path):  # Check if audio extraction was successful and audio file exists
            add_audio_cmd = f"ffmpeg -y -i \"{processed_video_path}\" -i \"{temp_audio_path}\" -c:v copy -c:a aac -strict experimental -map 0:v:0 -map 1:a:0 \"{final_output_path}\""
            add_status = subprocess.call(add_audio_cmd, shell=True)
            if add_status != 0:  # If adding audio fails, copy the processed video as the final output
                shutil.copy2(processed_video_path, final_output_path)
        else:
            shutil.copy2(processed_video_path, final_output_path)  # No audio track present or extraction failed, copy video only


    def extract_first_frame(self, video_path):
        filename, file_extension = os.path.splitext(os.path.basename(video_path))
        first_frame_path = os.path.join(self.output_dir, f"{filename}_first_frame.jpg")
        video = cv2.VideoCapture(video_path)
        success, image = video.read()
        if success:
            cv2.imwrite(first_frame_path, image)
        video.release()
        return first_frame_path


===== ./__init__.py =====


===== ./subtitle_metadata.py =====
import os
import re
from datetime import datetime

from lazyedit.openai_version_check import OpenAI



import json
import json5
import traceback

from lazyedit.utils import JSONParsingError, JSONValidationError
from lazyedit.openai_request import OpenAIRequestBase


import glob
from concurrent.futures import ThreadPoolExecutor


def robust_json5_parse(json_str):
    # Attempt to handle unexpected newlines and unescaped double quotes
    json_str = ''.join(line.strip() if not line.strip().startswith('"') else line for line in json_str.split('\n'))

    # Try to parse the JSON string using json5 for more flexibility
    try:
        parsed_json = json5.loads(json_str)
        return parsed_json
    except ValueError as e:
        print(f'JSON Decode Error: {e}')
        return None


class Subtitle2Metadata(OpenAIRequestBase):
    def __init__(self, openai_client, use_cache=False, max_retries=3, *args, **kwargs):

        kwargs["use_cache"] = use_cache
        kwargs["max_retries"] = max_retries

        super().__init__(*args, **kwargs)

        self.client = openai_client
        self.max_retries = max_retries
        # self.subtitles2metadata_file = 'subtitles2metadata.json'
        # self.subtitles2metadata = self.load_subtitles2metadata()

        self.subtitles2metadata_folder = 'subtitles2metadata'

        # self.base_filename = os.path.splitext(os.path.basename(subtitle_path))[0]
        self.datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

        # self.use_cache = use_cache

        # self.ensure_folder_exists(self.subtitles2metadata_folder)

    # def ensure_folder_exists(self, folder_path):
    #     if not os.path.exists(folder_path):
    #         os.makedirs(folder_path)

    def get_filename(self, subtitle_path, lang):
        # base_filename = os.path.basename(subtitle_path)
        # Strip extension from base_name if necessary
        # base_filename = os.path.splitext(base_filename)[0]
        # datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

        base_filename = os.path.splitext(os.path.basename(subtitle_path))[0]
        # datetime_str = self.datetime_str
        return f"{self.subtitles2metadata_folder}/{base_filename}_{lang}.json"

    # def save_subtitles2metadata(self, subtitle_path, prompt, ai_response, metatype="XiaoHongShu", lang="en"):
    #     filename = self.get_filename(subtitle_path, lang=lang)
    #     data_to_save = {
    #         "mixed_subtitle_path": subtitle_path,
    #         "prompt": prompt,
    #         "answer": ai_response,
    #         "type": metatype
    #     }
    #     with open(filename, 'w', encoding='utf-8') as file:
    #         json.dump(data_to_save, file, indent=4, ensure_ascii=False)


    # def load_latest_subtitles2metadata(self, subtitle_path, metatype="XiaoHongShu", lang="en"):
    #     base_name = os.path.basename(subtitle_path)
    #     base_name = os.path.splitext(base_name)[0]
    #     pattern = f"{self.subtitles2metadata_folder}/{base_name}-*_{lang}.json"
    #     files = glob.glob(pattern)
    #     if not files:
    #         return None  # No cache available

    #     # Find the latest file based on the naming convention
    #     latest_file = max(files, key=os.path.getctime)
    #     with open(latest_file, 'r', encoding='utf-8') as file:
    #         cached_data = json.load(file)


    #     # print(cached_data)
    #     # return json.dumps(cached_data["answer"], indent=4, ensure_ascii=False)
    #     return cached_data["answer"]#["answer"]


    # def load_subtitles2metadata(self):
    #     if os.path.exists(self.subtitles2metadata_file):
    #         with open(self.subtitles2metadata_file, 'r', encoding='utf-8') as file:
    #             return json.load(file)
    #     else:
    #         return []

    # def save_subtitles2metadata(self):
    #     with open(self.subtitles2metadata_file, 'w', encoding='utf-8') as file:
    #         json.dump(self.subtitles2metadata, file, indent=4, ensure_ascii=False)

    # def extract_and_parse_json(self, text):

    #     bracket_pattern = r'\{.*\}'
    #     matches = re.findall(bracket_pattern, text, re.DOTALL)

    #     if not matches:
    #         raise JSONParsingError("No JSON string found in text", text, text)

    #     json_string = matches[0]

    #     try:
    #         # json_string = ''.join(line.strip() if not line.strip().startswith('"') else line for line in json_string.split('\n'))
    #         json_string = json_string.replace('\n', '')
    #         parsed_json = json5.loads(json_string)
    #         if len(parsed_json) == 0:
    #             raise JSONParsingError("Parsed JSON string is empty", json_string, text)

    #         return parsed_json
    #     except ValueError as e:
    #         traceback.print_exc()
    #         raise JSONParsingError(f"JSON Decode Error: {e}", json_string, text)



    def parse_timestamp(self, timestamp):
        try:
            return datetime.strptime(timestamp, "%H:%M:%S,%f")
        except ValueError:
            # Handle incorrect format, you might want to adjust based on your needs
            return None

    def switch_timestamps_if_necessary(self, start, end):
        start_dt = self.parse_timestamp(start)
        end_dt = self.parse_timestamp(end)
        if start_dt and end_dt and start_dt > end_dt:
            return end, start
        return start, end

    def validate_metadata(self, metadata):
        # required_fields = [
        #     "title", "brief_description", "middle_description", "long_description",
        #     "tags", "english_words_to_learn", "teaser", "cover"
        # ]
        # missing_fields = [field for field in required_fields if field not in metadata]
        # if missing_fields:
        #     raise JSONValidationError(f"Missing required fields: {', '.join(missing_fields)}", metadata)

        # Validate teaser timestamps
        if 'teaser' in metadata:
            # start, end = metadata['teaser'].split(" --> ")
            start, end = metadata['teaser']["start"], metadata['teaser']["end"]
            metadata['teaser'] = " --> ".join(self.switch_timestamps_if_necessary(start, end))

        # Validate english_words_to_learn timestamp_range
        if 'english_words_to_learn' in metadata:
            for word_info in metadata['english_words_to_learn']:
                if 'timestamp_range' in word_info:
                    # start, end = word_info['timestamp_range'].split(" --> ")
                    start, end = word_info["timestamp_range"]["start"], word_info["timestamp_range"]["end"]
                    word_info['timestamp_range'] = " --> ".join(self.switch_timestamps_if_necessary(start, end))
                    word_info['word'] = word_info['word'].lower()




    def generate_video_metadata(self, subtitle_path, caption_path):
        with ThreadPoolExecutor(max_workers=1) as executor:
            # Initiate both tasks in parallel
            future_zh = executor.submit(self.generate_video_metadata_zh, subtitle_path, caption_path)
            future_en = executor.submit(self.generate_video_metadata_en, subtitle_path, caption_path)

            # Wait for both tasks to complete
            result_zh = future_zh.result()
            result_en = future_en.result()

        # Try to merge 'english_version' from result_en into result_zh
        try:
            result_zh["english_version"] = result_en
        except Exception:
            # In case there's an error, fallback to a copy of result_zh
            result_zh["english_version"] = result_zh.copy()

        # Try to merge 'english_words_to_learn' from result_en into result_zh,
        # while keeping a backup of the original 'english_words_to_learn' from result_zh
        try:
            # Backup 'english_words_to_learn' from the Chinese version before replacing
            if "english_words_to_learn" in result_zh:
                result_zh["english_words_to_learn_zh"] = result_zh["english_words_to_learn"].copy()

            # Replace 'english_words_to_learn' with the version from the English metadata
            if "english_words_to_learn" in result_en:
                # result_zh["english_words_to_learn_en"] = result_en["english_words_to_learn"]
                
                result_zh["english_words_to_learn"] = result_en["english_words_to_learn"]
                
        except Exception as e:
            # Handle any exceptions that might occur during the process
            print(f"An error occurred while merging 'english_words_to_learn': {e}")

        return result_zh

    # def generate_video_metadata(self, subtitle_path, caption_path):
    #     with ThreadPoolExecutor(max_workers=1) as executor:
    #         # Initiate both tasks in parallel
    #         future_zh = executor.submit(self.generate_video_metadata_zh, subtitle_path, caption_path)
    #         future_en = executor.submit(self.generate_video_metadata_en, subtitle_path, caption_path)

    #         # Wait for both tasks to complete
    #         result_zh = future_zh.result()
    #         result_en = future_en.result()

    #     # Try to merge 'english_version' from result_en into result_zh
    #     try:
    #         result_zh["english_version"] = result_en
    #     except Exception:
    #         result_zh["english_version"] = result_zh.copy()  # Fallback to a copy of result_zh

    #     # Merging 'english_words_to_learn' while keeping original lists intact
    #     try:
    #         result_zh["english_words_to_learn_zh"] = result_zh.get("english_words_to_learn", [])
    #         result_zh["english_words_to_learn_en"] = result_en.get("english_words_to_learn", [])

    #         # Create a dictionary to ensure each word appears only once with the earliest timestamp
    #         word_to_earliest = {}

    #         # Combine lists from both English and Chinese results
    #         combined_words = result_zh["english_words_to_learn_zh"] + result_zh["english_words_to_learn_en"]
    #         for entry in combined_words:
    #             word = entry["word"]
    #             timestamp = self.parse_timestamp(entry['timestamp_range'].split(" --> ")[0])

    #             if word not in word_to_earliest or timestamp < word_to_earliest[word]['timestamp']:
    #                 word_to_earliest[word] = {
    #                     'timestamp': timestamp,
    #                     'timestamp_range': entry['timestamp_range'],
    #                     'word': word
    #                 }

    #         # Convert to a sorted list by timestamp
    #         sorted_unique_words = sorted(
    #             word_to_earliest.values(),
    #             key=lambda x: x['timestamp']
    #         )

    #         result_zh["english_words_to_learn"] = [{'timestamp_range': info['timestamp_range'], 'word': info['word']} for info in sorted_unique_words]

    #     except Exception as e:
    #         print(f"An error occurred while merging 'english_words_to_learn': {e}")

    #     return result_zh

    # def generate_video_metadata_zh(self, subtitle_path):

    #     with open(subtitle_path, 'r', encoding='utf-8') as file:
    #         mixed_subtitles = file.read()

    #     retries = 0
    #     messages = [
    #         {"role": "system", "content": (
    #             "My name is OpenAI. I am an expert of social media who can help vlogers add influences, grow fans, "
    #             "reach out audiences and create values. "
    #             "I can help vlogers influence people subconsiously. "
    #         )},
    #         {"role": "user", "content": ""}  # Placeholder for the actual prompt
    #     ]

    #     while retries < self.max_retries:
    #         try:

    #             # Construct the prompt for the AI
    #             prompt = (
    #                 "I want to publish this video on XiaoHongShu, Bilibili, Douyin. \n\n"


    #                 "Based on the provided subtitles from a video, please generate a suitable title, "
    #                 "a brief introduction, a middle description, a long description, tags, "
    #                 "some English words that viewers can learn, teaser range, and a cover timestamp. \n\n"
                    
    #                 "Make it in normal, realistic narration but appealing and put some knowledge in description "
    #                 "that pique viewer's interest to favorite, collect, love and follow. "
    #                 "(This is our secret. Don't let it be seen in the title or description per se. "
    #                 "Achieve this subconsiously. ) \n\n"
                    
    #                 "The title should be in Chinese and up to 20 characters, the brief description should be in Chinese "
    #                 "and up to 80 characters, the middle description should be in Chinese and up to 250 characters, "
    #                 "the long description should be in Chinese and up to 1000 characters, there should be 10 tags related to the content of the video, "
    #                 "Five pure ENGLISH words or phrases that are important for viewers to learn from the video sorted by interestingness, "
    #                 "Each word should be accompanied by a timestamp range indicating when it appears in the video.\n\n"
    #                 "Give a 2~4 seconds timestamp range which can reflect the essense of the video as teaser. "
    #                 "Also, suggest a timestamp for the best scene to use as a cover image for the video. \n\n"
    #                 # "and a cover timestamp indicating the best scene to use as the cover image. "

    #                 "Try to find instructions also in subtitles if exist. \n\n"
                    
    #                 "Return correct format result with imagination even subtitles is little or even empty. \n\n"

    #                 "Multilingual subtitles:\n" + mixed_subtitles + "\n\n"
                    
    #                 "Based on the subtitles, please output in the following JSON format:\n"
    #                 # "{\"title\": \"\", \"brief_description\": \"\", \"middle_description\": \"\", \"long_description\": \"\", \"tags\": [], "
    #                 # "\"english_words_to_learn\": [{\"word\": \"\", \"timestamp_range\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\"}], \"cover\": \"HH:MM:SS,mmm\"}"
    #                 "```json"
    #                 "{\n"
    #                 "  \"title\": \"\",\n"
    #                 "  \"brief_description\": \"\",\n"
    #                 "  \"middle_description\": \"\",\n"
    #                 "  \"long_description\": \"\",\n"
    #                 "  \"tags\": [],\n"
    #                 "  \"english_words_to_learn\": [\n"
    #                 "    {\n"
    #                 "      \"word\": \"\",\n"
    #                 "      \"timestamp_range\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\"\n"
    #                 "    }\n"
    #                 "  ],\n"
    #                 "  \"teaser\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\",\n"
    #                 "  \"cover\": \"HH:MM:SS,mmm\"\n"
    #                 "}"
    #                 "```"
    #             )

    #             ai_response = None
    #             if self.use_cache:
    #                 ai_response = self.load_latest_subtitles2metadata(subtitle_path, lang="zh")
                
    #             if not self.use_cache or not ai_response:

    #                 messages[1]["content"] = prompt  # Update the actual prompt content


    #                 print("Querying OpenAI (Chinese) ...")

    #                 # Define the request to OpenAI API
    #                 response = self.client.chat.completions.create(
    #                     model=os.environ.get("OPENAI_MODEL", "gpt-4-0125-preview"),
    #                     messages=messages
    #                 )

    #                 # Extract the AI's response
    #                 ai_response = response.choices[0].message.content.strip()


    #             print(ai_response)

    #             # Extract and parse the JSON part of the AI's response
    #             result = self.extract_and_parse_json(ai_response)

    #             # Validate the parsed JSON data
    #             self.validate_metadata(result)
                
    #             # # Save the prompt and response pair
    #             # self.subtitles2metadata.append({
    #             #     "mixed_subtitle_path": subtitle_path,
    #             #     "prompt": prompt,
    #             #     "answer": ai_response,
    #             #     "type": "XiaoHongShu, Douyin, Bilibili"
    #             # })
    #             # self.save_subtitles2metadata()
    #             self.save_subtitles2metadata(subtitle_path, prompt, ai_response, metatype="XiaoHongShu", lang="zh")

    #             return result  # Successfully parsed JSON, return the result

            
    #         except (JSONParsingError, JSONValidationError) as e:
    #             error_message = f"Failed on attempt {retries + 1}: {e}"
    #             print(error_message)
    #             traceback.print_exc()
    #             retries += 1  # Increment the retry count
                
    #             # Append the response and error message for context
    #             messages.append({"role": "system", "content": ai_response})
    #             messages.append({"role": "user", "content": e.message})
                
    #             if retries >= self.max_retries:
    #                 raise e  # Re-raise the last exception (either JSONParsingError or JSONValidationError)


    #     raise JSONParsingError("Reached maximum retries without success.", ai_response, messages[-1]["content"])


    def generate_video_metadata_zh(self, subtitle_path, caption_path):
        # Load the subtitles from the given file path
        with open(subtitle_path, 'r', encoding='utf-8') as file:
            mixed_subtitles = file.read()

        with open(caption_path, 'r', encoding='utf-8') as file:
            captions = file.read()

        # Sample JSON structure and string definition to reflect the requested changes
        sample_json_structure = {
            "title": "",
            "brief_description": "",
            "middle_description": "",
            "long_description": "",
            "tags": [],
            "english_words_to_learn": [
                {
                    "word": "",
                    "timestamp_range": {
                        "start": "HH:MM:SS,mmm",
                        "end": "HH:MM:SS,mmm"
                    }
                }
            ],
            "teaser": {
                "start": "HH:MM:SS,mmm",
                "end": "HH:MM:SS,mmm"
            },
            "cover": "HH:MM:SS,mmm"
        }
        sample_json_string = json.dumps(sample_json_structure, indent=4, ensure_ascii=False)

        # System prompt and user prompt for API call
        system_content = (
            "My name is OpenAI. I am an expert of social media who can help vlogers add influences, grow fans, "
            "reach out audiences and create values. "
            "I can help vlogers influence people subconsiously."
        )
        prompt = (
            "I want to publish this video on XiaoHongShu, Bilibili, Douyin. \n\n"
            "Based on the provided CLIPxGPT caption of frames and subtitles from the voice track, "
            "please generate a suitable title, "
            "a brief introduction, a middle description, a long description, tags, "
            "some English words that viewers can learn, teaser range, and a cover timestamp. \n\n"
            "Make it in normal, realistic narration but appealing and put some knowledge in description "
            "that pique viewer's interest to favorite, collect, love and follow. "
            "(This is our secret. Don't let it be seen in the title or description per se. "
            "Achieve this subconsiously. ) \n\n"
            "The title should be in Chinese and up to 20 characters, the brief description should be in Chinese "
            "and up to 80 characters, the middle description should be in Chinese and up to 250 characters, "
            "the long description should be in Chinese and up to 1000 characters, there should be 10 tags related to the content of the video, "
            "Five pure ENGLISH words or phrases that are important for viewers to learn from the video sorted by interestingness, "
            "Each word should be accompanied by a timestamp range indicating when it appears in the video.\n\n"
            "Give a 2~4 seconds timestamp range which can reflect the essense of the video as teaser. "
            "Also, suggest a timestamp for the best scene to use as a cover image for the video. \n\n"
            "Try to find instructions also in subtitles if exist. \n\n"
            "Return correct format result with imagination even subtitles is little or even empty. \n\n"
            "Captions:\n" + captions + "\n\n"
            "Subtitles:\n" + mixed_subtitles + "\n\n"
            "Please provide the metadata in the following JSON format:\n"
            "```json\n" + sample_json_string + "\n```"
        )

        # Replace this placeholder with your actual send_request_with_retry call.
        # This is just a placeholder to show where the call would be made.
        result = self.send_request_with_retry(
            prompt=prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(subtitle_path, lang="zh")
        )

        self.validate_metadata(result)

        return result

    # def generate_video_metadata_en(self, subtitle_path):

    #     with open(subtitle_path, 'r', encoding='utf-8') as file:
    #         mixed_subtitles = file.read()


    #     retries = 0
    #     messages = [
    #         {"role": "system", "content": (
    #             "My name is OpenAI. I am an expert of social media who can help Youtube vlogers add influences, grow fans, "
    #             "reach out audiences and create values. "
    #             "I can help vlogers influence people subconsiously. "
    #         )},
    #         {"role": "user", "content": ""}  # Placeholder for the actual prompt
    #     ]

    #     while retries < self.max_retries:
    #         try:

    #             # Construct the prompt for the AI
    #             prompt = (
    #                 "I want to publish this video on Youtube. \n\n"


    #                 "Based on the provided subtitles from a video, please generate a suitable title, "
    #                 "a brief introduction, a middle description, a long description, tags, "
    #                 "some English words that viewers can learn, teaser range, and a cover timestamp. "
    #                 "\n\n"

    #                 "Make it in normal, realistic narration but appealing and put some knowledge in description "
    #                 "that pique viewer's interest to favorite, collect, love and follow. "
    #                 "(This is our secret. Don't let it be seen in the title or description per se. "
    #                 "Make it achieve this subconsiously. ) "
    #                 "\n\n"

    #                 "The title should be in English and up to 20 words, the brief description should be in English "
    #                 "and up to 80 words, the middle description should be in English and up to 250 words, "
    #                 "the long description should be in English and up to 500 words, there should be 10 tags related to the content of the video, "
    #                 "Five pure ENGLISH words that are important for viewers to learn from the video sorted by interestingness, "
    #                 "Each word should be accompanied by a timestamp range indicating when it appears in the video."
    #                 "Give 2~4 seconds timestamp range which can reflect the essense of the video as teaser. "
    #                 "Also, suggest a timestamp for the best scene to use as a cover image for the video. "
    #                 # "and a cover timestamp indicating the best scene to use as the cover image. "
    #                 "\n\n"

    #                 "Try to find instructions also in subtitles if exist. \n\n"

    #                 "Return correct format result with imagination even subtitles is little or even empty. \n\n"

    #                 "Multilingual subtitles:\n" + mixed_subtitles + "\n\n"
    #                 "Based on the subtitels, please output in the following JSON format:\n"
    #                 # "{\"title\": \"\", \"brief_description\": \"\", \"middle_description\": \"\", \"long_description\": \"\", \"tags\": [], "
    #                 # "\"english_words_to_learn\": [{\"word\": \"\", \"timestamp_range\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\"}], \"cover\": \"HH:MM:SS,mmm\"}"
    #                 "```json"
    #                 "{\n"
    #                 "  \"title\": \"\",\n"
    #                 "  \"brief_description\": \"\",\n"
    #                 "  \"middle_description\": \"\",\n"
    #                 "  \"long_description\": \"\",\n"
    #                 "  \"tags\": [],\n"
    #                 "  \"english_words_to_learn\": [\n"
    #                 "    {\n"
    #                 "      \"word\": \"\",\n"
    #                 "      \"timestamp_range\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\"\n"
    #                 "    }\n"
    #                 "  ],\n"
    #                 "  \"teaser\": \"HH:MM:SS,mmm --> HH:MM:SS,mmm\",\n"
    #                 "  \"cover\": \"HH:MM:SS,mmm\"\n"
    #                 "}"
    #                 "```"
    #             )

    #             ai_response = None
    #             if self.use_cache:
    #                 ai_response = self.load_latest_subtitles2metadata(subtitle_path, lang="en")
                
    #             if not self.use_cache or not ai_response:

    #                 messages[1]["content"] = prompt  # Update the actual prompt content


    #                 print("Querying OpenAI (English) ...")

    #                 # Define the request to OpenAI API
    #                 response = self.client.chat.completions.create(
    #                     model=os.environ.get("OPENAI_MODEL", "gpt-4-0125-preview"),
    #                     messages=messages
    #                 )

    #                 # Extract the AI's response
    #                 ai_response = response.choices[0].message.content.strip()

    #             # Extract and parse the JSON part of the AI's response
    #             result = self.extract_and_parse_json(ai_response)

    #             # Validate the parsed JSON data
    #             self.validate_metadata(result)
                
    #             # # Save the prompt and response pair
    #             # self.subtitles2metadata.append({
    #             #     "mixed_subtitle_path": subtitle_path,
    #             #     "prompt": prompt,
    #             #     "answer": ai_response,
    #             #     "type": "Youtube"
    #             # })
    #             # self.save_subtitles2metadata()

    #             self.save_subtitles2metadata(subtitle_path, prompt, ai_response, metatype="Youtube", lang="en")

    #             return result  # Successfully parsed JSON, return the result

            
    #         except (JSONParsingError, JSONValidationError) as e:
    #             error_message = f"Failed on attempt {retries + 1}: {e}"
    #             print(error_message)
    #             traceback.print_exc()
    #             retries += 1  # Increment the retry count
                
    #             # Append the response and error message for context
    #             messages.append({"role": "system", "content": ai_response})
    #             messages.append({"role": "user", "content": error_message})
                
    #             if retries >= self.max_retries:
    #                 raise e  # Re-raise the last exception (either JSONParsingError or JSONValidationError)


    #     raise JSONParsingError("Reached maximum retries without success.", ai_response, messages[-1]["content"])

    def generate_video_metadata_en(self, subtitle_path, caption_path):
        # Load the subtitles from the given file path
        with open(subtitle_path, 'r', encoding='utf-8') as file:
            mixed_subtitles = file.read()

        with open(caption_path, 'r', encoding='utf-8') as file:
            captions = file.read()

        # Sample JSON structure and string definition to reflect the requested changes
        sample_json_structure = {
            "title": "",
            "brief_description": "",
            "middle_description": "",
            "long_description": "",
            "tags": [],
            "english_words_to_learn": [
                {
                    "word": "",
                    "timestamp_range": {
                        "start": "HH:MM:SS,mmm",
                        "end": "HH:MM:SS,mmm"
                    }
                }
            ],
            "teaser": {
                "start": "HH:MM:SS,mmm",
                "end": "HH:MM:SS,mmm"
            },
            "cover": "HH:MM:SS,mmm"
        }
        sample_json_string = json.dumps(sample_json_structure, indent=4, ensure_ascii=False)

        # System prompt and user prompt for API call
        system_content = (
            "My name is OpenAI. I am an expert of social media who can help Youtube vlogers add influences, grow fans, "
            "reach out audiences and create values. "
            "I can help vlogers influence people subconsiously."
        )
        prompt = (
            "I want to publish this video on Youtube. \n\n"
            "Based on the provided CLIPxGPT caption of frames and subtitles from the voice track, "
            "please generate a suitable title, "
            "a brief introduction, a middle description, a long description, tags, "
            "some English words that viewers can learn, teaser range, and a cover timestamp. \n\n"
            "Make it in normal, realistic narration but appealing and put some knowledge in description "
            "that pique viewer's interest to favorite, collect, love and follow. "
            "(This is our secret. Don't let it be seen in the title or description per se. "
            "Achieve this subconsiously. ) \n\n"
            "The title should be in English and up to 20 words, the brief description should be in English "
            "and up to 80 words, the middle description should be in English and up to 250 words, "
            "the long description should be in English and up to 500 words, there should be 10 tags related to the content of the video, "
            "Five pure ENGLISH words that are important for viewers to learn from the video sorted by interestingness, "
            "Each word should be accompanied by a timestamp range indicating when it appears in the video.\n\n"
            "Give a 2~4 seconds timestamp range which can reflect the essence of the video as teaser. "
            "Also, suggest a timestamp for the best scene to use as a cover image for the video. \n\n"
            "Try to find instructions also in subtitles if exist. \n\n"
            "Return correct format result with imagination even subtitles is little or even empty. \n\n"
            "Captions:\n" + captions + "\n\n"
            "Subtitles:\n" + mixed_subtitles + "\n\n"
            "Please provide the metadata in the following JSON format:\n"
            "```json\n" + sample_json_string + "\n```"
        )

        # Replace this placeholder with your actual send_request_with_retry call.
        result = self.send_request_with_retry(
            prompt=prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(subtitle_path, lang="en")
        )

        self.validate_metadata(result)

        return result


    

if __name__ == "__main__":

    from io import StringIO

    # Usage example
    openai_client = OpenAI()  # Make sure to authenticate your OpenAI client
    sub2meta = Subtitle2Metadata(openai_client)
    english_subtitles = """
1
00:00:00,000 --> 00:00:03,500
The only good thing about this is the chicken. It's really good.

2
00:00:03,500 --> 00:00:04,500
Very tender.

3
00:00:05,500 --> 00:00:06,500
Really good.

"""  # Replace with actual subtitles

    chinese_subtitles = """
1
00:00:00,000 --> 00:00:02,880
这个唯一好吃的是这个鸡肉挺好吃的

2
00:00:02,880 --> 00:00:03,400
特别好吃

3
00:00:03,400 --> 00:00:04,180
很嫩

4
00:00:04,180 --> 00:00:05,980
太好吃了

"""
    # Creating file-like objects from strings.
    english_subtitles_file = StringIO(english_subtitles)
    chinese_subtitles_file = StringIO(chinese_subtitles)


    result = sub2meta.generate_video_metadata(english_subtitles_file, chinese_subtitles_file)
    print(result)


===== ./openai_version_check.py =====
# openai_version_check.py
import openai
from packaging import version

def check_openai_version():
    required_version = version.parse("1.1.1")
    current_version = version.parse(openai.__version__)

    if current_version < required_version:
        raise ValueError(f"Error: OpenAI version {openai.__version__} is less than the required version 1.1.1")
    else:
        print("OpenAI version is compatible.")

# Run the version check when this module is imported
check_openai_version()

# Attempt to import the OpenAI class
from openai import OpenAI

# You might want to initialize the OpenAI client here or in the module where it's imported
# For example:
# client = OpenAI()  # Uncomment and use if initializing OpenAI client directly here


===== ./utils.py =====
import json
import json5
from pprint import pprint
from PIL import Image, ImageDraw, ImageFont

sample_texts = {
    "English": "This is a sample English text that is designed to test the text wrapping capabilities of the ckjwrap library. The purpose is to see how well it can handle text from different languages, including English, and ensure that wrapping occurs correctly without breaking words inappropriately.",
    # "Japanese": "これはckjwrapライブラリのテキスト折り返し機能をテストするために設計されたサンプルの日本語テキストです。異なる言語のテキストをどの程度適切に扱えるか、そして単語を不適切に分割することなく正しく折り返しが行われるかどうかを確認することが目的です。さらに、このテキストはテキストラッピングのテストを継続しています。",
    "Japanese": "日月金木水火土学生山川愛情友達力美味時空海星雲雨風花雪国会話数家人生年安心出来世界平和夢希望工業科学技術歴史文化祭典会社経済政治法律教育健康医療食品安全環境資源エネルギー問題解決",
    "Chinese": "这是一个样本中文文本，旨在测试ckjwrap库的文本换行能力。目的是看看它如何能够正确处理不同语言的文本，包括中文，确保换行时不会不适当地拆分单词。此外，本文本继续测试文本换行功能。",
    "Arabic": "هذا نص عربي تجريبي مصمم لاختبار قدرات الفقرة في مكتبة ckjwrap. الغرض هو لرؤية كيف يمكنه التعامل مع نصوص من لغات مختلفة، بما في ذلك العربية، وضمان أن يتم الفقرة بشكل صحيح دون تقسيم الكلمات بشكل غير مناسب. بالإضافة إلى ذلك، يستمر هذا النص في اختبار قدرات الفقرة."
}


def get_text_size(text, font, max_width, max_height):
    test_canvas_size = (int(max_width), int(max_height))  # Canvas size based on max_width and max_height
    dummy_image = Image.new('RGB', test_canvas_size)
    draw = ImageDraw.Draw(dummy_image)
    return draw.textbbox((0, 0), text, font=font)[2:]

def find_font_size(text, font_path, max_width, max_height, start_size=360, step=2):
    font_size = start_size
    font = ImageFont.truetype(font_path, font_size)
    while True:
        text_width, text_height = get_text_size(text, font, max_width, max_height)
        if text_width <= max_width and text_height <= max_height:
            break
        font_size -= step
        if font_size <= 0:
            break
        font = ImageFont.truetype(font_path, font_size)
    return font_size



def safe_pretty_print(json_str):
    try:
        # Try to directly parse and pretty-print the JSON string
        parsed_json = json5.loads(json_str)
        # print(json.dumps(parsed_json, indent=2, ensure_ascii=False))

        pprint(parsed_json)


    except ValueError as e:
        print("JSONDecodeError: Could not parse the entire JSON string.")
        print("Attempting to print as much as possible...\n")
        
        # Fallback: Attempt to manually parse and print parts of the JSON
        try:
            # Very basic heuristic: split by '}, {' to handle simple cases
            parts = json_str.split('}, {')
            for part in parts:
                # Ensuring each part is properly enclosed in braces
                cleaned_part = part
                if not part.startswith('{'):
                    cleaned_part = '{' + part
                if not part.endswith('}'):
                    cleaned_part += '}'
                try:
                    # Try to parse each part individually
                    parsed_part = json.loads(cleaned_part)
                    print(json.dumps(parsed_part, indent=2, ensure_ascii=False))
                except json.JSONDecodeError:
                    print(f"Could not parse part: {cleaned_part}")
        except Exception as e:
            print(f"Error during fallback parsing: {e}")
            print(json_str)



class JSONParsingError(Exception):
    def __init__(self, message, json_string, user_message):
        super().__init__(message)

        print("JSON String: ")
        print(json_string)

        self.message = message        
        self.json_string = json_string
        self.user_message = user_message

class JSONValidationError(Exception):
    def __init__(self, message, parsed_json):
        super().__init__(message)

        self.message = message
        self.parsed_json = parsed_json





===== ./autocut_processor.py =====
# autocut_processor.py
import os
import subprocess

def create_or_replace_hard_link(source, link_name):
    if os.path.exists(link_name):
        os.remove(link_name)  # Remove the existing file to replace
    os.link(source, link_name)



class AutocutProcessor:
    def __init__(self, input_file, output_folder, base_name, extension):
        self.input_file = input_file
        self.output_folder = output_folder
        self.base_name = base_name
        self.extension = extension

    def run_autocut(self, lang, gpu_id):
        # Set the CUDA_VISIBLE_DEVICES environment variable
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)

        # Define filenames with language-specific suffixes before the extension
        input_file_lang = f"{self.output_folder}/{self.base_name}_{lang}{self.extension}"

        # Create hard link for language-specific version
        # os.link(self.input_file, input_file_lang)
        create_or_replace_hard_link(self.input_file, input_file_lang)

        # Define the command to process the video for the specified language
        # autocut_command = f"/home/lachlan/miniconda3/bin/python -m autocut -t \"{input_file_lang}\" --whisper-model large --lang={lang} --force"
        # autocut_command = f"/home/lachlan/miniconda3/envs/whisper/bin/python /home/lachlan/Projects/whisper_with_lang_detect/vad_lang_subtitle.py -t \"{input_file_lang}\" --whisper-model large-v2 --force"
        autocut_command = f"/home/lachlan/miniconda3/envs/whisper/bin/python /home/lachlan/Projects/whisper_with_lang_detect/vad_lang_subtitle.py -t \"{input_file_lang}\" --whisper-model large-v3 --force"
        print("autocut_command: ", autocut_command)


        # Run the autocut command with the specified environment
        subprocess.run(autocut_command, shell=True, check=True, env=env)
        print(f"Finished autocut with lang={lang} on GPU {gpu_id}")


===== ./count_furigana.py =====
import unicodedata

def analyze_text(text):
    # Initialize counts for the specified characters
    count_less_than = text.count('<')
    count_greater_than = text.count('>')
    count_open_bracket = text.count('[')
    count_close_bracket = text.count(']')
    hiragana_count = 0

    # Flag to keep track of whether we are inside square brackets
    inside_brackets = False
    
    for char in text:
        # Check if we enter or exit square brackets
        if char == '[':
            inside_brackets = True
        elif char == ']':
            inside_brackets = False
        # If inside brackets, check if the character is a Hiragana
        elif inside_brackets and 'HIRAGANA' in unicodedata.name(char, ''):
            hiragana_count += 1

    # Return the counts as a dictionary
    return {
        'less_than': count_less_than,
        'greater_than': count_greater_than,
        'open_bracket': count_open_bracket,
        'close_bracket': count_close_bracket,
        'hiragana_inside_brackets': hiragana_count,
    }

# Example usage with the provided string
text = "<ベトナム>[べとなむ]<語>[ご]と<韓国語>[かんこくご]を<使>[つか]って、ちうはを"
result = analyze_text(text)
print(result)


===== ./languages.py =====
LANGUAGES = {
    "en": "english",
    "zh": "chinese",
    "de": "german",
    "es": "spanish",
    "ru": "russian",
    "ko": "korean",
    "fr": "french",
    "ja": "japanese",
    "pt": "portuguese",
    "tr": "turkish",
    "pl": "polish",
    "ca": "catalan",
    "nl": "dutch",
    "ar": "arabic",
    "sv": "swedish",
    "it": "italian",
    "id": "indonesian",
    "hi": "hindi",
    "fi": "finnish",
    "vi": "vietnamese",
    "he": "hebrew",
    "uk": "ukrainian",
    "el": "greek",
    "ms": "malay",
    "cs": "czech",
    "ro": "romanian",
    "da": "danish",
    "hu": "hungarian",
    "ta": "tamil",
    "no": "norwegian",
    "th": "thai",
    "ur": "urdu",
    "hr": "croatian",
    "bg": "bulgarian",
    "lt": "lithuanian",
    "la": "latin",
    "mi": "maori",
    "ml": "malayalam",
    "cy": "welsh",
    "sk": "slovak",
    "te": "telugu",
    "fa": "persian",
    "lv": "latvian",
    "bn": "bengali",
    "sr": "serbian",
    "az": "azerbaijani",
    "sl": "slovenian",
    "kn": "kannada",
    "et": "estonian",
    "mk": "macedonian",
    "br": "breton",
    "eu": "basque",
    "is": "icelandic",
    "hy": "armenian",
    "ne": "nepali",
    "mn": "mongolian",
    "bs": "bosnian",
    "kk": "kazakh",
    "sq": "albanian",
    "sw": "swahili",
    "gl": "galician",
    "mr": "marathi",
    "pa": "punjabi",
    "si": "sinhala",
    "km": "khmer",
    "sn": "shona",
    "yo": "yoruba",
    "so": "somali",
    "af": "afrikaans",
    "oc": "occitan",
    "ka": "georgian",
    "be": "belarusian",
    "tg": "tajik",
    "sd": "sindhi",
    "gu": "gujarati",
    "am": "amharic",
    "yi": "yiddish",
    "lo": "lao",
    "uz": "uzbek",
    "fo": "faroese",
    "ht": "haitian creole",
    "ps": "pashto",
    "tk": "turkmen",
    "nn": "nynorsk",
    "mt": "maltese",
    "sa": "sanskrit",
    "lb": "luxembourgish",
    "my": "myanmar",
    "bo": "tibetan",
    "tl": "tagalog",
    "mg": "malagasy",
    "as": "assamese",
    "tt": "tatar",
    "haw": "hawaiian",
    "ln": "lingala",
    "ha": "hausa",
    "ba": "bashkir",
    "jw": "javanese",
    "su": "sundanese",
    "yue": "cantonese",
}

# language code lookup by name, with a few language aliases
TO_LANGUAGE_CODE = {
    **{language: code for code, language in LANGUAGES.items()},
    "burmese": "my",
    "valencian": "ca",
    "flemish": "nl",
    "haitian": "ht",
    "letzeburgesch": "lb",
    "pushto": "ps",
    "panjabi": "pa",
    "moldavian": "ro",
    "moldovan": "ro",
    "sinhalese": "si",
    "castilian": "es",
    "mandarin": "zh",
}

===== ./openai_request.py =====
import os
import json
import json5
import traceback
import glob
import re
import csv
from datetime import datetime
from openai import OpenAI


class JSONValidationError(Exception):
    def __init__(self, message, json_string=None):
        super().__init__(message)
        self.message = message
        self.json_string = json_string

class JSONParsingError(Exception):
    def __init__(self, message, json_string, text):
        super().__init__(message)

        print("The failed JSON string: \n\n")
        print(json_string)

        self.message = message
        self.json_string = json_string
        self.text = text

class OpenAIRequestBase:
    def __init__(self, use_cache=True, max_retries=3, cache_dir='cache'):
        self.client = OpenAI()  # Assume correct initialization with API key
        self.max_retries = max_retries
        self.use_cache = use_cache
        self.cache_dir = cache_dir
        self.ensure_dir_exists(self.cache_dir)

    def ensure_dir_exists(self, path):
        if not os.path.exists(path):
            os.makedirs(path)

    def get_cache_file_path(self, prompt, filename=None):
        if filename is None:

            filename = f"{abs(hash(prompt))}.json"
    
        cache_path = os.path.join(self.cache_dir, filename)
        print("cache_path: ", cache_path)
        cache_dir = os.path.dirname(cache_path)
        print("cache_dir: ", cache_dir)
        os.makedirs(cache_dir, exist_ok=True)
        return cache_path

    def save_to_cache(self, prompt, response, filename=None):
        file_path = self.get_cache_file_path(prompt, filename=filename)
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump({"prompt": prompt, "response": response}, file, ensure_ascii=False, indent=4)

    def load_from_cache(self, prompt, filename=None):
        file_path = self.get_cache_file_path(prompt, filename=filename)
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                cached_data = json.load(file)
                return cached_data["response"]
        return None
    
    def validate_json(self, json_data, sample_json):
        if type(json_data) != type(sample_json):
            raise JSONValidationError("JSON data type does not match the sample JSON type.")

        if isinstance(sample_json, dict):
            for key, sample_value in sample_json.items():
                if key not in json_data:
                    raise JSONValidationError(f"Key '{key}' is missing.")
                self.validate_json(json_data[key], sample_value)
        elif isinstance(sample_json, list):
            if len(sample_json) > 0:
                sample_item = sample_json[0]
                for item in json_data:
                    self.validate_json(item, sample_item)

    def parse_response(self, response):
        first_dict_index = response.find('{')
        first_list_index = response.find('[')
        if first_dict_index == -1 and first_list_index == -1:
            raise JSONParsingError("No JSON structure found.", response, response)
        
        if (first_dict_index != -1 and first_dict_index < first_list_index) or (first_list_index == -1):
            parse_pattern = r'\{.*\}'
        else:
            parse_pattern = r'\[.*\]'

        matches = re.findall(parse_pattern, response, re.DOTALL)
        if not matches:
            raise JSONParsingError("No matching JSON structure found.", response, response)

        json_string = matches[0]
        try:
            return json5.loads(json_string)
        except json.JSONDecodeError as e:
            print("json_string: ", json_string)
            raise JSONParsingError("Failed to decode JSON.", json_string, response)

    def send_request_with_retry(self, prompt, system_content="You are an AI.", sample_json=None, filename=None):
        

        retries = 0
        # messages = [{"role": "system", "content": system_content}, {"role": "user", "content": prompt}]
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": prompt}
        ]

        print("self.use_cache: ", self.use_cache)

        if self.use_cache:
            cached_response = self.load_from_cache(prompt, filename=filename)
            if cached_response:
                print("OpenAI cache found. ")
                return cached_response

        while retries < self.max_retries:
            try:
                ai_response = ""
                ai_response = self.client.chat.completions.create(
                    model=os.environ.get("OPENAI_MODEL", "gpt-4o-mini"),
                    messages=messages
                )
                ai_response = ai_response.choices[0].message.content.strip()
                parsed_response = self.parse_response(ai_response)

                if sample_json:
                    self.validate_json(parsed_response, sample_json)

                self.save_to_cache(prompt, parsed_response, filename=filename)
                return parsed_response
            except Exception as e:
                traceback.print_exc()
                retries += 1
                messages.append({"role": "system", "content": ai_response})
                try:
                    messages.append({"role": "system", "content": e.message})
                except:
                    messages.append({"role": "system", "content": str(e)})

        raise Exception("Maximum retries reached without success.")


===== ./subtitle_translate_old2.py =====
import json
import json5
import re
import traceback
import openai

from packaging import version
import json
import os

from concurrent.futures import ThreadPoolExecutor, as_completed



from lazyedit.openai_version_check import OpenAI


from lazyedit.utils import JSONParsingError, JSONValidationError
from lazyedit.utils import safe_pretty_print, sample_texts, find_font_size
from lazyedit.openai_request import OpenAIRequestBase

from datetime import datetime
from pprint import pprint

import cjkwrap

import glob

import numpy as np






class SubtitlesTranslator(OpenAIRequestBase):
    def __init__(self, 
        openai_client, 
        input_json_path, 
        input_sub_path, 
        output_sub_path,
        video_length=None,
        video_width=1080,
        video_height=1920,
        max_retries=3,
        use_cache=False,
        *args, **kwargs
    ):
        kwargs["use_cache"] = use_cache
        kwargs["max_retries"] = max_retries

        super().__init__(*args, **kwargs)


        self.client = openai_client
        self.input_json_path = input_json_path
        self.input_sub_path = input_sub_path
        self.output_sub_path = output_sub_path

        self.video_length = video_length
        self.video_width = video_width
        self.video_height = video_height
        self.base_width = 1920
        self.base_height = 1080

        self.wrapping_limit_half_width_default = 60  # Default wrapping_limit_half_width for landscape
        
        if not self.is_video_landscape:
            self.wrapping_limit_half_width_default = int(self.wrapping_limit_half_width_default * 0.5)  # Adjust wrapping_limit_half_width for portrait videos


        # self.is_video_landscape = video_width > video_height
        self.font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"

        self.translation_log_folder = 'translation_logs'

        # self.max_retries = max_retries
        # self.use_cache = use_cache

        print("Using translation cache: ", use_cache)

        # self.ensure_log_folder_exists()



    @property
    def is_video_landscape(self):
        """Determine if the video is landscape or portrait based on class variables."""
        return self.video_width > self.video_height

    


    def load_subtitles_from_json(self):
        """Load subtitles from a JSON file."""
        with open(self.input_json_path, 'r', encoding='utf-8') as file:
            return json5.load(file)

    @staticmethod
    def correct_json_string(s):
        # Remove trailing commas after object properties or array elements
        corrected_s = re.sub(r',\s*}', '}', s)
        corrected_s = re.sub(r',\s*\]', ']', corrected_s)
        return corrected_s

    def extract_and_parse_json(self, text):
        """Extract and parse JSON from text, handling potential parsing issues."""
        bracket_pattern = r'\[.*\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL)
        # json_string = ""

        if not matches:
            raise JSONParsingError("No JSON string found in text", text, text)
        json_string = matches[0].replace('\n', '')

        # pprint(json_string)
        safe_pretty_print(json_string)

        try:
            json_string = self.correct_json_string(json_string)
            return json5.loads(json_string)
        except ValueError as e:
            traceback.print_exc()
            raise JSONParsingError(f"JSON Decode Error: {e}", json_string, text)

    def validate_translated_subtitles(self, subtitles, required_fields=["start", "end", "en", "zh"]):
        """Validate the structure of translated subtitles."""
        # required_fields = ["start", "end", "en", "zh"]
        for subtitle in subtitles:
            if not all(field in subtitle for field in required_fields):
                raise JSONValidationError("Subtitle missing one of the required fields: " + ", ".join(required_fields))



    def translate_and_merge_subtitles(self, subtitles):
        """Splits subtitles into 1-minute batches and processes each batch in parallel."""
        # subtitles = self.load_subtitles_from_json()

        # Splitting subtitles into 1-minute batches
        batches = self.split_subtitles_into_batches(subtitles)

        # Process each batch and accumulate results in parallel
        translated_subtitles = self.process_batches_in_parallel(batches)

        # Save the final translated subtitles
        # self.save_translated_subtitles_to_ass(translated_subtitles)
        print("All subtitles have been processed and saved successfully.")

        return translated_subtitles

    def process_batches_in_parallel(self, batches):
        """Process subtitle batches in parallel using ThreadPoolExecutor."""
        translated_subtitles = []

        with ThreadPoolExecutor(max_workers=1) as executor:
            # Submit all batches to be processed in parallel
            future_to_batch = {executor.submit(self.translate_and_merge_subtitles_in_batch, batch, i): batch for i, batch in enumerate(batches)}

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    translated_batch = future.result()
                    translated_subtitles.extend(translated_batch)
                except Exception as exc:
                    print(f'Batch {batch} generated an exception: {exc}')
        
        # Optional: Sort the merged list by start timestamps if necessary
        translated_subtitles.sort(key=lambda x: datetime.strptime(x['start'], '%H:%M:%S,%f'))

        return translated_subtitles

    def split_subtitles_into_batches(self, subtitles):
        """Splits subtitles into 1-minute batches based on their timestamps."""
        batches = []
        current_batch = []
        current_batch_start_time = None

        for subtitle in subtitles:
            start_time = datetime.strptime(subtitle["start"], '%H:%M:%S,%f')
            if current_batch_start_time is None:
                current_batch_start_time = start_time

            if (start_time - current_batch_start_time).total_seconds() > 60:
                # Start a new batch if the current subtitle start time exceeds 1 minute from the batch's start time
                batches.append(current_batch)
                current_batch = [subtitle]
                current_batch_start_time = start_time
            else:
                current_batch.append(subtitle)

        # Add the last batch if it contains subtitles
        if current_batch:
            batches.append(current_batch)

        return batches

    

    # def translate_and_merge_subtitles_in_batch(self, subtitles, idx):
    #     """Merge translations from Japanese-specific and other languages' functions in parallel."""
    #     with ThreadPoolExecutor(max_workers=1) as executor:
    #         # Submit both translation tasks to the executor
    #         future_ja = executor.submit(self.translate_and_merge_subtitles_ja, subtitles, idx)
    #         future_major_lang = executor.submit(self.translate_and_merge_subtitles_major_languages, subtitles, idx)

    #         # Wait for both futures to complete and retrieve results
    #         translations_ja = future_ja.result()
    #         translations_major_lang = future_major_lang.result()

    #     # Merge translations as before
    #     timestamps_dict = {
    #         (translation['start'], translation['end']): translation for translation in translations_major_lang
    #     }

    #     for ja_translation in translations_ja:
    #         key = (ja_translation['start'], ja_translation['end'])
    #         if key in timestamps_dict:
    #             timestamps_dict[key]['ja'] = ja_translation['ja']
    #         else:
    #             timestamps_dict[key] = ja_translation

    #     merged_translations = list(timestamps_dict.values())
    #     merged_translations.sort(key=lambda x: x['start'])

    #     return merged_translations

    # def translate_and_merge_subtitles_in_batch(self, subtitles, idx):
    #     """Merge translations from multiple languages' functions in parallel."""
    #     # Define a dictionary of language codes to their respective translation functions
    #     translation_tasks = {
    #         'major': self.translate_and_merge_subtitles_major_languages,  # This handles other major languages
    #         'ja': self.translate_and_merge_subtitles_ja,
    #         'ko': self.translate_and_merge_subtitles_ko,  # Assuming you have a similar function for Korean
    #         'minor': self.translate_and_merge_subtitles_minor_languages
    #     }

    #     with ThreadPoolExecutor(max_workers=len(translation_tasks)) as executor:
    #         # Submit all translation tasks to the executor and collect Future objects
    #         futures = {
    #             executor.submit(translation_func, subtitles, idx): lang_code
    #             for lang_code, translation_func in translation_tasks.items()
    #         }

    #         # Initialize an empty dictionary to collect all translations
    #         all_translations = {}

    #         # Process completed translation tasks as they complete
    #         for future in as_completed(futures):
    #             lang_code = futures[future]
    #             try:
    #                 translations = future.result()
    #                 # Store translations by language code
    #                 all_translations[lang_code] = translations
    #             except Exception as exc:
    #                 print(f'{lang_code} translation generated an exception: {exc}')

    #     # Merge translations
    #     timestamps_dict = {}

    #     # Start with 'major' language translations as the base
    #     for translation in all_translations.pop('major', []):
    #         key = (translation['start'], translation['end'])
    #         timestamps_dict[key] = translation

    #     # Merge additional language translations
    #     for lang_code, translations in all_translations.items():
    #         for translation in translations:
    #             key = (translation['start'], translation['end'])
    #             if key in timestamps_dict:
    #                 timestamps_dict[key][lang_code] = translation[lang_code]
    #             else:
    #                 # If the key doesn't exist, it means this timestamp only has translation in this particular language
    #                 timestamps_dict[key] = translation

    #     merged_translations = list(timestamps_dict.values())
    #     merged_translations.sort(key=lambda x: x['start'])

    #     return merged_translations

    def translate_and_merge_subtitles_in_batch(self, subtitles, idx):
        """Merge translations from multiple languages' functions in parallel."""
        # Define a dictionary of language codes to their respective translation functions
        translation_tasks = {
            'major': self.translate_and_merge_subtitles_major_languages,
            'ja': self.translate_and_merge_subtitles_ja,
            'ko': self.translate_and_merge_subtitles_ko,
            'minor': self.translate_and_merge_subtitles_minor_languages
        }

        with ThreadPoolExecutor(max_workers=len(translation_tasks)) as executor:
            futures = {
                executor.submit(translation_func, subtitles, idx): lang_code
                for lang_code, translation_func in translation_tasks.items()
            }

            all_translations = {}

            for future in as_completed(futures):
                lang_code = futures[future]
                try:
                    translations = future.result()
                    all_translations[lang_code] = translations
                except Exception as exc:
                    print(f'{lang_code} translation generated an exception: {exc}')

        # Merge translations
        timestamps_dict = {}

        for translations in all_translations.values():
            for translation in translations:
                key = (translation['start'], translation['end'])

                # Initialize the dictionary entry if it doesn't exist
                if key not in timestamps_dict:
                    timestamps_dict[key] = {'start': translation['start'], 'end': translation['end']}
                
                # Dynamically add all language translations, regardless of the number of languages
                for sub_lang, text in translation.items():
                    if sub_lang not in ['start', 'end']:  # Skip timestamp keys
                        timestamps_dict[key][sub_lang] = text

        merged_translations = list(timestamps_dict.values())
        merged_translations.sort(key=lambda x: x['start'])

        return merged_translations


    def translate_and_merge_subtitles_major_languages(self, subtitles, idx):
        """Translate and merge subtitles using the OpenAI API."""

        print("Translating subtitles into other languages...")
        
        # client = self.client


        # # Define a sample JSON structure for validation purposes
        # sample_subtitles_structure = [
        #     {
        #         "start": "timestamp",
        #         "end": "timestamp",
        #         "en": "English text",
        #         "zh": "Chinese text",
        #         "ar": "Arabic text",
        #         # "...": "Text in the original language, if not in the listed before. "
        #     }
        # ]

        # sample_json_string = json.dumps(sample_subtitles_structure, indent=2, ensure_ascii=False)


        # Define a JSONC string with comments
        sample_json_string = """
        [
            {
                "start": "timestamp",  // Start time of the subtitle
                "end": "timestamp",    // End time of the subtitle
                "en": "English text",  // English translation
                "zh": "Chinese text",  // Chinese translation
                "ar": "Arabic text",    // Arabic translation
                // "...": "Text in the original language, if not in the listed before."
            }
        ]
        """

        # Parse the JSONC string into a Python object
        sample_subtitles_structure = json5.loads(sample_json_string)


        system_content = "Translate and merge mixed language subtitles into English and Chinese, providing coherent and accurate translations."
        prompt = (
            "Below are mixed language subtitles extracted from a video, including timestamps, "
            "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
            "is presented with English (en), Chinese (zh), and Arabic (ar) translations, "
            "maintaining the original timestamps. "
            "If a subtitle is already in English, provide the corresponding Chinese and Arabic translation, and vice versa. "
            "For subtitles in any other language, keep the original text but also provide translations in "
            "English, Chinese and Arabic. \n\n"

            "Fullfill the instructions/requests in subtitles per se for other languages with iso_code_639_1 language key. "
            "If I said in subtitles that I want to know or I don't know how to say something, "
            "provide the whole subtitles in that language. "
         
            "Correct some apparent speech recognition error and inconsistencies, "
            "especially homonym and mumble in both origin and its translation based on the context.\n\n"
    
            "Process the following subtitles, ensuring translations are accurate and coherent, "
            "and format the output as shown in the example. "
            "Note that the original timestamps should be preserved for each entry.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"
            
            # "Please provide a complete and accurate translation and formatting for each subtitle entry."

            "Output JSON format only:\n"
            "```json"
            f"{sample_json_string}\n"
            "```"
        )






        translated_subtitles = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure)

        print("Translated subtitles (Major): \n")
        pprint(translated_subtitles)



        return translated_subtitles

    def translate_and_merge_subtitles_minor_languages(self, subtitles, idx):
        """Translate and merge subtitles using the OpenAI API into Spanish, French, and Vietnamese."""

        print("Translating subtitles into minor languages...")
        
        # Define a JSONC string with comments for the minor languages
        sample_json_string = """
        [
            {
                "start": "timestamp",  // Start time of the subtitle
                "end": "timestamp",    // End time of the subtitle
                "es": "Spanish text",  // Spanish translation
                "fr": "French text",   // French translation
                "vi": "Vietnamese text"  // Vietnamese translation
                // "...": "Text in the original language, if not in the listed before."
            }
        ]
        """

        # Parse the JSONC string into a Python object using json5
        sample_subtitles_structure = json5.loads(sample_json_string)

        system_content = "Translate mixed language subtitles into Spanish, French, and Vietnamese, providing coherent and accurate translations."
        prompt = (
            "Below are mixed language subtitles extracted from a video, including timestamps, "
            "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
            "is presented with Spanish (es), French (fr), and Vietnamese (vi) translations, "
            "maintaining the original timestamps. "
            "If a subtitle is already in one of these languages, provide the corresponding translations in the other two languages. "
            # "For subtitles in any other language, keep the original text but also provide translations in "
            # "Spanish, French, and Vietnamese.\n\n"

            "Fulfill the instructions/requests in subtitles per se for other languages with iso_code_639_1 language key. "
            "If I said in subtitles that I want to know or I don't know how to say something, "
            "provide the whole subtitles in that language.\n\n"

            "Correct some apparent speech recognition error and inconsistencies, "
            "especially homonym and mumble in both origin and its translation based on the context.\n\n"

            "Process the following subtitles, ensuring translations are accurate and coherent, "
            "and format the output as shown in the example. "
            "Note that the original timestamps should be preserved for each entry.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "Output JSON format only:\n"
            "```json"
            f"{sample_json_string}\n"
            "```"
        )

        translated_subtitles = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure)

        print("Translated subtitles (Minor): \n")
        pprint(translated_subtitles)

        return translated_subtitles



    # Function to annotate Kanji and Katakana independently
    @staticmethod
    def annotate_kanji_katakana(subtitles):
        # Define the regular expression patterns for Kanji and Katakana
        kanji_pattern = r'[\u4e00-\u9faf]+'
        katakana_pattern = r'[\u30a0-\u30ff]+'
        
        # Function to wrap text in <>[]
        def replacer(match):
            return f'<{match.group(0)}>[]'
        
        # Annotate each item in the list
        for item in subtitles:
            # Annotate Kanji
            item['ja'] = re.sub(kanji_pattern, replacer, item['ja'])
            # Annotate Katakana
            item['ja'] = re.sub(katakana_pattern, replacer, item['ja'])
        
        return subtitles

    def translate_and_merge_subtitles_ja(self, subtitles, idx):
        """Request Japanese subtitles separately with specific formatting for furigana."""
        print("Translating subtitles to Japanese...")

        sample_subtitles_structure = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ja": "Japanese text with optional furigana annotations"
            }
        ]

        sample_json_string = json.dumps(sample_subtitles_structure, indent=2, ensure_ascii=False)

        system_content = "Translate subtitles into Japanese, correcting any errors based on context."
        prompt = (
            "Translate the following subtitles into Japanese. "
            "\n\n"

            "Correct speech recognition errors and inconsistencies based on context.\n\n"
            
            "Note that the original timestamps should be preserved for each entry.\n\n"

            
            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"


            "Output JSON format only:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        
        translated_subtitles_ja = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure)


        annotated_subtitles = self.annotate_kanji_katakana(translated_subtitles_ja)

        print("annotated subtitles: \n")
        pprint(annotated_subtitles)

        translated_subtitles_ja_with_furigana = self.add_furigana_for_japanese_subtitles(annotated_subtitles, idx)

        print("furigana subtitles: \n")
        pprint(translated_subtitles_ja_with_furigana)

        return translated_subtitles_ja_with_furigana

            


    def add_furigana_for_japanese_subtitles(self, subtitles, idx):
        """Request Japanese subtitles with specific formatting for furigana using the OpenAI API with retries."""

        sample_subtitles_structure_with_furigana = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ja": "Japanese text with furigana annotations"
            }
        ]

        sample_json_string = json.dumps(sample_subtitles_structure_with_furigana, indent=2, ensure_ascii=False)

        system_content = "Add furigana annotations to the provided Japanese text."
        prompt = (
            "Given the Japanese subtitles, add furigana annotations correctly based on the context. "
            "Preserve the original timestamps. Use the format '<Kanji>[Furigana]' for annotations. "
            "Correct any speech recognition errors and inconsistencies based on context.\n\n"
            
            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "Output JSON format only:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        # Utilize send_request_with_retry to handle API requests, including retries, caching, and JSON validation
        translated_subtitles_ja_with_furigana = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure_with_furigana)

        print("Translated subtitles (Japanese with furigana): \n")
        pprint(translated_subtitles_ja_with_furigana)

        return translated_subtitles_ja_with_furigana


    def translate_and_merge_subtitles_ko(self, subtitles, idx):
        """Request Korean subtitles with specific considerations."""
        print("Translating subtitles to Korean...")

        # Define a sample JSON structure for Korean subtitles
        sample_subtitles_structure_ko = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ko": "Korean text"  # Use "ko" for Korean text
            }
        ]

        sample_json_string_ko = json.dumps(sample_subtitles_structure_ko, indent=2, ensure_ascii=False)

        system_content = "Translate subtitles into Korean, correcting any errors based on context."
        prompt = (
            "Translate the following subtitles into Korean. "
            "\n\n"
            "Correct speech recognition errors and inconsistencies based on context.\n\n"
            "Note that the original timestamps should be preserved for each entry.\n\n"
            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"
            "Output JSON format only:\n"
            "```json\n"
            f"{sample_json_string_ko}\n"
            "```"
        )

        # Use the function for sending requests with retries, similarly to the Japanese version
        translated_subtitles_ko = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure_ko)

        # Here, you might add any specific post-processing for Korean subtitles if necessary
        # For example, annotating certain phrases, cultural references, or anything specific to Korean

        translated_subtitles_ko_with_hanja = self.replace_hangul_with_hanja(translated_subtitles_ko, idx)

        print("Translated subtitles (Korean): \n")
        pprint(translated_subtitles_ko_with_hanja)

        return translated_subtitles_ko_with_hanja


    


    def replace_hangul_with_hanja(self, subtitles, idx):
        """Annotate Korean subtitles with Hanja and conservatively replace original text with Hanja annotations."""

        print("Annotating Korean subtitles with Hanja...")

        system_content = "Annotate Korean text with original Chinese Hanja."

        # Sample JSON structure expected in the response
        sample_annotation_structure = {
            "korean_with_annotation": "",
            "korean_hanja_pairs": [
                # {"korean_part": "한자 ", "hanja": "漢字 ", "roman": "hanja"}
                {"korean_part": "", "hanja": ""}
            ]
        }
        sample_json_string = json.dumps(sample_annotation_structure, indent=2, ensure_ascii=False)


        # Helper function for conservative replacement in the original text
        def conservative_replace(original_text, hanja_pairs):
            updated_text = original_text
            for pair in hanja_pairs:
                korean_part = pair.get("korean_part", "")
                hanja = pair.get("hanja", "")
                pronunciation = pair.get("roman", "")

                # Check if the 'hanja' part is actually Hangul or if there's no Hanja present
                if re.match(r'^[\uAC00-\uD7AF]+$', hanja):
                    # If the 'hanja' part is fully Hangul, skip styling it as Hanja
                    # hanja_part = ""
                    continue

                if len(hanja) == 0:
                    continue

                replace_with = f"<{hanja}>[{korean_part}]({pronunciation})"

                # Perform a conservative replacement (only once for each pair)
                if korean_part in updated_text:
                    start_pos = updated_text.find(korean_part)
                    if start_pos != -1:
                        end_pos = start_pos + len(korean_part)
                        updated_text = updated_text[:start_pos] + replace_with + updated_text[end_pos:]
            
            return updated_text

        def process_subtitle(subtitle):
            """Construct and send a request for annotating a single subtitle with Hanja."""
            prompt = (
                "For words that have etymological Hanja alternatives (due to their Sino-Korean origin), "
                "I want to find the corresponding Chinese character origin to replace the Hangul. "
                "However, it's crucial that the Hangul in parenthesis followed by these traditional Hanja "
                "are put in brackets to maintain clarity as in this format: (hangul)[hanja].\n\n"
                # "Please also provide Roman pronunciation of the Hangul. "
                f"Korean to be annotated: {subtitle['ko']}\n\n"
                "I don't need step by step explanation. "
                "PLEASE ONLY output the JSON:\n"
                f"```json\n{sample_json_string}\n```"
            )

            response = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_annotation_structure)
            # Assume response is structured correctly
            annotated_text = response.get("korean_with_annotation", "")
            hanja_pairs = response.get("korean_hanja_pairs", [])
            
            # Update the original text using the conservative replace function
            updated_original_text = conservative_replace(subtitle["ko"], hanja_pairs)

            return {
                "start": subtitle["start"],
                "end": subtitle["end"],
                "ko": updated_original_text,  # Updated original Korean text with Hanja annotations
                # "annotated": annotated_text,  # Full annotated text (if needed separately)
                # "hanja_pairs": hanja_pairs  # Hanja pairs for reference
            }

        # annotated_subtitles = [process_subtitle(sub) for sub in subtitles]

        # Parallel execution with error handling to return original subtitles if an error occurs
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(process_subtitle, sub): sub for sub in subtitles}
            for future in as_completed(futures):
                sub = futures[future]
                try:
                    result = future.result()
                    sub.update(result)
                except Exception as exc:
                    print(f"Subtitle processing generated an exception: {exc}")
                    # If there's an error, the original subtitle remains unchanged

        print("Korean with Hanja: \n")
        pprint(subtitles)

        return subtitles


    def translate_and_merge_subtitles_vi(self, subtitles, idx):
        """Request Vietnamese subtitles with specific considerations."""
        print("Translating subtitles to Vietnamese...")

        # Define a sample JSON structure for Vietnamese subtitles
        sample_subtitles_structure_vi = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "vi": "Vietnamese text"  # Use "vi" for Vietnamese text
            }
        ]

        sample_json_string_vi = json.dumps(sample_subtitles_structure_vi, indent=2, ensure_ascii=False)

        system_content = "Translate subtitles into Vietnamese, correcting any errors based on context."
        prompt = (
            "Translate the following subtitles into Vietnamese. "
            "\n\n"
            "Correct speech recognition errors and inconsistencies based on context.\n\n"
            "Note that the original timestamps should be preserved for each entry.\n\n"
            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"
            "Output JSON format only:\n"
            "```json\n"
            f"{sample_json_string_vi}\n"
            "```"
        )

        translated_subtitles_vi = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_subtitles_structure_vi)

        translated_subtitles_vi_with_chuhan = self.replace_viet_with_chuhan(translated_subtitles_vi, idx)

        print("Translated subtitles (Vietnamese): \n")
        pprint(translated_subtitles_vi_with_chuhan)

        return translated_subtitles_vi_with_chuhan

    def replace_viet_with_chuhan(self, subtitles, idx):
        """Annotate Vietnamese subtitles with Chu-Han and conservatively replace original text with Chu-Han annotations."""
        
        print("Annotating Vietnamese subtitles with Chu-Han...")

        system_content = "Annotate Vietnamese text with original Chinese Chu-Han."
        
        # Sample JSON structure expected in the response
        sample_annotation_structure = {
            "viet_with_annotation": "",
            "viet_chuhan_pairs": [
                {"viet_part": "", "chuhan": ""}
            ]
        }
        sample_json_string = json.dumps(sample_annotation_structure, indent=2, ensure_ascii=False)

        def conservative_replace(original_text, chuhan_pairs):
            """Perform conservative replacement of Viet text with Chu-Han annotations."""
            updated_text = original_text
            for pair in chuhan_pairs:
                viet_part = pair.get("viet_part", "")
                chuhan = pair.get("chuhan", "")

                if len(chuhan) == 0:
                    continue

                replace_with = f"<{chuhan}>[{viet_part}]"
                
                # Perform a conservative replacement (only once for each pair)
                if viet_part in updated_text:
                    start_pos = updated_text.find(viet_part)
                    if start_pos != -1:
                        end_pos = start_pos + len(viet_part)
                        updated_text = updated_text[:start_pos] + replace_with + updated_text[end_pos:]
            
            return updated_text

        def process_subtitle(subtitle):
            """Construct and send a request for annotating a single subtitle with Chu-Han."""
            prompt = (
                "For words that have etymological Chu-Han alternative (due to their Sino-Korean origin), "
                "I want to find the corresponding Chinese origin to replace the Viet. "
                "However, it's crucial that the Viet in parenthesis followed by these traditional Chu-Han "
                "are put in brackets to maintain clarity. "
                "Like this example: (viet)[chuhan].\n\n"
                f"Vietnamese to be annotated: {subtitle['vi']}\n\n"
                "PLEASE ONLY output the JSON:\n"
                f"```json\n{sample_json_string}\n```"
            )

            response = self.send_request_with_retry(prompt, system_content=system_content, sample_json=sample_annotation_structure)
            annotated_text = response.get("viet_with_annotation", "")
            chuhan_pairs = response.get("viet_chuhan_pairs", [])
            
            updated_original_text = conservative_replace(subtitle["vi"], chuhan_pairs)

            return {
                "start": subtitle["start"],
                "end": subtitle["end"],
                "vi": updated_original_text  # Updated original Vietnamese text with Chu-Han annotations
            }

        # Parallel execution with error handling to return original subtitles if an error occurs
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(process_subtitle, sub): sub for sub in subtitles if 'vi' in sub}
            for future in as_completed(futures):
                sub = futures[future]
                try:
                    result = future.result()
                    sub.update(result)
                except Exception as exc:
                    print(f"Subtitle processing generated an exception: {exc}")
                    # If there's an error, the original subtitle remains unchanged

        print("Vietnamese with Chu-Han: \n")
        pprint(subtitles)

        return subtitles




    def save_translated_subtitles_to_srt(self, translated_subtitles):
        """Save the translated subtitles to an SRT file, ensuring language order."""
        srt_content = ""
        for index, subtitle in enumerate(translated_subtitles, start=1):
            # Start the subtitle entry with its sequence number and time range
            srt_content += f"{index}\n{subtitle['start']} --> {subtitle['end']}\n"
            
            # Always add Chinese (zh) translation first if it exists
            if 'zh' in subtitle:
                srt_content += f"{subtitle['zh']}\n"
            
            # Add English (en) translation
            if 'en' in subtitle:
                srt_content += f"{subtitle['en']}\n"
            
            # Add any additional languages present, excluding 'start', 'end', 'zh', and 'en'
            additional_languages = {k: v for k, v in subtitle.items() if k not in ['start', 'end', 'zh', 'en']}
            for lang_code, text in additional_languages.items():
                srt_content += f"{text}\n"
            
            # Separate subtitles with an empty line
            srt_content += "\n"
        
        # Write the constructed SRT content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(srt_content)


    def process_subtitles(self):
        """Main process to load, translate, merge, and save subtitles."""
        subtitles = self.load_subtitles_from_json()
        translated_subtitles = self.translate_and_merge_subtitles(subtitles)
        # self.save_translated_subtitles_to_srt(translated_subtitles)
        self.save_translated_subtitles_to_ass(translated_subtitles)
        print("Subtitles have been processed and saved successfully.")


    def rearrange_brackets(self, ja_text):
        """
        Detects and rearranges text where [] are found inside <> to follow the <>[] pattern.
        """
        
        # Define a regex pattern to match <...[...]...>
        pattern = re.compile(r'<([^>\[\]]+)\[([^\[\]]+)\]([^>\[\]]*)>')
        
        # Function to rearrange the matched pattern to <...> [...]
        def rearrange(match):
            # Extract the parts of the match
            before_bracket = match.group(1)
            inside_bracket = match.group(2)
            after_bracket = match.group(3)
            
            # Return the rearranged format
            return f'<{before_bracket}{after_bracket}>[{inside_bracket}]'

        # Apply the rearrange function to all matching patterns in the text
        rearranged_text = pattern.sub(rearrange, ja_text)

        return rearranged_text

    def remove_preceding_repetition(self, ja_text):
        """
        Removes a word or phrase that is repeated immediately before <...> when
        the content inside <> is the same as the preceding word.
        """
        
        # Define a regex pattern to match repetition before <>
        pattern = re.compile(r'(\S+)<\1>')
        
        # Function to replace the matched pattern with just the content in angle brackets
        def deduplicate(match):
            # Return only the content within angle brackets
            return f'<{match.group(1)}>'

        # Apply the deduplication function to all matching patterns in the text
        deduplicated_text = pattern.sub(deduplicate, ja_text)

        return deduplicated_text


    def clean_triplicated_sequences(self, ja_text):
        """
        Simplifies text by converting sequences of the form same<same>[same] to just 'same'.
        """
        # Define a regex pattern to match the 'same<same>[same]' structure
        pattern = re.compile(r'(?:(\S+)<\1>\[\1\])')
        
        # Function to perform the substitution
        def replacement(match):
            # Return just the first 'same' part of the match
            return match.group(1)
        
        # Remove same hiragana<same hiragana>[ same hiragana] anywhere
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text



    


    def clean_duplicated_kanji_hiragana_sequence(self, ja_text):
        """
        Converts sequences of the form 'same kanji<same kanji>[hiragana]' to '<kanji>[hiragana]'.
        """
        # Define a regex pattern to match the specified structure
        pattern = re.compile(r'([一-龠ァ-ヶ々ー]+)(<\1>)\[([ぁ-ん]+)\]')
        
        # Function to perform the substitution
        def replacement(match):
            # for i in range(4):
            #     print(match.group(i))
            
            # Return the simplified structure '<kanji>[hiragana]'
            kanji = match.group(2)
            hiragana = match.group(3)
            return f'{kanji}[{hiragana}]'
        
        # Convert kanji<kanji>[hiragana] to <kanji>[hiragana], preserving the sequence (two kanji seq is the same)
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text

    def clean_redundant_hiragana_sequence(self, ja_text):
        """
        Modifies text by removing redundant angle and square brackets for matching hiragana or kanji sequences.
        - For hiragana immediately before or at the start, followed by <hiragana>[hiragana], it leaves just the hiragana.
        - For kanji followed by <hiragana>[hiragana], it replaces them with [hiragana].
        """

        

        # Remove <same hiragana>[same hiragana] when at the start or directly after punctuation or preceded by different hiragana
        # hiragana_pattern = re.compile(r'(^|(?<=[ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        hiragana_pattern = re.compile(r'(^|(?<=[、。！？>\]ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        simplified_text = hiragana_pattern.sub(r'\1\2', ja_text)
        
        # Convert kanji<hiragana>[hiragana] to kanji[hiragana], preserving the sequence
        kanji_hiragana_pattern = re.compile(r'(?<=[一-龠ァ-ヶ々ー])<([ぁ-ん]+)>\[\1\]')
        simplified_text = kanji_hiragana_pattern.sub(r'[\1]', simplified_text)

        
        
        return simplified_text

    def clean_duplicated_hiragana_inside_angle_brackets(self, ja_text):
        """
        Simplifies sequences of the form 'same hiragana<same hiragana>' to just 'hiragana'.
        """
        # Define a regex pattern to match hiragana sequences repeated before and inside angle brackets
        pattern = re.compile(r'<?([ぁ-ん]+)>?<\1>')

        
        # Perform the substitution to replace matched patterns with just the hiragana
        simplified_text = pattern.sub(r'\1', ja_text)

        return simplified_text


    def convert_standalone_angle_to_square_brackets(self, ja_text):
        """
        Converts standalone angle brackets to square brackets in the given text,
        while leaving <kanji>[furigana] pairs unchanged.
        """
        
        # Define a regex pattern to find standalone <> not followed directly by []
        # This pattern assumes 'standalone' means there's no furigana in square brackets immediately following
        pattern = re.compile(r'<([^>]+)>(?!\[)')
        
        # Replace found patterns with square brackets
        converted_text = pattern.sub(r'[\1]', ja_text)
        
        return converted_text






    # # Example usage
    # original_text = "<字幕>[じまく]上[じょう]でこの<日本語>[にほんご]の発音[はつおん]を学[まな]べるようにしています。"
    # preprocessed_text = preprocess_text_for_furigana(original_text)

    # print("Preprocessed Text:", preprocessed_text)

    def katakana_to_hiragana(self, katakana):
        """
        Converts a katakana string to hiragana.
        """
        katakana_hiragana_map = str.maketrans(
            "アイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワヲンガギグゲゴザジズゼゾダヂヅデドバビブベボパピプペポ",
            "あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもやゆよらりるれろわをんがぎぐげござじずぜぞだぢづでどばびぶべぼぱぴぷぺぽ"
        )
        return katakana.translate(katakana_hiragana_map)

    def fill_blank_of_katakana_without_furigana(self, ja_text):
        """
        Converts sequences of the form <katakana>[] to <katakana>[hiragana],
        where hiragana is the equivalent of the katakana text inside the angle brackets.
        """
        # Define a regex pattern to match <katakana>[] structures
        pattern = re.compile(r'<([ァ-ヶー]+)>\[\]')
        
        # Function to perform the substitution
        def replacement(match):
            # Convert the matched katakana to hiragana
            katakana_text = match.group(1)
            hiragana_text = self.katakana_to_hiragana(katakana_text)
            return f'<{katakana_text}>[{hiragana_text}]'
        
        # Apply the replacement function to all matching patterns in the text
        converted_text = pattern.sub(replacement, ja_text)

        return converted_text

    def convert_katakana_in_brackets_to_hiragana(self, ja_text):
        """
        Detects <Katakana>[Katakana] then converts the [Katakana] into [Hiragana].
        """
        # # Define a regex pattern to match <Katakana>[Katakana]
        # pattern = re.compile(r'<([ァ-ヶー]+)>\[\1\]')

        # Define a regex pattern to match both <Katakana>[Katakana] and Katakana[Katakana]
        pattern = re.compile(r'(?:(<([ァ-ヶー]+)>)|([ァ-ヶー]+))\[(\2|\3)\]')

        # # Use the katakana_to_hiragana function to convert matched katakana to hiragana in []
        # corrected_text = re.sub(pattern, lambda m: f'<{m.group(1)}>[{katakana_to_hiragana(m)}]', ja_text)

        def replacement(match):
            # Determine if the match includes angle brackets or not
            katakana = match.group(2) if match.group(2) else match.group(3)
            hiragana = self.katakana_to_hiragana(katakana)
            # Reconstruct the string with the katakana in brackets converted to hiragana
            if match.group(1):  # If katakana was enclosed in angle brackets
                return f'<{katakana}>[{hiragana}]'
            else:  # If katakana was not enclosed in angle brackets
                return f'{katakana}[{hiragana}]'

        # Use the replacement function to convert matched katakana to hiragana in []
        corrected_text = pattern.sub(replacement, ja_text)

        return corrected_text

    
    def preprocess_text_for_furigana(self, ja_text):
        """
        Adjusts Japanese text to ensure that kanji/katakana sequences directly preceding standalone furigana
        are enclosed in <>, without altering or removing any unassociated hiragana or other characters.
        This version stops including characters in the sequence based on the starting character type (kanji or katakana).
        """
        
        # This pattern is updated to differentiate between sequences starting with kanji or katakana.
        # It ensures that if a sequence starts with kanji, it only includes kanji,
        # and if it starts with katakana, it only includes katakana, directly preceding the furigana annotation.
        # pattern = re.compile(
        #     r'(?<!<)'
        #     r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
        #     r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        # )
        pattern = re.compile(
            r'(?<!<)'
            r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
            r'([ぁ-ん]{0,2})'  # Optionally matches zero to two hiragana characters
            r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        )

        # Function to enclose matched kanji/katakana sequence in <>
        def enclose_kanji_katakana(match):
            kanji_katakana = match.group(1)  # The kanji/katakana sequence
            return f'<{kanji_katakana}>'

        # Apply the enclosure function to all appropriate sequences in the text
        preprocessed_text = pattern.sub(enclose_kanji_katakana, ja_text)

        # Return the modified text with appropriate enclosures
        return preprocessed_text


    

    # @staticmethod
    def convert_furigana_to_ass(self, ja_text):
        """Convert furigana format from <kanji>[furigana], standalone [furigana], or <kanji> to ASS ruby format,
        applying styles for kanji and furigana where present."""


        

        def replace_with_ruby(match):
            # Adjusted to handle both standalone and combined cases properly.
            kanji = match.group(1)   # Kanji could be in group 1 when present
            furigana = match.group(2 )or match.group(3)  # Furigana is in group 2  or standalone furigana in group 3
            
            # Initialize an empty string for the result
            result = ""
            
            # Format kanji and furigana if present
            if kanji and furigana:
                result = f"{{\\rKanji}}{kanji}{{\\rFurigana}}{furigana}{{\\rDefault}}"
            elif kanji:
                result = f"{{\\rKanji}}{kanji}{{\\rDefault}}"
            elif furigana:
                result = f"{{\\rFurigana}}{furigana}{{\\rDefault}}"
            
            return result

        # Adjusted regex to correctly match <kanji>[furigana], standalone [furigana], or <kanji>
        pattern = r"<([^>]+)>(?:\[(.*?)\])?|\[([^\]]+)\]"

        # Use lambda to pass match object directly to replace_with_ruby
        return re.sub(pattern, replace_with_ruby, ja_text)

    def convert_hanja_to_ass(self, text):
        """Converts text with Hanja annotations to ASS formatted text with styling, based on the given pattern."""
        
        def replace_with_ass(match):
            hanja = match.group(1)
            hangul = match.group(2)
            roman = match.group(3)

            result = ""


            
            # If Hanja is present, style it with the Kanji style
            if hanja:
                result += f"{{\\rHanja}}{hanja}{{\\rDefault}}"
            
            # Style the Hangul part with the Hangul style
            if hangul:
                result += f"{{\\rHangul}}{hangul}{{\\rDefault}}"
            
            # Add Romanized Korean (RomanKO) if present, styled differently
            if roman:
                # result += f"{{\\rRomanKO}}({roman}){{\\rDefault}}"
                # result += f"{{\\rRomanKO}}{roman}{{\\rDefault}}"
                pass

            return result

        # Pattern to match the format: <Hanja>[Hangul](RomanKO)
        pattern = r"<([^>]+)>\[([^]]+)\]\(([^)]*)\)"

        # Replace matches in the text with styled ASS text
        return re.sub(pattern, replace_with_ass, text)

    def convert_chuhan_to_ass(self, text):
        """Converts text with Chu-Han annotations to ASS formatted text with styling, based on the given pattern."""
        
        def replace_with_ass(match):
            chuhan = match.group(1)
            viet = match.group(2)

            result = ""

            # Style the Chu-Han part with the Kanji (or a specific Chu-Han) style
            if chuhan:
                result += f"{{\\rChuhan}}{chuhan}{{\\rDefault}}"
            
            # Style the Viet part with a specific Viet style
            if viet:
                result += f"{{\\rViet}}{viet}{{\\rDefault}}"

            return result

        # Pattern to match the format: <Chu-Han>[Viet]
        pattern = r"<([^>]+)>\[([^]]*)\]"

        # Replace matches in the text with styled ASS text
        return re.sub(pattern, replace_with_ass, text)

    def estimate_character_width(self, text):
        half_width_count = 0
        full_width_count = 0

        for char in text:
            # Ordinal value of the character
            ord_char = ord(char)
            # Simple checks for half-width vs. full-width character ranges
            if 0x0020 <= ord_char <= 0x007E or 0xFF61 <= ord_char <= 0xFFDC or 0xFFA0 <= ord_char <= 0xFFBE:
                half_width_count += 1
            elif 0x1100 <= ord_char <= 0x11FF or 0x2E80 <= ord_char <= 0x9FFF or 0xAC00 <= ord_char <= 0xD7AF or 0xFF01 <= ord_char <= 0xFF60 or 0xFFE0 <= ord_char <= 0xFFE6:
                full_width_count += 1

        # Determine the predominant character width in the text
        if half_width_count > full_width_count:
            return "half-width"
        else:
            return "full-width"


    # @staticmethod
    def generate_ass_header(self):
        """Generates the header for an ASS file, adjusting `PlayResX` and `PlayResY` based on video orientation."""
        # Determine if the video is landscape or portrait
        # is_video_landscape = self.video_width > self.video_height
        is_video_landscape = self.is_video_landscape

        # Adjust PlayResX and PlayResY based on orientation
        # play_res_x, play_res_y = (self.base_width, self.base_height) if is_video_landscape else (self.base_height, self.base_width)
        play_res_x, play_res_y = (self.video_width, self.video_height) if is_video_landscape else (self.video_height, self.video_width)

        # wrapping_limit_half_width_default = self.wrapping_limit_half_width_default
        # wrapping_limit_full_width_default = self.wrapping_limit_half_width_default // 2

        max_width = self.video_width * 0.8

        if self.is_video_landscape:
            max_height = self.video_height * 0.5 / 8
        else:
            max_height = self.video_height * 0.5 / 16

        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
        # english_portrait_font_size = find_font_size(sample_texts["english"][:wrapping_limit_half_width_default], font_path, max_width, max_height)


        font_sizes = {}
        for language, text in sample_texts.items():
            char_width_type = self.estimate_character_width(text)
            wrapping_limit = self.wrapping_limit_half_width_default # if char_width_type == 'half-width' else self.wrapping_limit_half_width_default // 2
            text_section = text[:wrapping_limit]
            font_sizes[language] = find_font_size(text_section, self.font_path, max_width, max_height)


        print("calculated font size: ")
        pprint(font_sizes)




        base_font_size = font_sizes["English"]
        chinese_font_size = font_sizes["Chinese"]
        english_font_size = font_sizes["English"]
        japanese_font_size = font_sizes["Japanese"]
        kanji_font_size = font_sizes["Japanese"]
        furigana_font_size = font_sizes["Japanese"]
        arabic_font_size = font_sizes["Arabic"]
        # korean
        korean_font_size = japanese_font_size
        hanja_font_size = kanji_font_size
        hangul_font_size = furigana_font_size
        romanko_font_size = furigana_font_size
        #
        spanish_font_size = english_font_size
        french_font_size = english_font_size
        # vietnamese
        vietnamese_font_size = english_font_size
        chuhan_font_size = chuhan_font_size
        viet_font_size = viet_font_size
        

        return f"""[Script Info]
ScriptType: v4.00+
Collisions: Normal
PlayResX: {play_res_x}
PlayResY: {play_res_y}

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Vernada,{base_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: English,Vernada,{english_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chinese,Vernada,{chinese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Japanese,Vernada,{japanese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Kanji,Vernada,{kanji_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Furigana,Vernada,{furigana_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Arabic1,Arial,{arabic_font_size},&H00FACE87,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Arabic,Arial,{arabic_font_size},&H0071B33C,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Korean,Vernada,{korean_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hanja1,Vernada,{hanja_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hanja,Vernada,{hanja_font_size},&H00E16941,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hangul1,Vernada,{hangul_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Hangul,Vernada,{hangul_font_size},&H00FACE87,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: RomanKO,Vernada,{romanko_font_size},&H00FACADE,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,1,1,2,10,10,10,1
Style: Spanish1,Arial,{spanish_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Spanish,Arial,{spanish_font_size},&H0000BFF1,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: French,Arial,{french_font_size},&H003929ED,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Vietnamese,Arial,{vietnamese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chuhan,Vernada,{chuhan_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Viet,Vernada,{viet_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""             

    def preprocess_japanese_ruby(self, subtitles):
        """Preprocess Japanese text within subtitles."""
        
        def custom_print(enable_print, message, text):
            """Custom print function controlled by an enable flag."""
            if enable_print:
                print(message, text)
        
        enable_print = False  # Control printing for debugging

        for subtitle in subtitles:
            # Check for Japanese text under the 'ja' key
            if 'ja' in subtitle:
                ja_text = subtitle['ja']
                custom_print(enable_print, "Initial text:", ja_text)

                # Apply preprocessing steps
                ja_text = self.rearrange_brackets(ja_text)
                custom_print(enable_print, "After rearranging brackets:", ja_text)

                ja_text = self.fill_blank_of_katakana_without_furigana(ja_text)
                custom_print(enable_print, "After filling in the blank of <katakana>[]:", ja_text)

                ja_text = self.convert_katakana_in_brackets_to_hiragana(ja_text)
                custom_print(enable_print, "After converting katakana in brackets to hiragana:", ja_text)

                ja_text = self.clean_triplicated_sequences(ja_text)
                custom_print(enable_print, "After simplifying triplicated sequences:", ja_text)

                ja_text = self.clean_duplicated_kanji_hiragana_sequence(ja_text)
                custom_print(enable_print, "After converting duplicated kanji-hiragana sequence:", ja_text)

                ja_text = self.clean_redundant_hiragana_sequence(ja_text)
                custom_print(enable_print, "After removing redundant hiragana sequence:", ja_text)

                ja_text = self.clean_duplicated_hiragana_inside_angle_brackets(ja_text)
                custom_print(enable_print, "After simplifying duplicated hiragana inside angle brackets:", ja_text)

                ja_text = self.convert_standalone_angle_to_square_brackets(ja_text)
                custom_print(enable_print, "After converting standalone angle to square brackets:", ja_text)

                ja_text = self.preprocess_text_for_furigana(ja_text)
                custom_print(enable_print, "After preprocessing text for furigana:", ja_text)


                # Update the Japanese text in the subtitle
                subtitle['ja'] = ja_text
                custom_print(enable_print, "Final text:", ja_text)

        return subtitles



    def save_translated_subtitles_to_ass(self, translated_subtitles):
        
        """Save the translated subtitles to an ASS file with text wrapping based on video orientation."""
        

        wrapping_limit_half_width_default = self.wrapping_limit_half_width_default

        wrapping_limit_half_width_ja = wrapping_limit_half_width_default

        translated_subtitles = self.preprocess_japanese_ruby(translated_subtitles)

        ass_content = self.generate_ass_header()
        for index, item in enumerate(translated_subtitles, start=1):
            # Convert HH:MM:SS,mmm format to seconds
            def convert_to_seconds(t):
                parts = re.split('[:|,]', t)
                if len(parts) == 4:
                    h, m, s, ms = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
                elif len(parts) == 3:
                    # Fallback in case the format is not as expected
                    h, m, s = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + float(s.replace(',', '.'))
                else:
                    raise Exception(f"The format of timestamps is not recognized: {t}. ")
                
                return total_seconds
            
            # Format start and end times to ensure two decimal places with comma
            start_seconds = convert_to_seconds(item['start'])
            end_seconds = convert_to_seconds(item['end'])
            start = "{:02d}:{:02d}:{:05.2f}".format(int(start_seconds // 3600), int((start_seconds % 3600) // 60), start_seconds % 60)#.replace('.', ',')
            end = "{:02d}:{:02d}:{:05.2f}".format(int(end_seconds // 3600), int((end_seconds % 3600) // 60), end_seconds % 60)#.replace('.', ',')

            dialogue_lines = []

            # Process languages in a specific order if needed, then all additional languages
            preferred_order = ['zh', 'en', 'ja', "ar", "ko", "es", "vi", "fr"]  # Example: Start with Chinese, then English, then Japanese
            handled_keys = set(preferred_order)

            # Add preferred languages first
            for lang in preferred_order:
                if lang in item:
                    text = item[lang]
                    
                    is_cjk = lang in ['zh', 'ja']  # Assuming CJK for Chinese and Japanese
                    
                    if lang == "ja":
                        wrapping_limit_half_width = wrapping_limit_half_width_ja
                    else:
                        wrapping_limit_half_width = wrapping_limit_half_width_default


                    wrapped_text_lines = self.wrap_text(text, wrapping_limit_half_width, is_cjk=is_cjk)
                    dialogue_line = '\\N'.join(wrapped_text_lines)



                    if lang == 'ja':
                        dialogue_line = self.convert_furigana_to_ass(dialogue_line)

                    if lang == "ko":
                        dialogue_line = self.convert_hanja_to_ass(dialogue_line)


                    if lang == "vi":
                        dialogue_line = self.convert_chuhan_to_ass(dialogue_line)


                    if lang == "ar":
                        style = "Arabic"
                    elif lang == "zh":
                        style = "Chinese"
                    elif lang == "en":
                        style = "English"
                    elif lang == "ja":
                        style = "Japanese"
                    elif lang == "es":
                        style = "Spanish"
                    elif lang == "fr":
                        style = "French"
                    elif lang == "vi":
                        style = "Vietnamese"
                    else:
                        style = "Default"

                    subtitle_line = f"Dialogue: 0,{start},{end},{style},,0,0,0,,{dialogue_line}"
                    ass_content += subtitle_line + "\n"

            # Add any additional languages present
            additional_languages = {k: v for k, v in item.items() if k not in handled_keys.union(['start', 'end'])}
            for lang_code, text in additional_languages.items():
                # Additional languages will use the default style as this block does not handle style differentiation
                dialogue_line = self.wrap_text(text, wrapping_limit_half_width_default, is_cjk=False)
                formatted_text = '\\N'.join(dialogue_line)
                subtitle_line = f"Dialogue: 0,{start},{end},Default,,0,0,0,,{formatted_text}"
                ass_content += subtitle_line + "\n"

        # Write the constructed ASS content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(ass_content)
        print(f"Subtitles have been processed and saved successfully to {self.output_sub_path}.")


    def wrap_text(self, text, wrapping_limit_half_width, is_cjk=False):
        if not is_cjk:
            # For non-CJK text, directly use the wrapping function.
            # return self.cjkwrap_punctuation(text, wrapping_limit_half_width)
            return [text]

        # Step 1: Wrap the text into lines.
        wrapped_lines = self.cjkwrap_punctuation(text, wrapping_limit_half_width)

        # Step 2: Join lines with '###' to mark original line breaks.
        joined_text = '###'.join(wrapped_lines)

        # if "歴史" in joined_text:
        #     print("joined_text: ", joined_text)

        # Step 3: Correct breakpoints only for structured texts that were split.
        def correct_breakpoints(match):
            structured_text = match.group(0)

            # if "歴史" in joined_text:
            #     for i in range(4):
            #         try:
            #             print(match.group(i))
            #         except:
            #             pass

            # If '###' is inside the structured text, it indicates an incorrect break.
            if '###' in structured_text:
                # Remove '###' and keep the structure intact.
                return "###" + structured_text.replace('###', '') + "###"
            else:
                # If no '###' inside, return the structured text as is.
                return structured_text

        pattern = r'(<[^>]*>)(###)?\[[^\]]*\]|<[^>]*>|(\[[^\]]*\])'
        corrected_text = re.sub(pattern, correct_breakpoints, joined_text)

        # Step 4: Split the text back into lines at '###'.
        corrected_lines = corrected_text.split('###')

        joined_lines = self.join_lines_with_length_check(corrected_lines, wrapping_limit_half_width)

        # return corrected_lines
        return joined_lines

    @staticmethod
    def strip_brackets(input_string):
        # Regular expression pattern to match <, >, [, and ]
        pattern = r'[\<\>\[\]]'
        # Replace matched characters with an empty string
        stripped_string = re.sub(pattern, '', input_string)
        return stripped_string

    def join_lines_with_length_check(self, lines, wrapping_limit_half_width):
        final_lines = []
        current_line = ""
        for line in lines:
            if current_line:
                # Attempt to join with the next line and check if it exceeds the wrapping limit.
                test_line = current_line + line
                wrapped_test = self.cjkwrap_punctuation(self.strip_brackets(test_line), wrapping_limit_half_width)
                if len(wrapped_test) > 1:
                    # If joining exceeds the limit, finalize the current line and start a new one.
                    final_lines.append(current_line)
                    current_line = line
                else:
                    # Otherwise, update the current line to include the next line.
                    current_line = test_line
            else:
                current_line = line

        # Ensure the last accumulated line is added to the final output.
        if current_line:
            final_lines.append(current_line)

        return final_lines


    def cjkwrap_punctuation(self, text, width):
        """
        Custom wrapper for CJK text that prioritizes wrapping at punctuation,
        and adjusts lines to avoid starting with punctuation.
        """


        wrapped_lines = cjkwrap.wrap(text, width)



        # Define full-width and half-width punctuation marks for CJK text
        punctuations = ".。、，,！!？?；;：:「」『』（）()【】[]《》<>「」『』“”\"\""
        # punctuations = ".。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""
        # punctuations = "。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""

        

        # Adjust lines to move punctuation from the beginning of a line to the end of the previous line
        for i in range(1, len(wrapped_lines)):
            if wrapped_lines[i][0] in punctuations:
                # Move punctuation to the end of the previous line if it doesn't exceed width
                if len(wrapped_lines[i-1]) + 1 <= width:
                    wrapped_lines[i-1] += wrapped_lines[i][0]  # Move punctuation to the end of the previous line
                    wrapped_lines[i] = wrapped_lines[i][1:]  # Remove punctuation from the current line

        # Handle case where the first line starts with punctuation and there's no previous line to adjust
        # This might involve a specific strategy, such as re-wrapping with cjkwrap if needed
        # For simplicity, this case is not explicitly handled here but should be considered based on your requirements

        return wrapped_lines



if __name__ == '__main__':
    
    # Example usage:
    input_json_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.json'
    output_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/translated_subtitles.srt'
    input_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.srt'


    openai_client = OpenAI()
    subtitles_processor = SubtitlesTranslator(openai_client, input_json_path, input_sub_path, output_sub_path)
    subtitles_processor.process_subtitles()

===== ./subtitle_translate.py =====
import json
import json5
import re
import traceback
import openai

from packaging import version
import json
import os

from concurrent.futures import ThreadPoolExecutor, as_completed



from lazyedit.openai_version_check import OpenAI


from lazyedit.utils import JSONParsingError, JSONValidationError
from lazyedit.utils import safe_pretty_print, sample_texts, find_font_size
from lazyedit.openai_request import OpenAIRequestBase
from lazyedit.languages import LANGUAGES, TO_LANGUAGE_CODE

from datetime import datetime
from pprint import pprint

import cjkwrap

import glob

import numpy as np

import unicodedata



class SubtitlesTranslator(OpenAIRequestBase):
    def __init__(self, 
        openai_client, 
        input_json_path, 
        input_sub_path, 
        output_json_path,
        output_sub_path,
        video_length=None,
        video_width=1080,
        video_height=1920,
        max_retries=3,
        use_cache=False,
        *args, **kwargs
    ):
        kwargs["use_cache"] = use_cache
        kwargs["max_retries"] = max_retries

        super().__init__(*args, **kwargs)


        self.client = openai_client
        self.input_json_path = input_json_path
        self.input_sub_path = input_sub_path
        self.output_json_path = output_json_path
        self.output_sub_path = output_sub_path

        self.base_filename = os.path.splitext(os.path.basename(self.input_json_path))[0]
        self.datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

        self.video_length = video_length
        self.video_width = video_width
        self.video_height = video_height
        self.base_width = 1920
        self.base_height = 1080

        self.wrapping_limit_half_width_default = 80  # Default wrapping_limit_half_width for landscape
        self.portrait_scale = 0.7   

        
        if not self.is_video_landscape:
            self.wrapping_limit_half_width_default = int(self.wrapping_limit_half_width_default * 0.5)  # Adjust wrapping_limit_half_width for portrait videos


        # self.is_video_landscape = video_width > video_height
        self.font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"

        self.translation_log_folder = 'translation_logs'

        self.subtitles = None

        # self.max_retries = max_retries
        # self.use_cache = use_cache

        self.flags = {
            'zh': '🇨🇳',  # China for Mandarin
            'en': '🇬🇧',  # United Kingdom for English
            'ja': '🇯🇵',  # Japan
            'ar': '🇸🇦',  # Saudi Arabia for Arabic
            'ko': '🇰🇷',  # Korea for Korean
            'es': '🇪🇸',  # Spain for Spanish
            'vi': '🇻🇳',  # Vietnam
            'fr': '🇫🇷',  # France
            'de': '🇩🇪',  # Germany for German
            'it': '🇮🇹',  # Italy for Italian
            'ru': '🇷🇺',  # Russia for Russian
            'pt': '🇵🇹',  # Portugal for Portuguese (Note: Brazil might use 🇧🇷 depending on the context)
            'nl': '🇳🇱',  # Netherlands for Dutch
            'sv': '🇸🇪',  # Sweden for Swedish
            'no': '🇳🇴',  # Norway for Norwegian
            'da': '🇩🇰',  # Denmark for Danish
            'fi': '🇫🇮',  # Finland for Finnish
            'pl': '🇵🇱',  # Poland for Polish
            'tr': '🇹🇷',  # Turkey for Turkish
            'el': '🇬🇷',  # Greece for Greek
            'he': '🇮🇱',  # Israel for Hebrew
            'th': '🇹🇭',  # Thailand for Thai
            'cs': '🇨🇿',  # Czech Republic for Czech
            'ro': '🇷🇴',  # Romania for Romanian
            'hu': '🇭🇺',  # Hungary for Hungarian
            'sk': '🇸🇰',  # Slovakia for Slovak
            'bg': '🇧🇬',  # Bulgaria for Bulgarian
            'sr': '🇷🇸',  # Serbia for Serbian
            'hr': '🇭🇷',  # Croatia for Croatian
            'sl': '🇸🇮',  # Slovenia for Slovenian
            'lt': '🇱🇹',  # Lithuania for Lithuanian
            'lv': '🇱🇻',  # Latvia for Latvian
            'et': '🇪🇪',  # Estonia for Estonian
            'id': '🇮🇩',  # Indonesia for Indonesian
            'ms': '🇲🇾',  # Malaysia for Malay
            'fil': '🇵🇭',  # Philippines for Filipino
            'sw': '🇹🇿',  # Tanzania for Swahili (Note: also widely spoken in Kenya 🇰🇪)
            'uk': '🇺🇦',  # Ukraine for Ukrainian
            'bn': '🇧🇩',  # Bangladesh for Bengali
            'hi': '🇮🇳',  # India for Hindi
            'fa': '🇮🇷',  # Iran for Persian (Farsi)
            'ur': '🇵🇰',  # Pakistan for Urdu
            'mn': '🇲🇳',  # Mongolia for Mongolian
            'ne': '🇳🇵',  # Nepal for Nepali
        }

        print("Using translation cache: ", use_cache)

        # self.ensure_log_folder_exists()



    @property
    def is_video_landscape(self):
        """Determine if the video is landscape or portrait based on class variables."""
        return self.video_width > self.video_height

    def get_filename(self, lang="ja", idx=0, timestamp=None):
        # base_filename = os.path.splitext(os.path.basename(self.input_json_path))[0]
        # datetime_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        base_filename = self.base_filename
        # datetime_str = self.datetime_str

        if timestamp:
            filename = f"{self.translation_log_folder}/{base_filename}-part{idx}-{lang}-{timestamp}.json"
        else:
            filename = f"{self.translation_log_folder}/{base_filename}-part{idx}-{lang}.json"

        return filename
    


    def load_subtitles_from_json(self):
        """Load subtitles from a JSON file."""
        with open(self.input_json_path, 'r', encoding='utf-8') as file:
            return json5.load(file)

    @staticmethod
    def correct_json_string(s):
        # Remove trailing commas after object properties or array elements
        corrected_s = re.sub(r',\s*}', '}', s)
        corrected_s = re.sub(r',\s*\]', ']', corrected_s)
        return corrected_s

    def extract_and_parse_json(self, text):
        """Extract and parse JSON from text, handling potential parsing issues."""
        bracket_pattern = r'\[.*\]'
        matches = re.findall(bracket_pattern, text, re.DOTALL)
        # json_string = ""

        if not matches:
            raise JSONParsingError("No JSON string found in text", text, text)
        json_string = matches[0].replace('\n', '')

        # pprint(json_string)
        safe_pretty_print(json_string)

        try:
            json_string = self.correct_json_string(json_string)
            return json5.loads(json_string)
        except ValueError as e:
            traceback.print_exc()
            raise JSONParsingError(f"JSON Decode Error: {e}", json_string, text)

    def validate_translated_subtitles(self, subtitles, required_fields=["start", "end", "en", "zh"]):
        """Validate the structure of translated subtitles."""
        # required_fields = ["start", "end", "en", "zh"]
        for subtitle in subtitles:
            if not all(field in subtitle for field in required_fields):
                raise JSONValidationError("Subtitle missing one of the required fields: " + ", ".join(required_fields))



    def translate_and_merge_subtitles(self, subtitles):
        self.subtitles = subtitles

        """Splits subtitles into 1-minute batches and processes each batch in parallel."""
        # subtitles = self.load_subtitles_from_json()

        # Splitting subtitles into 1-minute batches
        batches = self.split_subtitles_into_batches(subtitles)

        # Process each batch and accumulate results in parallel
        translated_subtitles = self.process_batches_in_parallel(batches)

        # Save the final translated subtitles
        # self.save_translated_subtitles_to_ass(translated_subtitles)
        print("All subtitles have been processed and saved successfully.")

        return translated_subtitles

    def process_batches_in_parallel(self, batches):
        """Process subtitle batches in parallel using ThreadPoolExecutor."""
        translated_subtitles = []

        with ThreadPoolExecutor(max_workers=1) as executor:
            # Submit all batches to be processed in parallel
            future_to_batch = {executor.submit(self.translate_and_merge_subtitles_in_batch, batch, i): batch for i, batch in enumerate(batches)}

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    translated_batch = future.result()
                    translated_subtitles.extend(translated_batch)
                except Exception as exc:
                    print(f'Batch {batch} generated an exception: {exc}')
        
        # Optional: Sort the merged list by start timestamps if necessary
        translated_subtitles.sort(key=lambda x: datetime.strptime(x['start'], '%H:%M:%S,%f'))

        return translated_subtitles

    def split_subtitles_into_batches(self, subtitles):
        """Splits subtitles into 1-minute batches based on their timestamps."""
        batches = []
        current_batch = []
        current_batch_start_time = None

        for subtitle in subtitles:
            start_time = datetime.strptime(subtitle["start"], '%H:%M:%S,%f')
            if current_batch_start_time is None:
                current_batch_start_time = start_time

            if (start_time - current_batch_start_time).total_seconds() > 60:
                # Start a new batch if the current subtitle start time exceeds 1 minute from the batch's start time
                batches.append(current_batch)
                current_batch = [subtitle]
                current_batch_start_time = start_time
            else:
                current_batch.append(subtitle)

        # Add the last batch if it contains subtitles
        if current_batch:
            batches.append(current_batch)

        return batches

    def translate_and_merge_subtitles_in_batch(self, subtitles, idx):
        """Merge translations from multiple languages' functions in parallel."""
        # Define a dictionary of language codes to their respective translation functions
        translation_tasks = {
            'major': self.translate_and_merge_subtitles_major_languages,
            'ja': self.translate_and_merge_subtitles_ja,
            'ko': self.translate_and_merge_subtitles_ko,
            'vi': self.translate_and_merge_subtitles_vi, 
            'minor': self.translate_and_merge_subtitles_minor_languages
        }

        with ThreadPoolExecutor(max_workers=len(translation_tasks)) as executor:
            futures = {
                executor.submit(translation_func, subtitles, idx): lang_code
                for lang_code, translation_func in translation_tasks.items()
            }

            all_translations = {}

            for future in as_completed(futures):
                lang_code = futures[future]
                try:
                    translations = future.result()
                    all_translations[lang_code] = translations
                except Exception as exc:
                    print(f'{lang_code} translation generated an exception: {exc}')

        # Merge translations
        timestamps_dict = {}

        for translations in all_translations.values():
            for translation in translations:
                key = (translation['start'], translation['end'])

                # Initialize the dictionary entry if it doesn't exist
                if key not in timestamps_dict:
                    timestamps_dict[key] = {'start': translation['start'], 'end': translation['end']}
                
                # Dynamically add all language translations, regardless of the number of languages
                for sub_lang, text in translation.items():
                    if sub_lang not in ['start', 'end']:  # Skip timestamp keys
                        timestamps_dict[key][sub_lang] = text

        merged_translations = list(timestamps_dict.values())
        merged_translations.sort(key=lambda x: x['start'])

        return merged_translations


    def translate_and_merge_subtitles_major_languages(self, subtitles, idx):
        """Translate and merge subtitles using the OpenAI API."""

        print("Translating subtitles into other languages...")
        
        # client = self.client


        # # Define a sample JSON structure for validation purposes
        # sample_json_structure = [
        #     {
        #         "start": "timestamp",
        #         "end": "timestamp",
        #         "en": "English text",
        #         "zh": "Chinese text",
        #         "ar": "Arabic text",
        #         # "...": "Text in the original language, if not in the listed before. "
        #     }
        # ]

        # sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)


        # Define a JSONC string with comments
        sample_json_string = """
        [
            {
                "start": "timestamp",  // Start time of the subtitle
                "end": "timestamp",    // End time of the subtitle
                "en": "English text",  // English translation
                "zh": "Chinese text",  // Chinese translation
                "ar": "Arabic text",    // Arabic translation
                // "...": "Text in the original language, if not in the listed before."
            }
        ]
        """

        # Parse the JSONC string into a Python object
        sample_json_structure = json5.loads(sample_json_string)


        system_content = "Translate and merge mixed language subtitles into Chinese, English and Arabic, providing coherent and accurate translations."
        prompt = (
            "Below are mixed language subtitles extracted from a video, including timestamps, "
            "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
            "is presented with English (en), Chinese (zh), and Arabic (ar) translations, "
            "maintaining the original timestamps.\n\n"

            # "If a subtitle is already in English, provide the corresponding Chinese and Arabic translation, and vice versa. "
            # "For subtitles in any other language, keep the original text but also provide translations in "
            # "English, Chinese and Arabic. \n\n"

            # "Fullfill the instructions/requests in subtitles per se for other languages with iso_code_639_1 language key. "
            # "If I said in subtitles that I want to know or I don't know how to say something, "
            # "provide the whole subtitles in that language. "
         
            # "Correct some apparent speech recognition error and inconsistencies, "
            # "especially homonym and mumble in both origin and its translation based on the context.\n\n"
    
            "Process the following subtitles, ensuring translations are accurate and coherent, "
            "and format the output as shown in the example.\n\n"

            # "Please provide a complete and accurate translation and formatting for each subtitle entry."

            # "PLEASE DON'T CHANGE ORIGINAL TIMESTAMPS.\n\n"
            
            # "Note that the original timestamps should be PRESERVED for each entry.\n\n"
            
            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"
            

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json"
            f"{sample_json_string}\n"
            "```"
        )






        translated_subtitles = self.send_request_with_retry(
            prompt, 
            system_content=system_content, 
            sample_json=sample_json_structure,
            filename=self.get_filename(lang="major", idx=idx)
        )

        print("Translated subtitles (Major): \n")
        pprint(translated_subtitles)



        return translated_subtitles


    def translate_and_merge_subtitles_minor_languages(self, subtitles, idx):
        """Translate and merge subtitles using the OpenAI API into Spanish, French, and Vietnamese by leveraging the 
        translate_and_merge_subtitles_with_specified_languages function."""

        # Define the list of minor languages to translate to
        minor_languages = ["Spanish", "French", "Russian"]
        
        # Call the translate_and_merge_subtitles_with_specified_languages function with the minor languages
        translated_subtitles = self.translate_and_merge_subtitles_with_specified_languages(subtitles, minor_languages, idx)
        
        return translated_subtitles


    def translate_and_merge_subtitles_with_specified_languages(self, subtitles, requested_languages, idx):
        """Translate and merge subtitles into the specified languages using the OpenAI API."""
        print("Translating subtitles into specified languages...")

        # Convert the list of language names into language codes and full names with initial capitalization
        language_codes = []
        language_full_names = []
        for lang_name in requested_languages:
            code = TO_LANGUAGE_CODE.get(lang_name.lower())
            if code and code in LANGUAGES:
                language_codes.append(code)
                language_full_names.append(LANGUAGES[code].capitalize())

        # Generate a sample JSON structure dynamically based on requested language codes
        sample_json_structure = [{"start": "timestamp", "end": "timestamp"}]
        for code in language_codes:
            sample_json_structure[0][code] = f"{LANGUAGES[code]} text"

        sample_json_string = json.dumps(sample_json_structure, indent=2)
        # sample_json_structure = json5.loads(sample_json_string)

        # Generate a human-readable string of the full language names for the prompt
        languages_list_str = ', '.join(language_full_names[:-1]) + ', and ' + language_full_names[-1] if len(language_full_names) > 1 else language_full_names[0]

        print(f"The specified languages are: {languages_list_str}. ")

        system_content = f"Translate mixed language subtitles into {languages_list_str}, providing coherent and accurate translations."
        prompt = (
            "Below are mixed language subtitles extracted from a video, including timestamps, "
            "language indicators, and the subtitle text itself. The task is to ensure that each subtitle "
            f"is presented with translations in {languages_list_str}, "
            "maintaining the original timestamps.\n\n"

            # "Correct some apparent speech recognition error and inconsistencies, "
            # "especially homonym and mumble in both origin and its translation based on the context.\n\n"

            "Process the following subtitles, ensuring translations are accurate and coherent, "
            "and format the output as shown in the example.\n\n"

            # "PLEASE DON'T CHANGE ORIGINAL TIMESTAMPS.\n\n"

            # "Note that the original timestamps should be PRESERVED for each entry.\n\n"

            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"


            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json"
            f"{sample_json_string}\n"
            "```"
        )

        lang_str = "_".join(language_codes)
        translated_subtitles = self.send_request_with_retry(
            prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(lang=lang_str, idx=idx)
        )

        print("Translated subtitles (Specified Languages): \n")
        pprint(translated_subtitles)

        return translated_subtitles

    # Function to annotate Kanji and Katakana independently
    @staticmethod
    def annotate_kanji_katakana(subtitles):
        # Define the regular expression patterns for Kanji and Katakana
        kanji_pattern = r'[\u4e00-\u9faf]+'
        katakana_pattern = r'[\u30a0-\u30ff]+'
        
        # Function to wrap text in <>[]
        def replacer(match):
            return f'<{match.group(0)}>[]'
        
        # Annotate each item in the list
        for item in subtitles:
            # Annotate Kanji
            item['ja'] = re.sub(kanji_pattern, replacer, item['ja'])
            # Annotate Katakana
            item['ja'] = re.sub(katakana_pattern, replacer, item['ja'])
        
        return subtitles

    def translate_and_merge_subtitles_ja(self, subtitles, idx):
        """Request Japanese subtitles separately with specific formatting for furigana."""
        print("Translating subtitles to Japanese...")

        sample_json_structure = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ja": "Japanese text with optional furigana annotations"
            }
        ]

        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)

        # system_content = "Translate subtitles into Japanese, correcting any errors based on context."
        system_content = "Translate subtitles into Japanese, providing coherent and accurate translations."
        prompt = (
            "Translate the following subtitles into Japanese. "
            "\n\n"

            # "Correct speech recognition errors and inconsistencies based on context.\n\n"
            
            # "PLEASE DON'T CHANGE ORIGINAL TIMESTAMPS.\n\n"

            # "Note that the original timestamps should be PRESERVED for each entry.\n\n"

            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"
            
            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        
        translated_subtitles = self.send_request_with_retry(
            prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(lang="ja", idx=idx)
        )


        annotated_subtitles = self.annotate_kanji_katakana(translated_subtitles)

        print("annotated subtitles: \n")
        pprint(annotated_subtitles)

        translated_subtitles = self.add_furigana_for_japanese_subtitles(annotated_subtitles, idx)

        print("furigana subtitles: \n")
        pprint(translated_subtitles)

        return translated_subtitles

            


    def add_furigana_for_japanese_subtitles(self, subtitles, idx):
        """Request Japanese subtitles with specific formatting for furigana using the OpenAI API with retries."""

        sample_json_structure = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ja": "Japanese text with furigana annotations"
            }
        ]

        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)

        system_content = "Add furigana annotations to the provided Japanese text."
        prompt = (
            "Given the Japanese subtitles, add furigana annotations correctly based on the context. "
            "Use the format '<Kanji>[Furigana]' for annotations.\n\n"

            # "Correct any speech recognition errors and inconsistencies based on context.\n\n"
            
            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        # Utilize send_request_with_retry to handle API requests, including retries, caching, and JSON validation
        translated_subtitles = self.send_request_with_retry(
            prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(lang="furigana", idx=idx)
        )

        print("Translated subtitles (Japanese with furigana): \n")
        pprint(translated_subtitles)

        return translated_subtitles


    def translate_and_merge_subtitles_ko(self, subtitles, idx):
        """Request Korean subtitles with specific considerations."""
        print("Translating subtitles to Korean...")

        # Define a sample JSON structure for Korean subtitles
        sample_json_structure = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "ko": "Korean text"  # Use "ko" for Korean text
            }
        ]

        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)

        # system_content = "Translate subtitles into Korean, correcting any errors based on context."
        system_content = "Translate subtitles into Korean, providing coherent and accurate translations."
        prompt = (
            "Translate the following subtitles into Korean. "
            "\n\n"
            # "Correct speech recognition errors and inconsistencies based on context.\n\n"

            # "PLEASE DON'T CHANGE ORIGINAL TIMESTAMPS. \n\n"

            # "Note that the original timestamps should be PRESERVED for each entry.\n\n"

            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        # Use the function for sending requests with retries, similarly to the Japanese version
        translated_subtitles = self.send_request_with_retry(
            prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(lang="ko", idx=idx)
        )

        # Here, you might add any specific post-processing for Korean subtitles if necessary
        # For example, annotating certain phrases, cultural references, or anything specific to Korean

        translated_subtitles = self.replace_hangul_with_hanja(translated_subtitles, idx)

        print("Translated subtitles (Korean): \n")
        pprint(translated_subtitles)

        return translated_subtitles


    


    def replace_hangul_with_hanja(self, subtitles, idx):
        """Annotate Korean subtitles with Hanja and conservatively replace original text with Hanja annotations."""

        print("Annotating Korean subtitles with Hanja...")

        system_content = "Annotate Korean text with original Chinese Hanja."

        # Sample JSON structure expected in the response
        sample_json_structure = {
            "korean_with_annotation": "",
            "korean_hanja_pairs": [
                # {"korean_part": "한자 ", "hanja": "漢字 ", "roman": "hanja"}
                {"korean_part": "", "hanja": ""}
            ]
        }
        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)


        # Helper function for conservative replacement in the original text
        def conservative_replace(original_text, hanja_pairs):
            """Perform conservative replacement of Korean text with Hanja annotations."""
            updated_text = original_text
            last_pos = 0  # Initialize the position tracker to the start of the string

            for pair in hanja_pairs:
                korean_part = pair.get("korean_part", "")
                hanja = pair.get("hanja", "")
                pronunciation = pair.get("roman", "")

                # Check if the 'hanja' part is actually Hangul or if there's no Hanja present
                if re.match(r'^[\uAC00-\uD7AF]+$', hanja) or len(hanja) == 0:
                    continue  # Skip replacement if Hanja is actually Hangul or if Hanja is missing

                # replace_with = f"<{hanja}>[{korean_part}]({pronunciation})"
                replace_with = f"<{hanja}>[{korean_part}]"

                # Start search from the last modification position to only look forward
                start_pos = updated_text.find(korean_part, last_pos)
                if start_pos != -1:
                    end_pos = start_pos + len(korean_part)
                    updated_text = updated_text[:start_pos] + replace_with + updated_text[end_pos:]
                    last_pos = start_pos + len(replace_with)  # Update last_pos to the end of the newly inserted segment

            return updated_text

        def process_subtitle(subtitle):
            """Construct and send a request for annotating a single subtitle with Hanja."""
            prompt = (
                "For words that have etymological Hanja alternatives (due to their Sino-Korean origin), "
                "I want to find the corresponding Chinese character origin to replace the Hangul. "
                "However, it's crucial that the Hangul in parenthesis followed by these traditional Hanja "
                "are put in brackets to maintain clarity as in this format: (hangul)[hanja].\n\n"
                # "Please also provide Roman pronunciation of the Hangul. "
                f"Korean to be annotated: {subtitle['ko']}\n\n"
                "I don't need step by step explanation. "
                "ONLY and ALWAYS return a valid JSON back:\n"
                f"```json\n{sample_json_string}\n```"
            )

            response = self.send_request_with_retry(
                prompt,
                system_content=system_content,
                sample_json=sample_json_structure,
                filename=self.get_filename(lang="hanja", idx=idx, timestamp=self.format_subtitle_range(subtitle))
            )
            # Assume response is structured correctly
            annotated_text = response.get("korean_with_annotation", "")
            hanja_pairs = response.get("korean_hanja_pairs", [])
            
            # Update the original text using the conservative replace function
            updated_original_text = conservative_replace(subtitle["ko"], hanja_pairs)

            return {
                "start": subtitle["start"],
                "end": subtitle["end"],
                "ko": updated_original_text,  # Updated original Korean text with Hanja annotations
                # "annotated": annotated_text,  # Full annotated text (if needed separately)
                # "hanja_pairs": hanja_pairs  # Hanja pairs for reference
            }

        # annotated_subtitles = [process_subtitle(sub) for sub in subtitles]

        # Parallel execution with error handling to return original subtitles if an error occurs
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(process_subtitle, sub): sub for sub in subtitles}
            for future in as_completed(futures):
                sub = futures[future]
                try:
                    result = future.result()
                    sub.update(result)
                except Exception as exc:
                    print(f"Subtitle processing generated an exception: {exc}")
                    # If there's an error, the original subtitle remains unchanged

        print("Korean with Hanja: \n")
        pprint(subtitles)

        return subtitles


    def translate_and_merge_subtitles_vi(self, subtitles, idx):
        """Request Vietnamese subtitles with specific considerations."""
        print("Translating subtitles to Vietnamese...")

        # Define a sample JSON structure for Vietnamese subtitles
        sample_json_structure = [
            {
                "start": "timestamp",
                "end": "timestamp",
                "vi": "Vietnamese text"  # Use "vi" for Vietnamese text
            }
        ]

        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)

        # system_content = "Translate subtitles into Vietnamese, correcting any errors based on context."
        system_content = "Translate subtitles into Vietnamese, providing coherent and accurate translations."
        prompt = (
            "Translate the following subtitles into Vietnamese. "
            "\n\n"

            # "Correct speech recognition errors and inconsistencies based on context.\n\n"

            # "PLEASE DON'T CHANGE ORIGINAL TIMESTAMPS.\n\n"

            # "Note that the original timestamps should be PRESERVED for each entry.\n\n"

            "Please PRESERVE ALL the original timestamps for EACH ENTRY.\n\n"

            "Subtitles to process:\n"
            f"{json.dumps(subtitles, indent=2, ensure_ascii=False)}\n\n"

            "ONLY and ALWAYS return a valid JSON back:\n"
            "```json\n"
            f"{sample_json_string}\n"
            "```"
        )

        translated_subtitles = self.send_request_with_retry(
            prompt,
            system_content=system_content,
            sample_json=sample_json_structure,
            filename=self.get_filename(lang="vi", idx=idx)
        )

        translated_subtitles = self.replace_viet_with_chuhan(translated_subtitles, idx)

        print("Translated subtitles (Vietnamese): \n")
        pprint(translated_subtitles)

        return translated_subtitles

    def replace_viet_with_chuhan(self, subtitles, idx):
        """Annotate Vietnamese subtitles with Chu-Han and conservatively replace original text with Chu-Han annotations."""
        
        print("Annotating Vietnamese subtitles with Chu-Han...")

        system_content = "Annotate Vietnamese text with original Chinese Chu-Han."
        
        # Sample JSON structure expected in the response
        sample_json_structure = {
            "viet_with_annotation": "",
            "viet_chuhan_pairs": [
                {"viet_part": "", "chuhan": ""}
            ]
        }
        sample_json_string = json.dumps(sample_json_structure, indent=2, ensure_ascii=False)

        def conservative_replace(original_text, chuhan_pairs):
            """Perform conservative replacement of Viet text with Chu-Han annotations, excluding text within <>[]."""
            updated_text = original_text
            last_pos = 0  # Keep track of the last position after the most recent replacement

            for pair in chuhan_pairs:
                viet_part = pair.get("viet_part", "")
                chuhan = pair.get("chuhan", "")

                if len(chuhan) == 0 or len(viet_part) == 0:
                    continue

                replace_with = f"<{chuhan}>[{viet_part}]"

                # Start search from the last modification position
                start_pos = updated_text.find(viet_part, last_pos)

                if start_pos != -1:
                    end_pos = start_pos + len(viet_part)
                    updated_text = updated_text[:start_pos] + replace_with + updated_text[end_pos:]
                    last_pos = start_pos + len(replace_with)  # Update last_pos to the end of the newly inserted segment
            
            return updated_text


        def process_subtitle(subtitle):
            """Construct and send a request for annotating a single subtitle with Chu-Han."""
            prompt = (
                "For words that have etymological Chu-Han alternative (due to their Sino-Vietnamese origin), "
                "I want to find the corresponding Chinese origin to replace the Viet WORDS (provide consecutive words if exist). "
                "However, it's crucial that the Viet in parenthesis followed by these traditional Chu-Han "
                "are put in brackets to maintain clarity. "
                "Like this example: (viet)[chuhan].\n\n"
                f"Vietnamese to be annotated: {subtitle['vi']}\n\n"
                "I don't need step by step explanation. "
                "ONLY and ALWAYS return a valid JSON back:\n"
                f"```json\n{sample_json_string}\n```"
            )

            response = self.send_request_with_retry(
                prompt,
                system_content=system_content,
                sample_json=sample_json_structure,
                filename=self.get_filename(lang="chuhan", idx=idx, timestamp=self.format_subtitle_range(subtitle))
            )
            annotated_text = response.get("viet_with_annotation", "")
            chuhan_pairs = response.get("viet_chuhan_pairs", [])
            
            updated_original_text = conservative_replace(subtitle["vi"], chuhan_pairs)

            return {
                "start": subtitle["start"],
                "end": subtitle["end"],
                "vi": updated_original_text  # Updated original Vietnamese text with Chu-Han annotations
            }

        # Parallel execution with error handling to return original subtitles if an error occurs
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(process_subtitle, sub): sub for sub in subtitles if 'vi' in sub}
            for future in as_completed(futures):
                sub = futures[future]
                try:
                    result = future.result()
                    sub.update(result)
                except Exception as exc:
                    print(f"Subtitle processing generated an exception: {exc}")
                    # If there's an error, the original subtitle remains unchanged

        print("Vietnamese with Chu-Han: \n")
        pprint(subtitles)

        return subtitles

    @staticmethod
    def format_subtitle_range(subtitle):
        """
        Takes a subtitle dictionary with 'start' and 'end' timestamp strings,
        removes all colons from the timestamps, and returns a concatenated string
        with a dash between the start and end timestamps.

        :param subtitle: A dictionary with 'start' and 'end' keys.
        :return: A string combining the start and end times, with colons removed.
        """
        start_formatted = subtitle["start"].replace(":", "").replace(",", "")
        end_formatted = subtitle["end"].replace(":", "").replace(",", "")
        return f"{start_formatted}-{end_formatted}"

    def save_translated_subtitles_to_srt(self, translated_subtitles):
        """Save the translated subtitles to an SRT file, ensuring language order."""
        srt_content = ""
        for index, subtitle in enumerate(translated_subtitles, start=1):
            # Start the subtitle entry with its sequence number and time range
            srt_content += f"{index}\n{subtitle['start']} --> {subtitle['end']}\n"
            
            # Always add Chinese (zh) translation first if it exists
            if 'zh' in subtitle:
                srt_content += f"{subtitle['zh']}\n"
            
            # Add English (en) translation
            if 'en' in subtitle:
                srt_content += f"{subtitle['en']}\n"
            
            # Add any additional languages present, excluding 'start', 'end', 'zh', and 'en'
            additional_languages = {k: v for k, v in subtitle.items() if k not in ['start', 'end', 'zh', 'en']}
            for lang_code, text in additional_languages.items():
                srt_content += f"{text}\n"
            
            # Separate subtitles with an empty line
            srt_content += "\n"
        
        # Write the constructed SRT content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(srt_content)


    def process_subtitles(self):
        """Main process to load, translate, merge, and save subtitles."""
        subtitles = self.load_subtitles_from_json()
        translated_subtitles = self.translate_and_merge_subtitles(subtitles)
        # self.save_translated_subtitles_to_srt(translated_subtitles)
        self.save_translated_subtitles_to_ass(translated_subtitles)
        self.save_translated_subtitles_to_json(translated_subtitles)
        print("Subtitles have been processed and saved successfully.")


    def rearrange_brackets(self, ja_text):
        """
        Detects and rearranges text where [] are found inside <> to follow the <>[] pattern.
        """
        
        # Define a regex pattern to match <...[...]...>
        pattern = re.compile(r'<([^>\[\]]+)\[([^\[\]]+)\]([^>\[\]]*)>')
        
        # Function to rearrange the matched pattern to <...> [...]
        def rearrange(match):
            # Extract the parts of the match
            before_bracket = match.group(1)
            inside_bracket = match.group(2)
            after_bracket = match.group(3)
            
            # Return the rearranged format
            return f'<{before_bracket}{after_bracket}>[{inside_bracket}]'

        # Apply the rearrange function to all matching patterns in the text
        rearranged_text = pattern.sub(rearrange, ja_text)

        return rearranged_text

    def remove_preceding_repetition(self, ja_text):
        """
        Removes a word or phrase that is repeated immediately before <...> when
        the content inside <> is the same as the preceding word.
        """
        
        # Define a regex pattern to match repetition before <>
        pattern = re.compile(r'(\S+)<\1>')
        
        # Function to replace the matched pattern with just the content in angle brackets
        def deduplicate(match):
            # Return only the content within angle brackets
            return f'<{match.group(1)}>'

        # Apply the deduplication function to all matching patterns in the text
        deduplicated_text = pattern.sub(deduplicate, ja_text)

        return deduplicated_text


    def clean_triplicated_sequences(self, ja_text):
        """
        Simplifies text by converting sequences of the form same<same>[same] to just 'same'.
        """
        # Define a regex pattern to match the 'same<same>[same]' structure
        pattern = re.compile(r'(?:(\S+)<\1>\[\1\])')
        
        # Function to perform the substitution
        def replacement(match):
            # Return just the first 'same' part of the match
            return match.group(1)
        
        # Remove same hiragana<same hiragana>[ same hiragana] anywhere
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text



    


    def clean_duplicated_kanji_hiragana_sequence(self, ja_text):
        """
        Converts sequences of the form 'same kanji<same kanji>[hiragana]' to '<kanji>[hiragana]'.
        """
        # Define a regex pattern to match the specified structure
        pattern = re.compile(r'([一-龠ァ-ヶ々ー]+)(<\1>)\[([ぁ-ん]+)\]')
        
        # Function to perform the substitution
        def replacement(match):
            # for i in range(4):
            #     print(match.group(i))
            
            # Return the simplified structure '<kanji>[hiragana]'
            kanji = match.group(2)
            hiragana = match.group(3)
            return f'{kanji}[{hiragana}]'
        
        # Convert kanji<kanji>[hiragana] to <kanji>[hiragana], preserving the sequence (two kanji seq is the same)
        # Apply the replacement function to all matching patterns in the text
        simplified_text = pattern.sub(replacement, ja_text)

        return simplified_text

    def clean_redundant_hiragana_sequence(self, ja_text):
        """
        Modifies text by removing redundant angle and square brackets for matching hiragana or kanji sequences.
        - For hiragana immediately before or at the start, followed by <hiragana>[hiragana], it leaves just the hiragana.
        - For kanji followed by <hiragana>[hiragana], it replaces them with [hiragana].
        """

        

        # Remove <same hiragana>[same hiragana] when at the start or directly after punctuation or preceded by different hiragana
        # hiragana_pattern = re.compile(r'(^|(?<=[ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        hiragana_pattern = re.compile(r'(^|(?<=[、。！？>\]ぁ-ん]))<([ぁ-ん]+)>\[\2\]')
        simplified_text = hiragana_pattern.sub(r'\1\2', ja_text)
        
        # Convert kanji<hiragana>[hiragana] to kanji[hiragana], preserving the sequence
        kanji_hiragana_pattern = re.compile(r'(?<=[一-龠ァ-ヶ々ー])<([ぁ-ん]+)>\[\1\]')
        simplified_text = kanji_hiragana_pattern.sub(r'[\1]', simplified_text)

        
        
        return simplified_text

    def clean_duplicated_hiragana_inside_angle_brackets(self, ja_text):
        """
        Simplifies sequences of the form 'same hiragana<same hiragana>' to just 'hiragana'.
        """
        # Define a regex pattern to match hiragana sequences repeated before and inside angle brackets
        pattern = re.compile(r'<?([ぁ-ん]+)>?<\1>')

        
        # Perform the substitution to replace matched patterns with just the hiragana
        simplified_text = pattern.sub(r'\1', ja_text)

        return simplified_text


    def convert_standalone_angle_to_square_brackets(self, ja_text):
        """
        Converts standalone angle brackets to square brackets in the given text,
        while leaving <kanji>[furigana] pairs unchanged.
        """
        
        # Define a regex pattern to find standalone <> not followed directly by []
        # This pattern assumes 'standalone' means there's no furigana in square brackets immediately following
        pattern = re.compile(r'<([^>]+)>(?!\[)')
        
        # Replace found patterns with square brackets
        converted_text = pattern.sub(r'[\1]', ja_text)
        
        return converted_text






    # # Example usage
    # original_text = "<字幕>[じまく]上[じょう]でこの<日本語>[にほんご]の発音[はつおん]を学[まな]べるようにしています。"
    # preprocessed_text = preprocess_text_for_furigana(original_text)

    # print("Preprocessed Text:", preprocessed_text)

    def katakana_to_hiragana(self, katakana):
        """
        Converts a katakana string to hiragana.
        """
        katakana_hiragana_map = str.maketrans(
            "アイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワヲンガギグゲゴザジズゼゾダヂヅデドバビブベボパピプペポ",
            "あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもやゆよらりるれろわをんがぎぐげござじずぜぞだぢづでどばびぶべぼぱぴぷぺぽ"
        )
        return katakana.translate(katakana_hiragana_map)

    def fill_blank_of_katakana_without_furigana(self, ja_text):
        """
        Converts sequences of the form <katakana>[] to <katakana>[hiragana],
        where hiragana is the equivalent of the katakana text inside the angle brackets.
        """
        # Define a regex pattern to match <katakana>[] structures
        pattern = re.compile(r'<([ァ-ヶー]+)>\[\]')
        
        # Function to perform the substitution
        def replacement(match):
            # Convert the matched katakana to hiragana
            katakana_text = match.group(1)
            hiragana_text = self.katakana_to_hiragana(katakana_text)
            return f'<{katakana_text}>[{hiragana_text}]'
        
        # Apply the replacement function to all matching patterns in the text
        converted_text = pattern.sub(replacement, ja_text)

        return converted_text

    def convert_katakana_in_brackets_to_hiragana(self, ja_text):
        """
        Detects <Katakana>[Katakana] then converts the [Katakana] into [Hiragana].
        """
        # # Define a regex pattern to match <Katakana>[Katakana]
        # pattern = re.compile(r'<([ァ-ヶー]+)>\[\1\]')

        # Define a regex pattern to match both <Katakana>[Katakana] and Katakana[Katakana]
        pattern = re.compile(r'(?:(<([ァ-ヶー]+)>)|([ァ-ヶー]+))\[(\2|\3)\]')

        # # Use the katakana_to_hiragana function to convert matched katakana to hiragana in []
        # corrected_text = re.sub(pattern, lambda m: f'<{m.group(1)}>[{katakana_to_hiragana(m)}]', ja_text)

        def replacement(match):
            # Determine if the match includes angle brackets or not
            katakana = match.group(2) if match.group(2) else match.group(3)
            hiragana = self.katakana_to_hiragana(katakana)
            # Reconstruct the string with the katakana in brackets converted to hiragana
            if match.group(1):  # If katakana was enclosed in angle brackets
                return f'<{katakana}>[{hiragana}]'
            else:  # If katakana was not enclosed in angle brackets
                return f'{katakana}[{hiragana}]'

        # Use the replacement function to convert matched katakana to hiragana in []
        corrected_text = pattern.sub(replacement, ja_text)

        return corrected_text

    
    def preprocess_text_for_furigana(self, ja_text):
        """
        Adjusts Japanese text to ensure that kanji/katakana sequences directly preceding standalone furigana
        are enclosed in <>, without altering or removing any unassociated hiragana or other characters.
        This version stops including characters in the sequence based on the starting character type (kanji or katakana).
        """
        
        # This pattern is updated to differentiate between sequences starting with kanji or katakana.
        # It ensures that if a sequence starts with kanji, it only includes kanji,
        # and if it starts with katakana, it only includes katakana, directly preceding the furigana annotation.
        # pattern = re.compile(
        #     r'(?<!<)'
        #     r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
        #     r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        # )
        pattern = re.compile(
            r'(?<!<)'
            r'((?:[一-龠々]+|[ァ-ヶー]+))'  # Matches a sequence of kanji or a sequence of katakana
            r'([ぁ-ん]{0,2})'  # Optionally matches zero to two hiragana characters
            r'(?=\[([^\]]+)\])'  # Lookahead for furigana enclosed in brackets without including them in the match
        )

        # Function to enclose matched kanji/katakana sequence in <>
        def enclose_kanji_katakana(match):
            kanji_katakana = match.group(1)  # The kanji/katakana sequence
            return f'<{kanji_katakana}>'

        # Apply the enclosure function to all appropriate sequences in the text
        preprocessed_text = pattern.sub(enclose_kanji_katakana, ja_text)

        # Return the modified text with appropriate enclosures
        return preprocessed_text


    

    # @staticmethod
    def convert_furigana_to_ass(self, ja_text):
        """Convert furigana format from <kanji>[furigana], standalone [furigana], or <kanji> to ASS ruby format,
        applying styles for kanji and furigana where present."""


        

        def replace_with_ruby(match):
            # Adjusted to handle both standalone and combined cases properly.
            kanji = match.group(1)   # Kanji could be in group 1 when present
            furigana = match.group(2 )or match.group(3)  # Furigana is in group 2  or standalone furigana in group 3
            
            # Initialize an empty string for the result
            result = ""
            
            # Format kanji and furigana if present
            if kanji and furigana:
                result = f"{{\\rKanji}}{kanji}{{\\rFurigana}}{furigana}{{\\rDefault}}"
            elif kanji:
                result = f"{{\\rKanji}}{kanji}{{\\rDefault}}"
            elif furigana:
                result = f"{{\\rFurigana}}{furigana}{{\\rDefault}}"
            
            return result

        # Adjusted regex to correctly match <kanji>[furigana], standalone [furigana], or <kanji>
        pattern = r"<([^>]+)>(?:\[(.*?)\])?|\[([^\]]+)\]"

        # Use lambda to pass match object directly to replace_with_ruby
        return re.sub(pattern, replace_with_ruby, ja_text)

    def convert_hanja_to_ass(self, text):
        """Converts text with Hanja annotations to ASS formatted text with styling, based on the given pattern."""
        
        def replace_with_ass(match):
            hanja = match.group(1)
            hangul = match.group(2)
            # roman = match.group(3)

            result = ""


            
            # If Hanja is present, style it with the Kanji style
            if hanja:
                result += f"{{\\rHanja}}{hanja}{{\\rKorean}}"
            
            # Style the Hangul part with the Hangul style
            if hangul:
                result += f"{{\\rHangul}}{hangul}{{\\rKorean}}"
            
            # Add Romanized Korean (RomanKO) if present, styled differently
            # if roman:
            #     # result += f"{{\\rRomanKO}}({roman}){{\\rDefault}}"
            #     # result += f"{{\\rRomanKO}}{roman}{{\\rDefault}}"
            #     pass

            return result

        # Pattern to match the format: <Hanja>[Hangul](RomanKO)
        # pattern = r"<([^>]+)>\[([^]]+)\]\(([^)]*)\)"
        pattern = r"<([^>]+)>\[([^]]+)\]"

        # Replace matches in the text with styled ASS text
        return re.sub(pattern, replace_with_ass, text)

    def convert_chuhan_to_ass(self, text):
        """Converts text with Chu-Han annotations to ASS formatted text with styling, based on the given pattern."""
        
        def replace_with_ass(match):
            chuhan = match.group(1)
            viet = match.group(2)

            result = ""

            # Style the Chu-Han part with the Kanji (or a specific Chu-Han) style
            if chuhan:
                result += f"{{\\rChuhan}}{chuhan}{{\\rVietnamese}}"
            
            # Style the Viet part with a specific Viet style
            if viet:
                result += f"{{\\rViet}}{viet}{{\\rVietnamese}}"

            return result

        # Pattern to match the format: <Chu-Han>[Viet]
        pattern = r"<([^>]+)>\[([^]]*)\]"

        # Replace matches in the text with styled ASS text
        return re.sub(pattern, replace_with_ass, text)

    def estimate_character_width(self, text):
        half_width_count = 0
        full_width_count = 0

        for char in text:
            # Ordinal value of the character
            ord_char = ord(char)
            # Simple checks for half-width vs. full-width character ranges
            if 0x0020 <= ord_char <= 0x007E or 0xFF61 <= ord_char <= 0xFFDC or 0xFFA0 <= ord_char <= 0xFFBE:
                half_width_count += 1
            elif 0x1100 <= ord_char <= 0x11FF or 0x2E80 <= ord_char <= 0x9FFF or 0xAC00 <= ord_char <= 0xD7AF or 0xFF01 <= ord_char <= 0xFF60 or 0xFFE0 <= ord_char <= 0xFFE6:
                full_width_count += 1

        # Determine the predominant character width in the text
        if half_width_count > full_width_count:
            return "half-width"
        else:
            return "full-width"


    # @staticmethod
    def generate_ass_header(self):
        """Generates the header for an ASS file, adjusting `PlayResX` and `PlayResY` based on video orientation."""
        # Determine if the video is landscape or portrait
        # is_video_landscape = self.video_width > self.video_height
        is_video_landscape = self.is_video_landscape

        # Adjust PlayResX and PlayResY based on orientation
        # play_res_x, play_res_y = (self.base_width, self.base_height) if is_video_landscape else (self.base_height, self.base_width)
        play_res_x, play_res_y = (self.video_width, self.video_height) if is_video_landscape else (self.video_height, self.video_width)

        # wrapping_limit_half_width_default = self.wrapping_limit_half_width_default
        # wrapping_limit_full_width_default = self.wrapping_limit_half_width_default // 2

            

        if self.is_video_landscape:
            max_width = self.video_width * 0.8
            max_height = self.video_height * 0.5 / 8
        else:
            max_width = self.video_width * 0.8 * self.portrait_scale
            max_height = self.video_height * 0.5 / 16

        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
        # english_portrait_font_size = find_font_size(sample_texts["english"][:wrapping_limit_half_width_default], font_path, max_width, max_height)


        font_sizes = {}
        for language, text in sample_texts.items():
            char_width_type = self.estimate_character_width(text)
            wrapping_limit = self.wrapping_limit_half_width_default # if char_width_type == 'half-width' else self.wrapping_limit_half_width_default // 2
            text_section = text[:wrapping_limit]
            font_sizes[language] = find_font_size(text_section, self.font_path, max_width, max_height)


        print("calculated font size: ")
        pprint(font_sizes)




        base_font_size = font_sizes["English"]
        chinese_font_size = font_sizes["Chinese"]
        english_font_size = font_sizes["English"] 
        japanese_font_size = font_sizes["Japanese"]
        kanji_font_size = font_sizes["Japanese"]
        furigana_font_size = font_sizes["Japanese"]
        arabic_font_size = font_sizes["Arabic"]
        # korean
        korean_font_size = japanese_font_size 
        hanja_font_size = kanji_font_size 
        hangul_font_size = furigana_font_size 
        romanko_font_size = furigana_font_size 
        #
        spanish_font_size = english_font_size
        french_font_size = english_font_size
        # vietnamese
        vietnamese_font_size = kanji_font_size 
        chuhan_font_size = kanji_font_size 
        viet_font_size = furigana_font_size 
        

        return f"""[Script Info]
ScriptType: v4.00+
Collisions: Normal
PlayResX: {play_res_x}
PlayResY: {play_res_y}

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Vernada,{base_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Emoji,BabelStone Flags,{base_font_size},&H00FFFFFF,&H00000000,&H00000000,&H00000000,0,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Micphone,Noto Color Emoji,{base_font_size},&H00FFFFFF,&H00000000,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: English,Vernada,{english_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chinese,Vernada,{chinese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Japanese,Vernada,{japanese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Kanji,Vernada,{kanji_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Furigana,Vernada,{furigana_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Arabic1,Arial,{arabic_font_size},&H00FACE87,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Arabic,Arial,{arabic_font_size},&H0071B33C,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Korean,Vernada,{korean_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hanja1,Vernada,{hanja_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hanja,Vernada,{hanja_font_size},&H00E16941,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Hangul1,Vernada,{hangul_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Hangul,Vernada,{hangul_font_size},&H00FACE87,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: RomanKO,Vernada,{romanko_font_size},&H00FACADE,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,1,1,2,10,10,10,1
Style: Spanish1,Arial,{spanish_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Spanish,Arial,{spanish_font_size},&H0000BFF1,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: French,Arial,{french_font_size},&H003929ED,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: French2,Arial,{french_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Vietnamese,Arial,{vietnamese_font_size},&H00FFFFFF,&H000000FF,&H00000000,&H64000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chuhan1,Vernada,{chuhan_font_size},&H003C14DC,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Chuhan,Vernada,{chuhan_font_size},&H00ADDEFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: Viet1,Vernada,{viet_font_size},&H007280FA,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1
Style: Viet,Vernada,{viet_font_size},&H00E1E4FF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,50,50,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""             

    def preprocess_japanese_ruby(self, subtitles):
        """Preprocess Japanese text within subtitles."""
        
        def custom_print(enable_print, message, text):
            """Custom print function controlled by an enable flag."""
            if enable_print:
                print(message, text)
        
        enable_print = False  # Control printing for debugging

        for subtitle in subtitles:
            # Check for Japanese text under the 'ja' key
            if 'ja' in subtitle:
                ja_text = subtitle['ja']
                custom_print(enable_print, "Initial text:", ja_text)

                # Apply preprocessing steps
                ja_text = self.rearrange_brackets(ja_text)
                custom_print(enable_print, "After rearranging brackets:", ja_text)

                ja_text = self.fill_blank_of_katakana_without_furigana(ja_text)
                custom_print(enable_print, "After filling in the blank of <katakana>[]:", ja_text)

                ja_text = self.convert_katakana_in_brackets_to_hiragana(ja_text)
                custom_print(enable_print, "After converting katakana in brackets to hiragana:", ja_text)

                ja_text = self.clean_triplicated_sequences(ja_text)
                custom_print(enable_print, "After simplifying triplicated sequences:", ja_text)

                ja_text = self.clean_duplicated_kanji_hiragana_sequence(ja_text)
                custom_print(enable_print, "After converting duplicated kanji-hiragana sequence:", ja_text)

                ja_text = self.clean_redundant_hiragana_sequence(ja_text)
                custom_print(enable_print, "After removing redundant hiragana sequence:", ja_text)

                ja_text = self.clean_duplicated_hiragana_inside_angle_brackets(ja_text)
                custom_print(enable_print, "After simplifying duplicated hiragana inside angle brackets:", ja_text)

                ja_text = self.convert_standalone_angle_to_square_brackets(ja_text)
                custom_print(enable_print, "After converting standalone angle to square brackets:", ja_text)

                ja_text = self.preprocess_text_for_furigana(ja_text)
                custom_print(enable_print, "After preprocessing text for furigana:", ja_text)


                # Update the Japanese text in the subtitle
                subtitle['ja'] = ja_text
                custom_print(enable_print, "Final text:", ja_text)

        return subtitles

    @staticmethod
    def count_furigana(text):
        # Initialize counts for the specified characters
        count_less_than = text.count('<')
        count_greater_than = text.count('>')
        count_open_bracket = text.count('[')
        count_close_bracket = text.count(']')
        hiragana_count = 0

        # Flag to keep track of whether we are inside square brackets
        inside_brackets = False
        
        for char in text:
            # Check if we enter or exit square brackets
            if char == '[':
                inside_brackets = True
            elif char == ']':
                inside_brackets = False
            # If inside brackets, check if the character is a Hiragana
            elif inside_brackets and 'HIRAGANA' in unicodedata.name(char, ''):
                hiragana_count += 1

        # Return the counts as a dictionary
        # return {
        #     'less_than': count_less_than,
        #     'greater_than': count_greater_than,
        #     'open_bracket': count_open_bracket,
        #     'close_bracket': count_close_bracket,
        #     'hiragana_inside_brackets': hiragana_count,
        # }


        # return count_less_than + count_greater_than + \
        #         count_open_bracket + count_close_bracket + \
        #         hiragana_count / 2

        return hiragana_count / 2



    def save_translated_subtitles_to_json(self, translated_subtitles):
        # Write the constructed ASS content to the output file
        with open(self.output_json_path, 'w', encoding='utf-8') as file:
            file.write(json.dumps(translated_subtitles, indent=2, ensure_ascii=False))
        print(f"Subtitles have been processed and saved successfully to {self.output_json_path}.")


    def add_flag_emoji(self, lang_code):
        flags = self.flags
        
        # Return the flag emoji or an empty string if the language code is not found
        return flags.get(lang_code, "")



    def add_microphone_symbol_to_translated_subtitles(self, translated_subtitles):
        """Add a 🎙️ symbol to subtitles in translated_subtitles based on the language specified in self.subtitles."""
        
        for original_subtitle in self.subtitles:
            # Extract the start, end, and language from the original subtitle
            start, end, lang = original_subtitle['start'], original_subtitle['end'], original_subtitle['lang']
            
            # Find the corresponding translated subtitle
            for translated_subtitle in translated_subtitles:
                if translated_subtitle['start'] == start and translated_subtitle['end'] == end:
                    # If a matching subtitle is found, append the 🎙️ symbol to the specified language's text
                    if lang in translated_subtitle:
                        translated_subtitle[lang] = "🔊 " + translated_subtitle[lang] 
        
        return translated_subtitles



    def save_translated_subtitles_to_ass(self, translated_subtitles):
        
        """Save the translated subtitles to an ASS file with text wrapping based on video orientation."""

        # translated_subtitles = self.add_country_flags_to_translated_subtitles(translated_subtitles)
        translated_subtitles = self.add_microphone_symbol_to_translated_subtitles(translated_subtitles)


        if self.is_video_landscape:
            wrapping_limit_half_width_default = self.wrapping_limit_half_width_default
        else:
            wrapping_limit_half_width_default = int(self.wrapping_limit_half_width_default / self.portrait_scale)

        wrapping_limit_half_width_ja = wrapping_limit_half_width_default

        translated_subtitles = self.preprocess_japanese_ruby(translated_subtitles)

        ass_content = self.generate_ass_header()
        for index, item in enumerate(translated_subtitles, start=1):
            # Convert HH:MM:SS,mmm format to seconds
            def convert_to_seconds(t):
                parts = re.split('[:|,]', t)
                if len(parts) == 4:
                    h, m, s, ms = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
                elif len(parts) == 3:
                    # Fallback in case the format is not as expected
                    h, m, s = parts
                    total_seconds = int(h) * 3600 + int(m) * 60 + float(s.replace(',', '.'))
                else:
                    raise Exception(f"The format of timestamps is not recognized: {t}. ")
                
                return total_seconds
            
            # Format start and end times to ensure two decimal places with comma
            start_seconds = convert_to_seconds(item['start'])
            end_seconds = convert_to_seconds(item['end'])
            start = "{:02d}:{:02d}:{:05.2f}".format(int(start_seconds // 3600), int((start_seconds % 3600) // 60), start_seconds % 60)#.replace('.', ',')
            end = "{:02d}:{:02d}:{:05.2f}".format(int(end_seconds // 3600), int((end_seconds % 3600) // 60), end_seconds % 60)#.replace('.', ',')

            dialogue_lines = []

            # Process languages in a specific order if needed, then all additional languages
            preferred_order = ['zh', 'en', 'ja', "ar", "ko", "es", "vi", "fr", "ru"][::-1]  # Example: Start with Chinese, then English, then Japanese
            handled_keys = set(preferred_order)

            # Add preferred languages first
            for lang in preferred_order:
                if lang in item:
                    text = item[lang]

                    flag_emoji = self.add_flag_emoji(lang)  # Get the flag emoji without adding a space for separation
                    
                    is_cjk = lang in ['zh', 'ja']  # Assuming CJK for Chinese and Japanese
                    
                    if lang == "ja":
                        wrapping_limit_half_width = wrapping_limit_half_width_ja + self.count_furigana(text)
                    else:
                        wrapping_limit_half_width = wrapping_limit_half_width_default

                    if lang == "ar":
                        style = "Arabic"
                    elif lang == "zh":
                        style = "Chinese"
                    elif lang == "en":
                        style = "English"
                    elif lang == "ja":
                        style = "Japanese"
                    elif lang == "es":
                        style = "Spanish"
                    elif lang == "fr":
                        style = "French"
                    elif lang == "vi":
                        style = "Vietnamese"
                    else:
                        style = "Default"

                   

                    wrapped_text_lines = self.wrap_text(text, wrapping_limit_half_width, is_cjk=is_cjk)
                    dialogue_line = '\\N'.join(wrapped_text_lines)

                    # Prepend flag emoji with the Emoji style and revert back to the original style for the text
                    # if "🔊" in dialogue_line:
                    #     dialogue_line = f"{{\\rMicphone}}{dialogue_line[:1]}{{\\rEmoji}}{flag_emoji}{{\\r{style}}}{dialogue_line[1:]}"
                    # else:
                    #     dialogue_line = f"{{\\rEmoji}}{flag_emoji}{{\\r{style}}}{dialogue_line}"


                    if lang == 'ja':
                        dialogue_line = self.convert_furigana_to_ass(dialogue_line)

                    if lang == "ko":
                        dialogue_line = self.convert_hanja_to_ass(dialogue_line)

                    if lang == "vi":
                        dialogue_line = self.convert_chuhan_to_ass(dialogue_line)


                    subtitle_line = f"Dialogue: 0,{start},{end},{style},,0,0,0,,{dialogue_line}"
                    ass_content += subtitle_line + "\n"

            # Add any additional languages present
            additional_languages = {k: v for k, v in item.items() if k not in handled_keys.union(['start', 'end'])}
            for lang_code, text in additional_languages.items():
                # Additional languages will use the default style as this block does not handle style differentiation
                dialogue_line = self.wrap_text(text, wrapping_limit_half_width_default, is_cjk=False)
                formatted_text = '\\N'.join(dialogue_line)
                subtitle_line = f"Dialogue: 0,{start},{end},Default,,0,0,0,,{formatted_text}"
                ass_content += subtitle_line + "\n"

        # Write the constructed ASS content to the output file
        with open(self.output_sub_path, 'w', encoding='utf-8') as file:
            file.write(ass_content)
        print(f"Subtitles have been processed and saved successfully to {self.output_sub_path}.")


    def wrap_text(self, text, wrapping_limit_half_width, is_cjk=False):
        if not is_cjk:
            # For non-CJK text, directly use the wrapping function.
            # return self.cjkwrap_punctuation(text, wrapping_limit_half_width)
            return [text]

        # Step 1: Wrap the text into lines.
        wrapped_lines = self.cjkwrap_punctuation(text, wrapping_limit_half_width)

        # Step 2: Join lines with '###' to mark original line breaks.
        joined_text = '###'.join(wrapped_lines)

        # if "歴史" in joined_text:
        #     print("joined_text: ", joined_text)

        # Step 3: Correct breakpoints only for structured texts that were split.
        def correct_breakpoints(match):
            structured_text = match.group(0)

            # if "歴史" in joined_text:
            #     for i in range(4):
            #         try:
            #             print(match.group(i))
            #         except:
            #             pass

            # If '###' is inside the structured text, it indicates an incorrect break.
            if '###' in structured_text:
                # Remove '###' and keep the structure intact.
                return "###" + structured_text.replace('###', '') + "###"
            else:
                # If no '###' inside, return the structured text as is.
                return structured_text

        pattern = r'(<[^>]*>)(###)?\[[^\]]*\]|<[^>]*>|(\[[^\]]*\])'
        corrected_text = re.sub(pattern, correct_breakpoints, joined_text)

        # Step 4: Split the text back into lines at '###'.
        corrected_lines = corrected_text.split('###')

        joined_lines = self.join_lines_with_length_check(corrected_lines, wrapping_limit_half_width)

        # return corrected_lines
        return joined_lines

    @staticmethod
    def strip_brackets(input_string):
        # Regular expression pattern to match <, >, [, and ]
        pattern = r'[\<\>\[\]]'
        # Replace matched characters with an empty string
        stripped_string = re.sub(pattern, '', input_string)
        return stripped_string

    def join_lines_with_length_check(self, lines, wrapping_limit_half_width):
        final_lines = []
        current_line = ""
        for line in lines:
            if current_line:
                # Attempt to join with the next line and check if it exceeds the wrapping limit.
                test_line = current_line + line
                wrapped_test = self.cjkwrap_punctuation(self.strip_brackets(test_line), wrapping_limit_half_width)
                if len(wrapped_test) > 1:
                    # If joining exceeds the limit, finalize the current line and start a new one.
                    final_lines.append(current_line)
                    current_line = line
                else:
                    # Otherwise, update the current line to include the next line.
                    current_line = test_line
            else:
                current_line = line

        # Ensure the last accumulated line is added to the final output.
        if current_line:
            final_lines.append(current_line)

        return final_lines


    def cjkwrap_punctuation(self, text, width):
        """
        Custom wrapper for CJK text that prioritizes wrapping at punctuation,
        and adjusts lines to avoid starting with punctuation.
        """


        wrapped_lines = cjkwrap.wrap(text, width)



        # Define full-width and half-width punctuation marks for CJK text
        punctuations = ".。、，,！!？?；;：:「」『』（）()【】[]《》<>「」『』“”\"\""
        # punctuations = ".。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""
        # punctuations = "。、，,！!？?；;：:「」『』（）()【】《》「」『』“”\"\""

        

        # Adjust lines to move punctuation from the beginning of a line to the end of the previous line
        for i in range(1, len(wrapped_lines)):
            if wrapped_lines[i][0] in punctuations:
                # Move punctuation to the end of the previous line if it doesn't exceed width
                if len(wrapped_lines[i-1]) + 1 <= width:
                    wrapped_lines[i-1] += wrapped_lines[i][0]  # Move punctuation to the end of the previous line
                    wrapped_lines[i] = wrapped_lines[i][1:]  # Remove punctuation from the current line

        # Handle case where the first line starts with punctuation and there's no previous line to adjust
        # This might involve a specific strategy, such as re-wrapping with cjkwrap if needed
        # For simplicity, this case is not explicitly handled here but should be considered based on your requirements

        return wrapped_lines



if __name__ == '__main__':
    
    # Example usage:
    input_json_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.json'
    output_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/translated_subtitles.srt'
    input_sub_path = '/home/lachlan/Projects/lazyedit/lazyedit/data/IMG_6276_mixed.srt'


    openai_client = OpenAI()
    subtitles_processor = SubtitlesTranslator(openai_client, input_json_path, input_sub_path, output_sub_path)
    subtitles_processor.process_subtitles()

===== ./video_captioner.py =====
# import os
# import subprocess
# import argparse

import os
import subprocess
import argparse
import signal
import time



# class VideoCaptioner:
#     def __init__(self, video_path, output_folder, num_frames=3, model_size='L', checkpoint_name='model.pt', temperature=1.0):
#         self.video_path = video_path
#         self.output_folder = output_folder
#         self.num_frames = num_frames
#         self.model_size = model_size
#         self.checkpoint_name = checkpoint_name
#         self.temperature = temperature
#         self.caption_srt_path = os.path.splitext(video_path)[0] + "_caption.srt"
#         self.caption_json_path = os.path.splitext(video_path)[0] + "_caption.json"
#         self.conda_env_path = "/home/lachlan/miniconda3/envs/caption/bin/python"

#     def run_captioning(self):
#         # Build the command for the captioning process
#         # caption_command = f"{self.conda_env_path} /home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/v2c.py -V \"{self.video_path}\" -N {self.num_frames}"
#         caption_command = f"{self.conda_env_path} /home/lachlan/Projects/vit-gpt2-image-captioning/vit_captioner_video.py -V \"{self.video_path}\" -N {self.num_frames}"
        
#         # Run the command in a subprocess
#         result = subprocess.run(caption_command, shell=True, check=True, text=True)
#         if result.returncode == 0:
#             print(f"Captioning completed successfully, output saved to: {self.caption_srt_path}")
#             print(f"Captioning completed successfully, output saved to: {self.caption_json_path}")
#         else:
#             print("Captioning process failed.")


class VideoCaptioner:
    def __init__(self, video_path, output_folder, num_frames=3, model_size='L', checkpoint_name='model.pt', temperature=1.0):
        self.video_path = video_path
        self.output_folder = output_folder
        self.num_frames = num_frames
        self.model_size = model_size
        self.checkpoint_name = checkpoint_name
        self.temperature = temperature
        self.caption_srt_path = os.path.splitext(video_path)[0] + "_caption.srt"
        self.caption_json_path = os.path.splitext(video_path)[0] + "_caption.json"
        self.conda_env_path = "/home/lachlan/miniconda3/envs/caption/bin/python"
        self.base_command = "/home/lachlan/Projects/vit-gpt2-image-captioning/vit_captioner_video.py"
        self.alternate_command = "/home/lachlan/Projects/image_captioning/clip-gpt-captioning/src/v2c.py"

    def vague_kill(self, command):
        # Kill processes based on the script path only
        kill_command = f"pkill -f \"{self.conda_env_path} {command}\""
        os.system(kill_command)

    def specific_kill(self, full_command):
        # Kill the specific full command
        kill_command = f"pkill -f \"{full_command}\""
        os.system(kill_command)

    def run_captioning(self):
        # Clear all potential interfering processes initially
        # self.vague_kill(self.base_command)
        # self.vague_kill(self.alternate_command)
        
        try:
            # First attempt to run the base command
            caption_command = f"{self.conda_env_path} {self.base_command} -V \"{self.video_path}\" -N {self.num_frames}"
            result = subprocess.run(caption_command, shell=True, check=True, timeout=180)  # 180 seconds = 3 minutes
            print(f"Captioning completed successfully, output saved to: {self.caption_srt_path}")
            print(f"Captioning completed successfully, output saved to: {self.caption_json_path}")
        except subprocess.TimeoutExpired:
            print("First command timed out. Trying alternative command.")
            self.specific_kill(caption_command)

            # Alternative command
            alternative_command = f"{self.conda_env_path} {self.alternate_command} -V \"{self.video_path}\" -N {self.num_frames}"
            try:
                subprocess.run(alternative_command, shell=True, check=True, timeout=180)
                print("Alternative captioning completed successfully.")
            except subprocess.TimeoutExpired:
                print("Alternative command timed out. Saving empty file.")
                self.specific_kill(alternative_command)
                
                with open(self.caption_srt_path, 'w') as file:
                    file.write("")  # Create an empty file

                with open(self.caption_json_path, 'w') as file:
                    file.write("")  # Create an empty file


        self.specific_kill(caption_command)
        self.specific_kill(alternative_command)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Automate video captioning process using a specific conda environment")
    parser.add_argument("-V", "--video-path", type=str, required=True, help="Path to the video file")
    parser.add_argument("-O", "--output-folder", type=str, required=True, help="Output folder to store results")
    parser.add_argument("-N", "--num-frames", type=int, default=10, help="Number of frames to caption")
    parser.add_argument("-S", "--size", type=str, default="L", help="Model size [S, L]")
    parser.add_argument("-C", "--checkpoint-name", type=str, default="model.pt", help="Name of the model checkpoint")
    parser.add_argument("-T", "--temperature", type=float, default=1.0, help="Temperature for sampling")

    args = parser.parse_args()

    vc_processor = VideoCaptioner(args.video_path, args.output_folder, args.num_frames, args.size, args.checkpoint_name, args.temperature)
    vc_processor.run_captioning()



# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Automate video captioning process using a specific conda environment")
#     parser.add_argument("-V", "--video-path", type=str, required=True, help="Path to the video file")
#     parser.add_argument("-O", "--output-folder", type=str, required=True, help="Output folder to store results")
#     parser.add_argument("-N", "--num-frames", type=int, default=10, help="Number of frames to caption")
#     parser.add_argument("-S", "--size", type=str, default="L", help="Model size [S, L]")
#     parser.add_argument("-C", "--checkpoint-name", type=str, default="model.pt", help="Name of the model checkpoint")
#     parser.add_argument("-T", "--temperature", type=float, default=1.0, help="Temperature for sampling")

#     args = parser.parse_args()

#     vc_processor = VideoCaptioner(args.video_path, args.output_folder, args.num_frames, args.size, args.checkpoint_name, args.temperature)
#     vc_processor.run_captioning()


